<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Spencer's Blog</title>
        <link>https://blog.spencerwoo.com/</link>
        <description>Spencer Woo - 开发者 / 设计师 / 少数派 / 学生</description>
        <lastBuildDate>Thu, 16 May 2024 17:34:13 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>Gridsome Feed Plugin</generator>
        <image>
            <title>Spencer's Blog</title>
            <url>https://blog.spencerwoo.com/av.png</url>
            <link>https://blog.spencerwoo.com/</link>
        </image>
        <atom:link href="https://blog.spencerwoo.com/feed.xml" rel="self" type="application/rss+xml"/>
        <item>
            <title><![CDATA[Recap 2020]]></title>
            <link>https://blog.spencerwoo.com/2021/01/new-year-2021/</link>
            <guid>https://blog.spencerwoo.com/2021/01/new-year-2021/</guid>
            <pubDate>Fri, 01 Jan 2021 18:00:00 GMT</pubDate>
            <content:encoded><![CDATA[<p>Welcome 2021. 昨天（2020/1/1）我也才刚刚从北京回家的高铁上下来，现在我坐在刚搬来的新家里，用着陪伴了我快 3 年的机械键盘，写下这份迟到了一天的碎碎念。希望大家不要嫌弃。</p>
<hr>
<p>糟透的一年过去了，这一年里疫情起起伏伏，我的心情也反反复复。大学的最后一个学期我全程在家里度过，没有隆重的毕业典礼，没有同学们的哭着道别，更没有喝到天明的狂欢通宵。直到我毕业答辩结束了，在 6 月的某一天凌晨，我才突然意识到，<strong>我自己的本科生涯已经结束了。</strong></p>
<p>当然，这些可能是唯一的遗憾了。我们划掉重来，看看 2020 年开心的事情。7 月后的日子仿佛过得飞快，先是疫情总算缓解，和零零散散（但是玩的最好）的几个同学终于约上了去云南的毕业旅行，那 10 天可能是 2020 年里面过得最快的 10 天。这期间，毕业的事情尘埃落定，我也拿到了校「优秀」毕业设计的成绩、为同学们制作 LaTeX 毕业论文模板的酬金、以及大学最后一学期的学业奖学金。另外，未来的硕士学校 University of Glasgow 也给我发来了 Unconditional Offer 的邮件。事情总算好了起来。</p>
<p><img src="https://cdn.spencer.felinae98.cn/blog/75928f2f96bfa9c9633fd04c842ccd65.png" alt="Unconditional Offer from UofG"></p>
<p>可惜，还是因为疫情，我又被迫推迟了半年，在 2021/1 才能正式入学上课（也就是马上啦！），2020 下半年的这段 Gap 里面，我自告奋勇留在了之前做毕业设计的实验室里面，以科研助理的身份继续科研。于是在 9 月，我又一次回到了北京，跟大老板见面，谈了谈诗词歌赋、人生哲理，大师兄为我安排了工位，我也算是半只脚踏进了科研的世界。没想到 2020 年后半年的这段时间，可能是我工作最丰富，但是每天却最快乐一段时间。非常感谢实验室的收留，也非常感谢坐在我前后左右的师兄弟姐妹：不论是科研工作还是<del>快乐上号</del>。这或许是我距离顶会最近的一次，希望希望一定能成功。❤️</p>
<hr>
<p>枯燥的流水账说完了，接下来说说被疫情憋在家里的我，无聊得都写了啥子。</p>
<p>GitHub 贡献榜单或许是最能说明问题的了，截止到 2020/12/31，我全年贡献了 2813 个 commit，实际上除了过年的那几天以及毕业旅行的几天，我几乎每天都有 commit，自己还算骄傲的！其实还挺明显的，不过就是——前半年的我在忙着毕业，后半年的我在忙着科研嘛 🤣。</p>
<p><img src="https://cdn.spencer.felinae98.cn/blog/9f473bba9cdf7154753ab524f4a9c986.png" alt="GitHub Contributions for 2020"></p>
<p>抛开这两个看似占据大部分工作量的事情外，我也还创建了几个<em>备受欢迎的</em>开源项目：</p>
<ul>
<li><a href="https://github.com/spencerwooo/Substats">Substats</a> - 订阅、粉丝、关注者统计 Serverless API <img style="display: inline; transform: translateY(0.25em);" src="https://img.shields.io/github/stars/spencerwooo/Substats?style=social" alt="github repo star"/></li>
<li><a href="https://github.com/spencerwooo/onedrive-cf-index">onedrive-cf-index</a> - 基于 Cloudflare Workers 的 OneDrive 网盘分享站点  <img style="display: inline; transform: translateY(0.25em);" src="https://img.shields.io/github/stars/spencerwooo/onedrive-cf-index?style=social" alt="github repo star"/></li>
<li><a href="https://github.com/TeamMeow/vscode-math-to-image">vscode-math-to-image</a> - 渲染 Markdown 中 LaTeX 公式为 SVG 的 VS Code 插件  <img style="display: inline; transform: translateY(0.25em);" src="https://img.shields.io/github/stars/TeamMeow/vscode-math-to-image?style=social" alt="github repo star"/></li>
</ul>
<p>对开源项目进行贡献，确实是有着巨大成就感作为回报的，希望 2021 年我也能如此「奋发图强」🧑🏿‍🚒</p>
<hr>
<figure>
  <img src="https://cdn.spencer.felinae98.cn/blog/7caf20c00c5cad3cb7dda7c2e6529a9a.png" width="360px" alt="少数派年度作者 2020">
  <figcaption>少数派年度作者 2020</figcaption>
</figure>

<p>最后我再来一起讲讲所谓的「内容输出」。众所周知，少数派是我的主战场这一点毋庸置疑，即刻复活之后虽然没有从前清净但也还不错，轻芒杂志作为后起之秀专注阅读所以其实没有特别大的输出空间，我的博客呢，跟我微信公众号一样半死不活 🤦🏼。幸好，年初的隔离让我能有时间在少数派上疯狂输出，在即使是后半年科研压力缠身的情况下，依旧拿下了年度作者的称号，实属荣幸。另外，轻芒杂志邀请我做先锋读者我真是于心有愧，毕竟我这个文化沙漠，哪能有那些大佬们的经验见解啊。（但是轻芒杂志确实好用，推荐大家下载用一用啊，非线性的推荐首页能把你订阅的内容分割开来，有传统 RSS 阅读器无法媲美的独到阅读体验。）</p>
<p>好了，文章到这里也差不多结束了。新的一年，我唯一的愿望应该就是平安到达英国，平安完成硕士学业，如果论文能被接受，还能再找到完美的 PhD 机会那就真的是十全十美了。谢谢大家对我的关注，我们 2021 见。</p>
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[震惊！竟然有人在 GitHub 上冒充我的身份！]]></title>
            <link>https://blog.spencerwoo.com/2020/08/wait-this-is-not-my-commit/</link>
            <guid>https://blog.spencerwoo.com/2020/08/wait-this-is-not-my-commit/</guid>
            <pubDate>Sun, 30 Aug 2020 09:50:00 GMT</pubDate>
            <content:encoded><![CDATA[<h2 id="起因">起因</h2>
<p>这件事情还要从我校为毕业生收拾行李开始讲起。今年六月末北京疫情复发，这一波直接让我们北京高校毕业生无法返校，个人行李物品必须由学校老师代为整理快递回家。这件事情让同级的同学们非常不满，于是微博知乎节奏飞起。当然这件事情是北京统一的行为，其中不光有我校学生自己不满，其他北京的学校听说有些处理的比我校更要糟糕，所以这件事情我们暂且不做评价。但从事情开始，就有「好事」的同学除了在知乎等平台上进行回答评论、表达意见，还直接在 GitHub 上面直接整理记录时间线。</p>
<p><img src="https://cdn.spencer.felinae98.cn/blog/2020/08/200830_153718.png" alt="一个目录，暂时不放具体仓库与地址"></p>
<h2 id="我被牵扯进去了？">我被牵扯进去了？</h2>
<p>为什么这个仓库引起了我的注意呢？首先，对于收拾行李这件事情来说，我所在学院做的其实不错，我自己是我院一位备受尊敬的副教授老师为我收拾的，整理的非常好，所以我个人自始至终并没有对这件事情有什么太大意见，没有参与知乎讨论，也没有进行所谓的争论抗议。这些都是两个月之前发生的事情，但是，两个月之后的今天，有认识我的同学告诉我这个仓库的存在，<strong>并私下询问我为什么也给这个仓库进行了贡献。</strong></p>
<p><strong>是的，一个我直到昨天都还完全不知道存在的仓库，有人看到了「一条 commit 是由我的邮箱签入的」，并「链接到我的 GitHub 账户」。</strong></p>
<p>我当时：</p>
<p><img src="https://cdn.spencer.felinae98.cn/blog/2020/08/200830_154650.png" alt="……"></p>
<p>好奇心让我点开了同学发给我的链接，向下划到 Contributors，果然，我的头像就在那里。</p>
<p><img src="https://cdn.spencer.felinae98.cn/blog/2020/08/200830_154938.png" alt="淦，老子昨天才知道你这仓库的，宁真厉害"></p>
<p>好嘛，除了我的头像，还有个我非常眼熟的头像 —— 下面这位老爷子：</p>
<p><img src="https://cdn.spencer.felinae98.cn/blog/2020/08/200830_155124.png" alt="这人我看着有点眼熟……"></p>
<p>我去，这不是 Python 他爹吗？！！敢情 Python 之父也关心我们小破学校毕业生收拾行李的事情了？</p>
<h2 id="我惊了">我惊了</h2>
<p>仔细看了一下贡献者列表，其中不乏开源世界的知名人物，也包括一些我校在 GitHub 上有账号的同学们。好了，这处处透露着诡异的仓库，<strong>看起来除了这位仓库主人自己的 commit 以外，其余所有的 commit 的身份信息都是伪造的。</strong></p>
<p><img src="https://cdn.spencer.felinae98.cn/blog/2020/08/200830_160029.png" alt="细节修改你个大头鬼啊，这里「我的」commit 周围都被篡改为我身边认识的大佬同学了"></p>
<p>这件事情昨天确实让我非常震惊：Git 的 commit 记录竟然还可以伪造。我确实之前从来没有在意过这类安全问题，以为签入 Git 记录的内容都有完善的身份验证，<strong>而事实证明我太天真了。</strong></p>
<p>这位我完全不认识的同学，我不知道你是何种初衷，把这个「搞事情」的仓库的 commit 记录在我们都不知情的情况下篡改为我们的身份，<strong>但是你这种行为让我感到非常恶心</strong>。如果你自己都不敢于承担自己「搞事情」所带来的风险，一定要「强行」拉着一群不知情的同学，来装作好多人都对此有所看法、跃跃欲试、一起贡献的样子，<strong>那你还搞个 🐔 8️⃣ 啊！</strong></p>
<p>那…现在怎么办？我自己给 GitHub Support 已经发去了邮件，不知道这种事情 GitHub 会不会帮我解决，但是从我自己的角度来说，除了尽可能通知我认识的同学他们在 GitHub 上也被人冒充了外，也只能分析一下为什么这种漏洞会存在，以及接下来该如何解决这种问题了。</p>
<h2 id="漏洞分析">漏洞分析</h2>
<h3 id="git-的设计缺陷">Git 的设计缺陷</h3>
<p>事实上，Git 本身是具有这样的设计缺陷的。<strong>Git commit 信息的 author 是一个可以零成本造假的字符串。</strong>首先，我们来看看一个 commit 里面包含哪些信息。我们可以用 <code>git log</code>（或 Oh My Zsh 的 alias 命令：<code>glog</code> 来打印一个更为清楚的 commit 历史）来查看本地 Git 仓库的 commit 记录，并找到一个特定 commit 的 hash，比如我当前仓库的 HEAD commit hash 为 <code>d3f97ef</code>。</p>
<p><img src="https://cdn.spencer.felinae98.cn/blog/2020/08/200830_163010.png" alt="Git 仓库的 commit 记录"></p>
<p>我使用一个上周的 commit hash <code>df6eb5f</code>，我们可以用 <code>git cat-file -p df6eb5f</code> 来查看这一 commit 的具体信息：</p>
<p><img src="https://cdn.spencer.felinae98.cn/blog/2020/08/200830_163212.png" alt="Commit hash 为 df6eb5f 的 commit 具体信息"></p>
<p>可以发现，每个 commit 都拥有 commit 的 author 和 commit 的 committer，分别是 commit 的第一作者和执行 commit 具体操作的人。如何确认这两人的具体身份呢？Git 仅记录了 commit author 和 committer 二人的名称、邮箱和时间戳，而其中的名称和邮箱正是我们配置 Git 时设定的 <code>user.name</code> 和 <code>user.email</code>，而 GitHub 也正是通过这两个内容确定 commit 的具体作者和 GitHub 身份的。</p>
<p>好的，既然我们知道了 Git 和 GitHub 是如何确认身份的，那么我们如何修改 commit author 和 committer 呢？事实上，这两个内容仅是字符串存储的，<code>user.name</code> 和 <code>user.email</code> 都是可以任意篡改的，因此我们完全可以直接修改自己 git config 中存储的 <code>user.name</code> 和 <code>user.email</code> 来让本次 commit 的作者变为另一个人。原生 Git 完全没有任何第二层防护！</p>
<p>甚至，我们可以将整个仓库的 Git commit 历史通过 <code>filer-branch</code> 批量修改为其他的人：</p>
<pre><code class="language-shell">git commit -am &quot;Destroy production&quot;
git filter-branch --env-filter \
  &#39;if [ &quot;$GIT_AUTHOR_EMAIL&quot; = &quot;iamthe@evilguy.com&quot; ]; then
     GIT_AUTHOR_EMAIL=&quot;unsuspecting@victim.com&quot;;
     GIT_AUTHOR_NAME=&quot;Unsuspecting Victim&quot;;
     GIT_COMMITTER_EMAIL=$GIT_AUTHOR_EMAIL;
     GIT_COMMITTER_NAME=&quot;$GIT_AUTHOR_NAME&quot;; fi&#39; -- --all
git push -f</code></pre>
<p>大概就是这样，Git 的 commit 是可以任意修改的，你可以将某个坏 commit 嫁祸给别人，甚至将某个坏仓库的 commit 批量嫁祸给毫不知情的人，但我希望大家永远都不要做这样的事情！</p>
<h3 id="如何防范这种行为？">如何防范这种行为？</h3>
<p>这可怎么办，我们该如何在互联网上证明自己是自己？该如何证明这不是自己？对于 Git 来说，其实我们还是有办法的 —— GPG 签名。GPG 全称为 GNU Privacy Guard，GPG 通过非对称加密来帮助我们从密码学的角度证明「我是我」，也从而证明「这不一定真的是我」。</p>
<p>使用一个只有我们自己手中拥有的 GPG 私钥对我们的 commit 进行签名，可以让 GitHub 确认我们本次 commit 是真实且是本人操作的。这样，别有用心的他人就无法以我们的身份创建「被签名」的 commit。在 GitHub 上使用的 GPG 密钥和我们的 SSH 密钥并不一样，后者 SSH key 唯一存在的原因是为了向 GitHub 证明身份，用于向我们拥有权限的仓库中进行 commit，而前者 GPG key 则是为了「证明我拥有本次 commit 的著作权」，也只有用 GPG 私钥签名的 commit 在 GitHub 上才会显示如下图的 Verified 绿色钦定小标标。</p>
<p><img src="https://cdn.spencer.felinae98.cn/blog/2020/08/200830_165356.png" alt="使用 GPG 签名的 commit 会在 GitHub 上显示 Verified 标志"></p>
<h2 id="使用-gpg-key-来证明-commit-著作权">使用 GPG key 来证明 commit 著作权</h2>
<p>:::note 💙 GitHub 官方文档
GitHub 官方文档拥有更为详细的 GPG 密钥构建和添加方法：<a href="https://docs.github.com/en/github/authenticating-to-github/managing-commit-signature-verification">Managing commit signature verification</a>.
:::</p>
<h3 id="下载安装-gpg">下载安装 GPG</h3>
<p>首先，我们需要下载安装 GPG 命令行工具，在 Windows 上可以通过 <code>scoop install gpg</code> 来安装，大部分 Linux 发行版也应该直接拥有 GPG 工具。</p>
<pre><code class="language-shell"># Windows 用户下载安装 GPG
$ scoop install gpg</code></pre>
<p>使用 <code>gpg --version</code> 查看 GPG 安装情况和版本信息，并记住 GPG 存储根目录：即输出内容中的 Home 目录。</p>
<pre><code class="language-shell"># 测试 GPG（Windows 或 Linux）
$ gpg --version

gpg (GnuPG) 2.2.19
libgcrypt 1.8.5
Copyright (C) 2019 Free Software Foundation, Inc.
License GPLv3+: GNU GPL version 3 or later &lt;https://gnu.org/licenses/gpl.html&gt;
This is free software: you are free to change and redistribute it.
There is NO WARRANTY, to the extent permitted by law.

Home: /home/spencer/.gnupg
Supported algorithms:
Pubkey: RSA, ELG, DSA, ECDH, ECDSA, EDDSA
Cipher: IDEA, 3DES, CAST5, BLOWFISH, AES, AES192, AES256, TWOFISH,
        CAMELLIA128, CAMELLIA192, CAMELLIA256
Hash: SHA1, RIPEMD160, SHA256, SHA384, SHA512, SHA224
Compression: Uncompressed, ZIP, ZLIB, BZIP2</code></pre>
<h3 id="为自己生成一对-gpg-密钥">为自己生成一对 GPG 密钥</h3>
<p>之后，我们就可以用下面的命令来为自己生成一个 GPG 公钥和私钥：</p>
<pre><code class="language-shell">$ gpg --full-generate-key</code></pre>
<ul>
<li>在密钥种类处：选择默认 RSA and DSA 即可；</li>
<li>在密钥长度选项处：按照 GitHub 的要求选择 4096 bits；</li>
<li>在密钥过期时间处：按照自己的需要选择，默认为永不过期；</li>
<li>在我们的用户 ID 和 GPG key 签名邮箱处：填写我们的常用用户名，并<strong>填入 GitHub 上面认证过的邮箱</strong>；</li>
<li>最后，为密钥设置一个安全的密码，并一定记住这一密码。</li>
</ul>
<p>这样，我们就生成了我们的第一对 GPG 密钥！我们可以用这样的命令查看当前我们拥有的所有 GPG key：</p>
<pre><code class="language-shell">$ gpg --list-secret-keys --keyid-format LONG

/home/spencer/.gnupg/pubring.kbx
--------------------------------
sec   rsa4096/24CD550268849CA0 2020-08-29 [SC]
      9433E1B6807DE7C15E20DC3B24CD550268849CA0
uid                 [ultimate] Spencer Woo (My GPG key) &lt;my@email.com&gt;
ssb   rsa4096/EB754D2B2409E9FE 2020-08-29 [E]</code></pre>
<p>其中，<code>sec</code> 一行的 <code>rsa4096/24CD550268849CA0</code> 就是我们的 GPG 私钥，其中的 <code>24CD550268849CA0</code> 即为我们的 GPG 私钥 ID。</p>
<h3 id="告诉-git-自己的-gpg-密钥-id">告诉 Git 自己的 GPG 密钥 ID</h3>
<p>生成了 GPG 密钥，并拿到了我们的 GPG 私钥 ID 后，我们即可让 Git 用这一 GPG key 为我们的 commit 进行签名：</p>
<pre><code class="language-shell">$ git config --global user.signingkey 24CD550268849CA0
$ git config --global commit.gpgsign true</code></pre>
<p>这样设置后，如果没有问题，之后的 commit 中 Git 就会自动为我们用这一 GPG 私钥进行签名。我们可以用这一命令确认签名的存在：</p>
<pre><code class="language-shell">$ git log --show-signature

commit c407d4efc980cbee981da50d714a751999b19ddf (HEAD -&gt; master)
gpg: Signature made Sun Aug 30 17:16:18 2020 CST
gpg:                using RSA key 9433E1B6807DE7C15E20DC3B24CD550268849CA0
gpg: Good signature from &quot;Spencer Woo (My GPG key) &lt;my@email.com&gt;&quot; [ultimate]
Author: spencerwooo &lt;my@email.com&gt;
Date:   Sun Aug 30 17:16:18 2020 +0800

    Signed by GPG</code></pre>
<p>另外，此时我们再次用之前查看 commit 详细信息的命令查看本次 commit，我们会发现 GPG 签名已经直接保存于这一 commit 之中了：</p>
<pre><code class="language-shell">$ git cat-file -p c407d4e</code></pre>
<p><img src="https://cdn.spencer.felinae98.cn/blog/2020/08/200830_172152.png" alt="已经签名过的 commit 包含有我们使用的 PGP signature"></p>
<p>另外，这里如果出现类似的问题，可能是 Git 使用的 GPG 命令行工具跟我们生成密钥使用的不一致。我们可以首先用 <code>which gpg</code> 来找到我们所使用的 GPG 工具的具体地址，比如 <code>/usr/bin/gpg</code>，之后告诉 Git 使用这一 GPG binary 即可：</p>
<pre><code class="language-shell">$ git config --global gpg.program /usr/bin/gpg</code></pre>
<p>:::important 🥦 GPG 可执行文件路径
Windows 上的同学，也可以使用 <code>which</code> 命令！只需要用 scoop 安装：<code>scoop install which</code>，即可方便的用类似 Linux 上的语法找到相应的可执行文件具体路径。
:::</p>
<h3 id="告诉-github-自己的-gpg-公钥">告诉 GitHub 自己的 GPG 公钥</h3>
<p>最后，我们需要告诉 GitHub 我们使用的 GPG 公钥。对于刚刚我们拿到的私钥 ID：<code>24CD550268849CA0</code>，我们使用下面的命令即可导出我们的 GPG 公钥：</p>
<pre><code class="language-shell">$ gpg --armor --export 24CD550268849CA0</code></pre>
<p>将输出粘贴进入 GitHub 的 <a href="https://github.com/settings/keys">Settings » SSH and GPG keys » New GPG key</a>，并保存。之后，我们就可以开始在 GitHub 上享受 Verified 被钦定的感觉！</p>
<h2 id="小结">小结</h2>
<p>使用 GPG 不仅可以证明我们的每次 commit 的所有权，还可以用类似的密码学方法证明 GitHub 账号的所有权、域名的所有权、Twitter 账号的所有权等等。我们将我们的 GPG 公钥托管在某个 GPG 服务器上面，别人就可以利用这一公钥来验证某个被签名的内容是否确实是我们所操作。<a href="https://keybase.io/inv/784d1a88fa">Keybase.io</a> 是一个 trusted database for public keys，推荐大家使用 <a href="https://keybase.io/inv/784d1a88fa">Keybase.io</a> 托管自己的 GPG 公钥。</p>
<p>无论如何，大家都可以用这一命令拉取并导入我（Spencer Woo）的 GPG 公钥签名：</p>
<pre><code class="language-shell">$ curl https://keybase.io/spencerwoo/pgp_keys.asc | gpg --import</code></pre>
<ul>
<li>我的 Keybase 地址：<a href="https://keybase.io/spencerwoo">keybase.io/spencerwoo</a></li>
<li>我的 Keybase 公钥：ASCtXMcCY0UpKPF6NpoLlwJT3xXsD5nzunxF2ei4gBRBkgo</li>
</ul>
<p>感谢大家的阅读，希望大家都不会遭遇被冒充的情况！</p>
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Time Complexity：CPython 实现的 Python 操作的时间复杂度]]></title>
            <link>https://blog.spencerwoo.com/2020/08/cpython-implemented-python-time-complexity/</link>
            <guid>https://blog.spencerwoo.com/2020/08/cpython-implemented-python-time-complexity/</guid>
            <pubDate>Thu, 20 Aug 2020 03:00:00 GMT</pubDate>
            <content:encoded><![CDATA[<blockquote>
<p>🧊 我算法实在太菜了，本文部分内容可能是小学二年级就应该知道的东西，所以如果各位大佬看到这篇文章的话，就当看个乐呵，还请不要嫌弃 555。(⊙﹏⊙)</p>
</blockquote>
<p>这篇文章起源于各大 OJ 平台可能是最经典的一道题：<a href="https://leetcode.com/problems/two-sum">Two Sum（两数之和）</a>。我已经<strong>很久很久</strong>（可能已经有两年了）没做算法题了，由于我之前的项目里面其实很少涉及到一些比较复杂的算法，各个语言和框架的封装也让我对具体程序逻辑的优化放松了警惕，导致我对我写的代码的性能非常不敏感。于是我在前几天准备秒掉 TwoSums 的时候，就出现了极为尴尬的情况（提交顺序自下而上）：</p>
<p><img src="https://cdn.spencer.felinae98.cn/blog/2020/08/200819_223330.png" alt="嗯，第一发正式比赛应该直接就 TLE 了，真实的菜"></p>
<p>感觉自己被自己的水平羞辱到之后，我开始仔细找到 Python 官方文档里面对各个 native 数据结构操作的时间复杂度介绍，并也着手尝试通过巧妙的办法寻找时间优化方法，于是便有了这篇文章。</p>
<h2 id="基础知识">基础知识</h2>
<h3 id="cpython">CPython</h3>
<p>先来介绍一些基础知识。Python 是一门动态语言（Dynamic language），也就是说：Python 在执行过程中是 Python 解释器对每一条 Python 代码进行翻译再运行，而不是像静态语言，比如 C、C++ 等，被编译为 native binary 后再执行的。其中，CPython 就是一个用 C 语言实现的 Python 解释器，也就是我们默认从官网安装 Python 时得到的那个最传统、最经典的 Python 解释器（但是性能表现并不算优秀）。</p>
<h3 id="时间复杂度">时间复杂度</h3>
<p>为了准确的表达或衡量我们算法的执行时间，我们常常采用大 O 表示法，也就是 Big-O notation，来表达时间复杂度。比如 $O(1)$ 表示常数时间、$O(\log n)$ 表示对数时间、$O(n)$ 表示线性时间，等等。</p>
<p><img src="https://cdn.spencer.felinae98.cn/blog/2020/08/200819_230352.png" alt="来自：https://www.bigocheatsheet.com"></p>
<h2 id="cpython-实现的-python-操作的时间复杂度">CPython 实现的 Python 操作的时间复杂度</h2>
<p>CPython 是 Python 官方的解释器，也是我们最为常用的 Python 解释器。我重点研究了官方给出的 CPython 实现的几个 Python 原生数据结构操作的时间复杂度[^1]，本文我把重点放在列表 <code>list</code> 和字典 <code>dict</code> 这两种数据结构上。</p>
<h3 id="列表-list">列表 <code>list</code></h3>
<p>Python 中的列表 <code>list</code> 是一个非常典型的数据结构，也是我们在 Python 里面最常用的一个数据结构。内部实现中，Python 的列表是用动态数组 — dynamic array[^2]来实现的（而非链表）。在 CPython 中，Python 的列表实际上这样的一个 C 语言结构体：</p>
<pre><code class="language-c">typedef struct {
    PyObject_VAR_HEAD;
    PyObject **ob_item;
    Py_ssize_t allocated;
} PyListObject;</code></pre>
<p>其中，<code>**ob_item</code> 是指向列表中每个项目的一组指针，<code>allocated</code> 是为列表在内存中分配的空间格数（可以理解为一个列表项目一格）。</p>
<p>CPython 实现的 Python 列表数据结构决定了 Python 列表各种操作的时间复杂度。很容易理解下面常见的「列表操作」的时间复杂度[^6]：</p>
<ul>
<li>向列表末尾插入元素 <code>.append(x)</code>、移除列表末尾元素 <code>.pop()</code> 的时间复杂度为 $O(1)$；</li>
<li>向列表中任意位置插入元素 <code>.insert(idx)</code>，移除列表中<strong>任意元素</strong> <code>.remove(x)</code>，移除列表中<strong>任意位置的元素</strong> <code>.pop(idx)</code> 的时间复杂度为 $O(n)$；</li>
<li>查询列表包含 / 不包含元素：<code>x in/not in l</code> 时间复杂度为 $O(n)$（线性查找）；</li>
<li>典型的列表排序 <code>.sort()</code> 的时间复杂度为 $O(n\log n)$。</li>
</ul>
<h3 id="字典-dict">字典 <code>dict</code></h3>
<p>字典，也就是 Python 的 dictionaries —— <code>dict</code>，是一种索引数据结构，且是通过 <code>dict</code> 的键 —— <code>keys</code> 进行索引，我们可以将其看作是关联性数组（Associative arrays）。在 CPython 内部，<code>dict</code> 是利用 Hash Table（哈希表）来实现的，也就是说：Python 字典是一个数组，其索引是使用每个 <code>key</code> 上的哈希函数获得的。</p>
<p>另外，实际上在 Hash Table 中，当我们使用一些较为复杂的随机哈希函数时，我们的主要目的是为了让设定的 <code>key</code> 的哈希能够在数组中均匀分布，同时最小化哈希冲突（Hash collisions），避免多个 <code>key</code> 对应同一个哈希值。然而 Python 的哈希函数实现则比较简单，实际也并不具备这种特性，我们可以尝试：</p>
<pre><code class="language-python"># Hash integers
list(map(hash, (0, 1, 2, 3)))
# Hash strings
list(map(hash, (&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;)))</code></pre>
<p><img src="https://cdn.spencer.felinae98.cn/blog/2020/08/200820_000117.png" alt="Python 默认哈希函数对整数和字符串的处理"></p>
<p>不过，Python 在 <code>dict</code> 中所使用的这样的 <code>hash</code> 函数，对于常见的具有连续 <code>key</code> 的 <code>dict</code> 则是被证明非常高效，不容易产生冲突的。当然，冲突还是避免不了，因此 Python 在 <code>dict</code> 实现中为了解决冲突（Collision resolution），会通过「开放地址法」来为冲突地址重新分配[^3]。</p>
<p>好了，讲了这么多，我们的重点还是在于 Python 字典数据结构存储「键值对」的<strong>哈希表 Hash Table 实现</strong>。在实际 CPython（以及 PyPy[^4]）中，Python 字典是使用如下的 C 语言结构体定义的：</p>
<pre><code class="language-c">struct dict {
    long num_items;
    variable_int *sparse_array;
    dict_entry* compact_array;
}

struct dict_entry {
    long hash;
    PyObject *key;
    PyObject *value;
}</code></pre>
<p>其中的 <code>dict_entry</code> 就是存储我们字典中「键值对」的数据结构了。使用 Hash Table 实现的 <code>dict</code> 字典能够极为高效的实现数据存取，也就是说[^6]：</p>
<ul>
<li>根据 <code>key</code> 获取字典中的 <code>value</code> 的 <code>d[k]</code> 或 <code>d.get(k)</code> 时间复杂度均为 $O(1)$；</li>
<li>设定某个 <code>key</code> 的值 <code>value</code> 的 <code>d[k] = v</code> 的时间复杂度也为 $O(1)$；</li>
<li>查询字典中是否包括某个 <code>key</code> 的 <code>k in/not in d</code> 的时间复杂度依旧为 $O(1)$；</li>
<li>甚至于获取整个字典的全部 <code>keys</code> 的 <code>d.keys()</code> 的时间复杂度还是 $O(1)$！</li>
</ul>
<p>大部分 Python 字典的操作时间复杂度都是 $O(1)$ 的，因此使用「字典」作为我们的数据载体往往能够极大的提升我们算法的执行效率。当然，其他语言的 Hash Table（比如 Java 的 <code>HashSet</code>、JavaScript 的 Object）实现同样也有类似的效果。</p>
<h2 id="疏忽导致的性能问题和解决措施">疏忽导致的性能问题和解决措施</h2>
<h3 id="用-index-寻找列表中的元素序号">用 <code>index()</code> 寻找列表中的元素序号</h3>
<p>讲完了 CPython 内部的列表与字典的实现，我们回到最初那道算法题。Two Sum 这道题本身其实非常简单，其中要求返回的是取得的两个数字在原列表中的 index：</p>
<blockquote>
<p>..., <strong>return indices of the two numbers</strong> such that they add up to a specific target.</p>
</blockquote>
<p>当时我并没多思考性能问题，为了简单的用一个函数同时判断数字是否存在于列表中，并返回数字在列表中的 index，我大胆使用 <code>.index()</code>！</p>
<pre><code class="language-python"># ...
try:
    partner_idx = nums.index(partner)
    if idx == partner_idx:
        continue
    return [idx, partner_idx]
except ValueError:
    continue
# ...</code></pre>
<p>Python 列表的 <code>.index(x)</code> 方法会遍历整个列表，并返回所求元素 <code>x</code> 第一次出现的 index，如果没有找到，那么会 raise 一个 <code>ValueError</code>。请大家看，我这里为了能用上 <code>.index()</code> 方法，甚至直接用 try except 异常处理来写，太憨憨了。<code>.index(x)</code> 需要遍历整个列表，因此这部分的时间复杂度是 $O(n)$ 的，整个算法的时间复杂度就趋近于 $O(n^2)$，我的第一发 1224ms 的憨批运行时间也主要耽误在这里了。使用 <code>dict</code> 实现一个查找表，使用 $O(n)$ 的时间，一次遍历即可完成查询：</p>
<pre><code class="language-python"># ...
if num not in lookup_dict:
    lookup_dict[partner] = idx
else:
    return [lookup_dict[num], idx]
# ...</code></pre>
<h3 id="用-dgetk-与-dk-对字典进行访问">用 <code>d.get(k)</code> 与 <code>d[k]</code> 对字典进行访问</h3>
<p>除了上面这个问题以外，在使用 Python 字典的时候我还遇到了这样的问题。我们都知道其实根据键 <code>k</code> 来访问 Python 字典对应的值 <code>value</code> 时，用 <code>d.get(k)</code> 相比直接用 <code>d[k]</code> 算是更为优雅的一种方法[^5]：</p>
<ul>
<li>我们如果直接用 <code>d[k]</code> 访问一个不存在的 <code>k</code>，那么运行时 Python 会 raise 一个 <code>KeyError</code> 的错误，而 <code>d.get(k)</code> 则会优雅的返回 <code>None</code>；</li>
<li>我们可以通过 <code>d.get(k, default_value)</code> 的语法来设定一个默认值，当 <code>k</code> 不存在时返回这一值即可；</li>
</ul>
<p>习惯于「优雅的解决方法一定更好」的我，在算法题里面同样用了 <code>d.get(k)</code> 来获取字典中的 value，但是我发现这样的方法要比同样的 <code>d[k]</code> 慢很多（提交顺序自下而上）：</p>
<p><img src="https://cdn.spencer.felinae98.cn/blog/2020/08/200820_010904.png" alt="80ms 的提交使用的是 .get(k)，而 48ms 的提交使用的是传统方法 d\[k\]"></p>
<p>当然，不用说我们也知道主要因为 Python 的 <code>d.get(k)</code> 实现中需要处理更多的事情，同时 <code>get()</code> 还是一个函数调用，需要入栈出栈，肯定会消耗更多的时间。因此，这里更需要我们注意，如果我们希望编写效率最高的代码，那么一定需要考虑这些会触发性能瓶颈的问题。</p>
<p>:::warning 🍫 注意
但是我这里并不是在说 <code>d.get(k)</code> 方法不好，只是在「时间敏感」的地方，我们可以通过优化这种调用来大幅度提高我们程序的运行效率。
:::</p>
<h2 id="小结">小结</h2>
<p>最初的时候我做算法题大部分都是用 C++ 编写的，从来都没尝试过用像 Python 这种抽象程度这么高的语言来做算法题。因此如果想要追求高性能，除了对数据结构有充分的了解，还需要对自己所使用语言的内部实现有所认知。我们在使用高级语言实现某些算法中，使用很多封装好的方法时，都需要对「算法本身」和「所使用的方法实现」这二者更深刻的认识才能最大化的发挥他们的功力。当然，使用 Python 来写算法题更多的还是快乐，比如：「哎！这也能直接用！那也能直接用！快排都不用我手写了！」开心 ~ 好啦，文章就到这里，感谢大家的阅读！</p>
<p>[^1]: <a href="https://wiki.python.org/moin/TimeComplexity">Python.org Wiki - TimeComplexity</a>
[^2]: <a href="https://stackoverflow.com/questions/3917574/how-is-pythons-list-implemented">StackOverflow - How is Python&#39;s List implemented?</a>
[^3]: <a href="https://github.com/python/cpython/blob/master/Objects/dictobject.c">GitHub - python/cpython - dictobject.c</a>
[^4]: <a href="https://morepypy.blogspot.com/2015/01/faster-more-memory-efficient-and-more.html">Faster, more memory efficient and more ordered dictionaries on PyPy</a>
[^5]: <a href="https://stackoverflow.com/questions/11041405/why-dict-getkey-instead-of-dictkey">StackOverflow - Why dict.get(key) instead of dict[key]?</a>
[^6]: <a href="https://www.ics.uci.edu/~pattis/ICS-33/lectures/complexitypython.txt">Complexity of Python Operations</a></p>
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[VS Code Math to Image：在不支持 LaTeX 的地方插入数学公式]]></title>
            <link>https://blog.spencerwoo.com/2020/08/vscode-math-to-image/</link>
            <guid>https://blog.spencerwoo.com/2020/08/vscode-math-to-image/</guid>
            <pubDate>Tue, 04 Aug 2020 05:50:00 GMT</pubDate>
            <content:encoded><![CDATA[<p>:::note 🛹 同步发布</p>
<ul>
<li><strong>在少数派上阅读本文</strong> — <a href="https://sspai.com/post/61877">不支持 LaTeX 也能插入数学公式，这个小插件帮你实现「徒手渲染」</a>。</li>
<li><strong>Read the English version on Medium</strong> — <a href="https://medium.com/spencerweekly/vs-code-math-to-image-write-latex-math-equations-in-github-markdown-the-easy-way-9fa8b81dc910?source=friends_link&amp;sk=cff035b443fb81f5b20a47370f23aed3">VS Code Math to Image: Write LaTeX Math Equations in GitHub Markdown the Easy Way!</a>
:::</li>
</ul>
<p>作为每天都在跟数学公式打交道的工程师、数学家，如果我们想要将自己的学术成果开源并发布在网络平台上，那么这个平台至少要支持在 Markdown 里面用 LaTeX 撰写公式。可惜，想要直接在 Markdown 里面撰写数学公式就必须借助第三方库的帮助：比如 MathJax 和 KaTeX。</p>
<p>因此很多常见的平台都尚未支持 LaTeX 数学公式的渲染，GitHub 就是其中一员——作为可能是世界上最大、最受欢迎的代码开源平台，GitHub 是很多研究人员开源自己学术成果的首选。如果我们想要在 GitHub 的 README 等 Markdown 文件里面撰写数学公式，就必须寻找别的办法。</p>
<p>非常幸运的是：GitHub 允许我们在 Markdown 里面直接插入 SVG 图片（以及 HTML）。这也就意味着，我们可以先将手上的 LaTeX 数学公式渲染成 SVG，再手动将原来的数学公式替换成 SVG 图片，<strong>达成「徒手渲染 LaTeX 公式」的成就。</strong>💪</p>
<p><img src="https://cdn.spencer.felinae98.cn/blog/2020/08/200804_132840.png" alt="将 LaTeX 公式渲染为 SVG 后在 GitHub 上面的显示效果"></p>
<p>如何才能实现并自动化整个过程呢？为了尽量减少各位的麻烦，提升工作效率，我和同学专门编写了一个 VS Code 插件，用来自动将我们在 VS Code 里面选择的数学公式转换成 SVG 图片，并自动嵌入原位置 —— <strong>VS Code Math to Image：</strong></p>
<ul>
<li>目前，项目已经开源在 GitHub 上面：<a href="https://github.com/TeamMeow/vscode-math-to-image">GitHub — vscode-math-to-image</a>；</li>
<li>同时 VS Code 插件也发布到了 Visual Studio Marketplace：<a href="https://marketplace.visualstudio.com/items?itemName=MeowTeam.vscode-math-to-image">VS Marketplace — Math to Image</a>；</li>
</ul>
<h2 id="插件安装">插件安装</h2>
<p>我们在 VS Code 的插件市场搜索 <code>vscode-math-to-image</code>，找到 Math to Image 点击安装即可。直接访问 <a href="https://marketplace.visualstudio.com/items?itemName=MeowTeam.vscode-math-to-image">Visual Studio Marketplace</a> 点击 Install 也可以安装。</p>
<p><img src="https://cdn.spencer.felinae98.cn/blog/2020/08/200804_134114.png" alt="VS Code 插件市场搜索：vscode-math-to-image"></p>
<h2 id="插件使用">插件使用</h2>
<p>我们成功安装插件之后，就可以直接非常方便地用插件将公式转换成可以显示在 GitHub 里面的 SVG 图片啦：</p>
<p><img src="https://cdn.spencer.felinae98.cn/blog/2020/08/200804_133321.gif" alt="使用 VSCode Math to Image 插件将公式渲染为 SVG"></p>
<p>具体来讲，比如上面演示动画里面的公式：</p>
<pre><code class="language-latex">$$
\ell = \sum_{i}^{N}(y_i - \hat{y}_i)^2 - ||w||_2^2
$$</code></pre>
<p>$$
\ell = \sum_{i}^{N}(y_i - \hat{y}_i)^2 - ||w||_2^2
$$</p>
<p>正常在 Markdown 中我们是会直接用上面的语法让支持渲染 LaTeX 的编辑器渲染公式的，比如 VS Code 默认的 Markdown 预览视图就支持 LaTeX 数学公式的渲染。如果这一 Markdown 文件里面的公式需要在 GitHub 上显示，那么我们就需要用插件将公式渲染成 SVG。我们直接选择这三行 LaTeX 公式代码（包含第一行和第三行的 <code>$$</code>），右键并在菜单中选择「Math » Image: Insert rendered equation (local)」或者「Math » Image: Insert rendered equation (remote)」，之后插件就会将我们选中的公式自动注释掉，并在下方添加相应的 SVG 图片引用。（插件里面 <code>local</code> 和 <code>remote</code> 两个选项工作原理不同，请见后文。）</p>
<p><img src="https://cdn.spencer.felinae98.cn/blog/2020/08/200804_133228.png" alt="选择公式右键后在菜单中选择渲染选项"></p>
<p>此时，我们可以发现在 VS Code 的 Markdown 预览界面里面，SVG 图片形式的 LaTeX 公式依旧能够成功渲染，同时这一公式在 GitHub 等仍然不支持 LaTeX 的平台也能正常展现，nice！</p>
<h2 id="工作原理">工作原理</h2>
<p>所以，我们插件是如何将一个纯文本格式的 LaTeX 数学公式渲染为 SVG 图片的呢？前面提到的 <code>remote</code> 和 <code>local</code> 又有怎样的区别呢？这里，我就简单讲解一下插件的渲染和工作原理。</p>
<p>首先，插件的工作原理非常简单、极易理解，我们实际上就是通过某种方式，解析 LaTeX 撰写的数学公式并将之渲染为 SVG， <code>remote</code> 和 <code>local</code> 的不同之处就在于：</p>
<ul>
<li>前者是借助服务器渲染公式，生成的 SVG 存在于云端；</li>
<li>后者是直接在本地渲染公式，生成的 SVG 当然也保存于本地；</li>
</ul>
<p><img src="https://cdn.spencer.felinae98.cn/blog/2020/08/200804_133452.png" alt="VSCode Math to Image 插件的简单工作原理"></p>
<h3 id="将公式借助服务器渲染为云端-svg-图片">将公式借助服务器渲染为云端 SVG 图片</h3>
<p>虽然 GitHub 的 README 等 Markdown 文件里面并不支持 LaTeX 数学公式的渲染，但是：<strong>GitHub 确实会在 Jupyter notebook 里面解析并正常渲染 LaTeX 数学公式！</strong>那既然 GitHub 并没有引用第三方的渲染服务，它们究竟是怎样渲染数学公式的呢？答案是：GitHub 有自己的 LaTeX 渲染服务器，我们给这一服务器一个格式正确的 LaTeX 公式，服务器会直接给我们返回渲染好的 SVG 图片。</p>
<p>嚯，我们要的正好就是这个服务啊！所以，我们插件的 <code>remote</code> 选项，就是借助 GitHub 官方的 LaTeX 渲染服务器，将我们的 LaTeX 数学公式转换为云端 SVG 图片。简单来说，比如下面这个标准正态分布公式：</p>
<pre><code class="language-latex">$$
P(x) = \frac{1}{\sigma\sqrt{2\pi}} e^{\frac{-(x-\mu)^2}{2\sigma^2}}
$$</code></pre>
<p>$$
\Large P(x) = \frac{1}{\sigma\sqrt{2\pi}} e^{\frac{-(x-\mu)^2}{2\sigma^2}}
$$</p>
<p>我们可以借助插件直接将其转换为用 GitHub 服务器渲染的 SVG 图片，并用 <code>&lt;img&gt;</code> 标签插入 Markdown 之中：</p>
<pre><code class="language-html">&lt;div align=&quot;center&quot;&gt;&lt;img src=&quot;https://render.githubusercontent.com/render/math?math=P(x)%20%3D%20%5Cfrac%7B1%7D%7B%5Csigma%5Csqrt%7B2%5Cpi%7D%7D%20e%5E%7B%5Cfrac%7B-(x-%5Cmu)%5E2%7D%7B2%5Csigma%5E2%7D%7D%0D&quot;&gt;&lt;/div&gt;</code></pre>
<p>这样，我们原公式就被替换为用 GitHub 服务器渲染好的 SVG 图片：</p>
<figure>
  <img src="https://cdn.spencer.felinae98.cn/blog/2020/08/200804_134758.png" alt="标准正态分布公式（GitHub LaTeX 渲染服务器渲染得到的 SVG 图片）" width="300px" >
  <figcaption>标准正态分布公式（GitHub LaTeX 渲染服务器渲染得到的 SVG 图片）</figcaption>
</figure>

<p>同时，由于渲染服务器正好是 GitHub 自己的，所以肯定不会出现被屏蔽、无法访问的现象，非常靠谱。</p>
<h3 id="将公式在本地渲染并直接保存为-svg-图片">将公式在本地渲染并直接保存为 SVG 图片</h3>
<p>SVG 这一格式非常强大，不仅是矢量图的格式标准，我们还可以在其中添加动画等高级功能。所以实际上，SVG 格式的图片会给网站带来一些安全隐患，也正因为 SVG 的这一特点，导致并非所有的地方都支持外部 SVG 的引用。为了规避这一问题，也为了让 SVG 文件的存在更可控（服务器还是有宕机的风险），我们也实现了 LaTeX 公式「本地渲染」的功能。</p>
<p>我们插件的 <code>local</code> 功能实际上就是借助 MathJax 在本地直接将 LaTeX 公式渲染为本地 SVG 图片。这样，我们就可以将这个保存于本地的 SVG 直接在 Markdown 文件里面引用显示，同样方便。</p>
<p>继续用上面标准正态分布的公式，我们可以将公式渲染为存储于本地的 SVG：<code>svg/e40qQ5G9jw.svg</code>，并直接保存在 Markdown 文件相邻的一个 <code>svg</code> 文件夹中。这样，我们就可以继续用 <code>&lt;img&gt;</code> 标签将这个本地 SVG 插入我们的 Markdown 之中：</p>
<pre><code class="language-html">&lt;div align=&quot;center&quot;&gt;&lt;img src=&quot;svg/e40qQ5G9jw.svg&quot;/&gt;&lt;/div&gt;</code></pre>
<figure>
  <img src="https://cdn.spencer.felinae98.cn/blog/2020/08/200804_134658.png" alt="标准正态分布（本地使用 MathJax 渲染得到的 SVG 图片）" width="300px" >
  <figcaption>标准正态分布（本地使用 MathJax 渲染得到的 SVG 图片）</figcaption>
</figure>

<h2 id="小结">小结</h2>
<p>以上就是 VS Code Math to Image 这个 VS Code 插件的简单介绍，两种方法（<code>remote</code> 和 <code>local</code>）都可以帮我们渲染出高质量的 SVG 公式图片，这样我们就可以解决 GitHub 等平台不支持数学公式渲染的一大难题啦！如果你觉得我们的插件非常有用，那么 <a href="https://github.com/TeamMeow/vscode-math-to-image">一定要去 GitHub 给我们点上一个 Star</a>，如果 <a href="https://marketplace.visualstudio.com/items?itemName=MeowTeam.vscode-math-to-image&amp;ssr=false#review-details">能去 VS Marketplace 给我们个五星好评</a> 那就更棒啦。</p>
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Quit using nvm：快删掉这个占据 Zsh 启动时间一半的怪物！]]></title>
            <link>https://blog.spencerwoo.com/2020/07/remove-nvm-to-speed-up-zsh/</link>
            <guid>https://blog.spencerwoo.com/2020/07/remove-nvm-to-speed-up-zsh/</guid>
            <pubDate>Tue, 28 Jul 2020 06:30:00 GMT</pubDate>
            <content:encoded><![CDATA[<blockquote>
<p>✍ Sept 5. 2020 的更新：</p>
</blockquote>
<p>今天尝试了一下 <a href="https://github.com/zdharma/zinit">zinit</a>，这个作者跟 Powerlevel10k 的想法非常类似，都是对 Zsh 的插件进行懒加载，导致其 Turbo mode 加载 Zsh 速度快的飞起！</p>
<p><img src="https://cdn.spencer.felinae98.cn/blog/2020/09/200905_212629.png" alt="使用 zinit 加载插件，启动时间仅需 0.6s，但实际上打开瞬间 Zsh 就已经可以交互了"></p>
<p>另外，看到 <code>zplug</code> 的 GitHub 上一次提交时间还是今年 2 月份，而且 issue 区已经出现寻找 Maintainer 的请求了，而 <code>zinit</code> 最近更新得非常频繁，因此这里推荐大家使用 <code>zinit</code> 作为 Zsh 的插件管理器。</p>
<hr>
<blockquote>
<p>🍳 July 28. 2020 的原文章：</p>
</blockquote>
<p>我实在是受不了了。我这 10 代 i7 的顶配 ThinkPad 在 WSL 2 里面打开一个 Shell，竟然每次都需要在心里面默念 2 个数才能敲进去字。淦啊 (╬▔皿▔)╯</p>
<h2 id="我管理-zsh-配置的方法">我管理 Zsh 配置的方法</h2>
<p>对了，得先跟大家说说，我还在用 Zsh，但是我丢掉了 Oh My Zsh 这个好像大家都在用的 Zsh 框架，转而使用更加灵活的 <a href="https://github.com/zplug/zplug">zplug</a> 来管理我的 Zsh 配置。如果你用 Vim 和 <code>vim-plug</code>，那么 <code>zplug</code> 用起来的感觉将非常熟悉：<code>zplug</code> 跟 <code>vim-plug</code> 的设计风格就非常相似。当然，<code>zplug</code> 最吸引我的一点还是「高度的可自定义」。不像 Oh My Zsh 把 Zsh 所有配置都为我们设定好了，<code>zplug</code> 支持用「插件」的方式安装、配置 Zsh 的各项功能，甚至可以像安装插件一样安装 Oh My Zsh 的部分功能。</p>
<p>比如，Oh My Zsh 的 Git 插件和预设 alias 们就很好用啊，那我直接就能用 <code>zplug</code> 装上：</p>
<pre><code class="language-bash">zplug &quot;plugins/git&quot;, from:oh-my-zsh
zplug &quot;plugins/common-aliases&quot;, from:oh-my-zsh</code></pre>
<p>自动补全 <code>zsh-completions</code> 和类似 Fish shell 的自动命令建议 <code>zsh-auto-suggestions</code> 也非常有用啊，那我也立刻拿 <code>zplug</code> 装上：</p>
<pre><code class="language-bash">zplug &quot;zsh-users/zsh-completions&quot;
zplug &quot;zsh-users/zsh-autosuggestions&quot;</code></pre>
<p>可以看到，<code>zplug</code> 胜在灵活，自定义程度高，所以我才舍 Oh My Zsh 而取 <code>zplug</code>。另外，<code>zplug</code> 还支持 parallel update，这点也非常讨我欢心：</p>
<p><img src="https://cdn.spencer.felinae98.cn/blog/2020/07/20200727-221325.gif" alt="zplug update 支持多个插件同时并行更新"></p>
<p>好了，这篇文章不光是吹 <code>zplug</code> 多好，而是为了找到到底哪个混蛋在耽误我 Zsh 的启动速度。</p>
<h2 id="对-zsh-启动时间进行测量">对 Zsh 启动时间进行测量</h2>
<p>为了定量的衡量 Zsh 的启动过程，我们先建立一个 baseline：测量在当前没有任何插件调整情况下 Zsh 的「冷启动」时间。</p>
<p>我使用了下面的命令来测量 Zsh 启动时间：</p>
<pre><code class="language-bash">time zsh -i -c exit</code></pre>
<p>未经调整的 Zsh 启动时间数据如下：</p>
<p><img src="https://cdn.spencer.felinae98.cn/blog/2020/07/20200727-221927.png" alt="初始 Zsh 启动时间"></p>
<p>最后一行可以看到，总时间用了 1.93s，多次启动得到的数据类似，1.93s 也符合上面我人肉感知的「心中默念两个数」的时间。</p>
<p>要知道，为了让 Zsh 更快的显示，我可是直接用上了 Powerlevel10k 这个地表最快，连作者都疯狂优化的 Zsh prompt 主题框架。用 Powerlevel10k 渲染的 Zsh prompt 显示速度可以说是优化到了极致，<a href="https://github.com/romkatv/powerlevel10k#uncompromising-performance"><strong>提供 uncompromising performance</strong></a>。但是接近 2s 的冷启动时间还是令人难受，而这显然不是 Powerlevel10k 的锅。</p>
<h2 id="深入评估-zsh-冷启动过程中的时间使用">深入评估 Zsh 冷启动过程中的时间使用</h2>
<p>经过一番搜索，我发现 Zsh 内部就有一个能够 benchmark 并 profile Zsh 自己启动过程时间使用的工具：<code>zprof</code>。如果你学过软件工程，你应该知道评价软件质量的一个重要工具：Profiler，用于衡量软件各个部分各个模块具体执行时间的评测工具。</p>
<blockquote>
<p>A <em>profile</em> is a set of statistics that describes how often and for how long various parts of the program executed. <a href="https://docs.python.org/3/library/profile.html">^1</a></p>
</blockquote>
<p>常见的语言环境都有原生的 Profiler，比如 Python 内置的 <code>cProfile</code>、Node.js 内置的功能 <code>node --prof</code>……部分 IDE 比如 Visual Studio 也有类似的工具，这些 Profiler 在优化软件的执行速度上起到了举足轻重的作用。</p>
<p>我们用 <code>zprof</code> 来对 Zsh 进行 Profile 评估：</p>
<ul>
<li>在 <code>.zshrc</code> 的最开头新增一行并写入 <code>zmodload zsh/zprof</code>；</li>
<li>在 <code>.zshrc</code> 文件末尾添加一行再写入 <code>zprof</code>；</li>
<li>保存 <code>.zshrc</code> 再重启我们的 Zsh Shell（关闭再打开终端）；</li>
</ul>
<p>添加了 <code>zprof</code> 必要命令后，重新打开 Zsh 时 <code>zprof</code> 会开始自动对 Zsh 启动过程中各个过程所用时间进行测算，最终得到类似这样的报告：</p>
<p><img src="https://cdn.spencer.felinae98.cn/blog/2020/07/20200727-224130.png" alt="zprof 测量 Zsh 冷启动过程各个功能模块执行所用时间报告"></p>
<p>看前几行的 <code>nvm_die_on_prefix</code>、<code>nvm</code>、<code>nvm_auto</code> 和 <code>nvm_ensure_version_installed</code>，它们依次占用了 17.82%、16.34%、15.18% 和 4.80% 的启动时间，<code>nvm</code> 相关的模块一共占据了我 Zsh 启动时间的一半以上。原来是你，<code>nvm</code>！(ノ｀Д)ノ</p>
<h2 id="删掉-nvm？">删掉 <code>nvm</code>？</h2>
<p>显然，删掉 <code>nvm</code> 看起来应该是我们最显而易见、一劳永逸的解决方案，根据上面的数据，删掉 <code>nvm</code> 或者不让 <code>nvm</code> 在 Zsh 启动时加载大概率能节省一半的启动时间。后者被称为「懒加载」，也就是我们常说的 lazy loading。不过我 Node.js 环境用的还是挺多的，同时 <code>nvm</code> 也是出了名的慢，而 <code>nvm</code> 市面上的替代品还是挺多的，所以咱们先删掉再说。</p>
<p><code>nvm</code> 实际上仅是一个帮我们管理 Node.js 版本的 Bash 脚本，<code>.zshrc</code> 中 <code>nvm</code> 相关的加载不多，只有这些：</p>
<pre><code class="language-bash">export NVM_DIR=&quot;$HOME/.nvm&quot;
[ -s &quot;$NVM_DIR/nvm.sh&quot; ] &amp;&amp; \. &quot;$NVM_DIR/nvm.sh&quot; # This loads nvm
[[ -r $NVM_DIR/bash_completion ]] &amp;&amp; \. $NVM_DIR/bash_completion</code></pre>
<p>将我 <code>.zshrc</code> 中加载 <code>nvm</code> 的这部分删掉后，重新对 Zsh 的冷启动时间进行测量，得到这样的结果：</p>
<p><img src="https://cdn.spencer.felinae98.cn/blog/2020/07/20200727-221837.png" alt="删除 nvm 优化后 Zsh 启动时间"></p>
<p>哟，直接降到 1s 以内了，跟我们设想预期完全一致。拜拜了您嘞 <code>nvm</code>！</p>
<pre><code class="language-bash">rm -rf ~/.nvm</code></pre>
<h2 id="那我后面用什么来管理安装-nodejs？">那我后面用什么来管理安装 Node.js？</h2>
<p>好了，删掉了 <code>nvm</code>，我们后面用什么呢？这里我推荐一个设计更精良，安装更合理的 Node.js version manager：<a href="https://github.com/tj/n">n - Interactively Manage Your Node.js Versions</a>。基本的使用方法跟 <code>nvm</code> 其实非常相似，但是 <code>n</code> 不用往我们 <code>.zshrc</code> 里面加一些奇奇怪怪的执行命令，最多只需要一个 <code>N_PREFIX</code> 的环境变量来定义 <code>n</code> 安装目录。轻量简便，推荐使用！</p>
<p>推荐大家用 <a href="https://github.com/mklement0/n-install">n-install</a> 来在 Linux 和 macOS 上安装 <code>n</code>：</p>
<pre><code class="language-bash">curl -L https://git.io/n-install | bash</code></pre>
<p><code>n-install</code> 可以自动帮我们在 <code>$HOME</code> 文件夹下创建 <code>n</code> 所使用的安装目录，并将环境变量替我们设定完整，应该是目前为止最方便的 <code>n</code> 安装方法。</p>
<p>:::important 🎍 阅读更多
有关 <code>n-install</code> 的更多使用细节（包括安装、更新、卸载……）请参考 <code>n-install</code> 官方仓库：<a href="https://github.com/mklement0/n-install">mklement0/n-install</a>。
:::</p>
<p>安装成功 <code>n</code> 之后，我们就可以像往常一样，安装使用多个版本的 Node.js 啦。</p>
<h2 id="使用-wsl-同学的注意事项">使用 WSL 同学的注意事项</h2>
<p>在上面用 <code>n-install</code> 安装 <code>n</code> 的时候，在 WSL 里面执行时，我发现了一个很憨批的问题。<code>n-install</code> 会检测当前系统 <code>$PATH</code> 中是否已经有 <code>n</code>、Node.js 或者其他相关的二进制文件，如果发现就会报错：</p>
<pre><code>Aborting, because n and/or Node.js-related binaries are already in the $PATH.</code></pre><p>而 WSL 默认情况下会将 Windows 的 <code>$PATH</code> 一并 append 到自己的 <code>$PATH</code> 里面，当然这样做无可厚非，毕竟这样可以让我们直接在 WSL 里面调用比如 <code>clip.exe</code>、<code>explorer.exe</code> 等 Windows 可执行文件。但是，由于我 Windows 里面也安装了 Node.js、yarn 等等，导致 <code>n-install</code> 检测到 WSL 的 <code>$PATH</code> 包含这些内容，拒绝安装。（在 WSL 中我们可以用 <code>echo $PATH</code> 来查看当前 <code>$PATH</code> 中包含哪些路径，大概率包含许多 <code>/mnt/c/xxx</code> 的路径，这些就是 Windows 的可执行文件路径。）</p>
<p>这一情况就要我们自己来修改 WSL 的 <code>$PATH</code> 了。为了后续工作的顺利开展，我直接利用 <code>/etc/wsl.conf</code> 来设定 WSL 的 <code>$PATH</code> 中默认不包含 Windows <code>$PATH</code>：</p>
<pre><code>[interop]
appendWindowsPath = false</code></pre><p>重启 WSL 环境（在 Windows 中用命令 <code>wsl --shutdown</code>），再次 <code>echo $PATH</code>，我们就会得到非常干净的纯 WSL 的 <code>$PATH</code>。这样我们即可用 <code>n-install</code> 顺利安装 <code>n</code> 了。</p>
<p>不过，这样设定后，我们就无法继续在 WSL 中直接运行 Windows 的可执行文件了。别慌！我们手动将 Windows 中所需要的几个可执行文件添加到 WSL 的 <code>$PATH</code> 里面即可。常见的几个 Windows 系统可执行文件的目录位于：</p>
<table>
<thead>
<tr>
<th align="left">工具名称</th>
<th align="left">可执行文件</th>
<th align="left">WSL 路径</th>
</tr>
</thead>
<tbody><tr>
<td align="left">Windows 剪贴板</td>
<td align="left"><code>clip.exe</code></td>
<td align="left"><code>/mnt/c/WINDOWS/system32</code></td>
</tr>
<tr>
<td align="left">Windows 资源管理器</td>
<td align="left"><code>explorer.exe</code></td>
<td align="left"><code>/mnt/c/WINDOWS</code></td>
</tr>
<tr>
<td align="left">VS Code 的 <code>code</code> 命令</td>
<td align="left"><code>code.exe</code></td>
<td align="left"><code>/mnt/c/Users/&lt;YOUR WINDOWS USERNAME&gt;/AppData/Local/Programs/Microsoft VS Code/bin</code></td>
</tr>
</tbody></table>
<p>我们依次将我们所需要的这些路径在 <code>.zshrc</code> 中重新添加到 WSL 的 <code>$PATH</code> 即可：</p>
<pre><code class="language-bash"># Manually add Windows explorer and clipboard executables etc. to Linux $PATH
export PATH=&quot;$PATH:/mnt/c/WINDOWS:/mnt/c/WINDOWS/system32&quot;
export PATH=&quot;$PATH:/mnt/c/Users/Spencer/AppData/Local/Programs/Microsoft VS Code/bin&quot;</code></pre>
<h2 id="小结">小结</h2>
<p>文章到这里就介绍完毕啦，<strong>这里我只是为大家提供给 Zsh 启动过程进行时间测量和 profile benchmark 的标准方法</strong>，如果各位也想加速自己 Zsh 的启动过程，那么可能除了删掉 <code>nvm</code> 换用 <code>n</code>，还需要结合自己的实际情况，删除、懒加载部分插件或工具。个人认为优化到 1s 以内就是比较合理的、可以接受的冷启动时间啦。就酱，感谢阅读。(<em>/ω＼</em>)</p>
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Yadm：我是如何同步并管理我的 Dotfiles 的？]]></title>
            <link>https://blog.spencerwoo.com/2020/07/how-i-manage-my-dotfiles/</link>
            <guid>https://blog.spencerwoo.com/2020/07/how-i-manage-my-dotfiles/</guid>
            <pubDate>Fri, 24 Jul 2020 13:45:00 GMT</pubDate>
            <content:encoded><![CDATA[<p>没想到啊，<a href="https://github.com/spencerwooo/dotfiles" title="我的 dotfiles 仓库">我的 dotfiles 仓库</a>竟然是目前我 GitHub 上面星星数量最多的单仓库。</p>
<p>Dotfiles 顾名思义，就是我们在使用软件的时候，软件为了存储我们个人偏好设置而建立的一个以 <code>.</code> 开头的文件。</p>
<blockquote>
<p>User-specific application configuration is traditionally stored in so called <strong><em>dotfiles.</em></strong> <a href="%5Bhttps://wiki.archlinux.org/index.php/Dotfiles%5D(https://wiki.archlinux.org/index.php/Dotfiles)">^1</a></p>
</blockquote>
<p>比如，编辑器 Vim 有 <code>.vimrc</code>，常用的 Zsh、Bash 等 Shell 分别有 <code>.zshrc</code>、<code>.bashrc</code> 等等。另外，广义的 dotfiles 也包括 <code>JSON</code>、<code>TOML</code> 等常规配置文件（当然也包含 Neovim 的 <code>init.vim</code> 等等）。总之，这么多的 dotfiles 都是我们第一次配置安装好系统、软件之后存在于我们电脑上面的个性化配置文件。但是这些 dotfiles 往往都只存在于我们电脑上独一份，一旦重置系统、更新换代，新电脑上就有需要重新配置一遍所有的 dotfiles 才能还原我们原本的设置。手动配置这些内容一想就非常繁琐，为了最快让新电脑能够用上我们熟悉的工具和配置，也为了对我们自己的配置进行版本控制，我们需要用工具来管理我们的 dotfiles。</p>
<h2 id="用-git-的思想来管理-dotfiles">用 Git 的思想来管理 dotfiles</h2>
<p>如果说要管理 dotfiles，我的第一反应就是用 Git 来管理，<strong>毕竟专业的版本控制工具，用来管理纯文本文件的修改和同步再合适不过了。</strong>现在前面提到的那个星星数量最多的 repo 就是我最初用 Git 来将我的 dotfiles 集合到一个 GitHub 仓库里面时所做的尝试。那个仓库现在来看其实比较混乱，由于是一个标准的 Git 仓库，所有的文件都在当前文件夹下，因此为了三个系统的 dotfiles 都同步于一个仓库里，我建立了三个不同的文件夹：macOS、Windows 和 Linux。同时我也得手动将我本地的实际修改复制粘贴到这一 dotfiles 仓库之中。</p>
<p><img src="https://cdn.spencer.felinae98.cn/blog/2020/07/20200724-173035.png" alt="我之前利用 Git 手动同步的 Dotfiles 仓库"></p>
<p>那么这样管理的弊端显而易见：这一仓库仅仅算是「一个配置参考」，<strong>而非实际意义上的 portable dotfiles repository。</strong>多机同步靠粘贴，版本控制靠脑袋，有时候我的工具或者配置更换了，只能徒手把原来的配置移动到 <code>/archive</code> 文件夹下……这样管理一点也不优雅！如果我们可以直接在每个 dotfile 所在的原本位置就将其签入 Git 系统进行版本控制和远程同步就好了。🎈</p>
<h2 id="yet-another-dotfiles-manager">Yet Another Dotfiles Manager</h2>
<p><code>yadm</code> —— Yet Another Dotfiles Manager，恰好就是为管理 dotfiles 而生的一个工具。利用 <code>yadm</code>，我们可以在每个 dotfile 的原始位置，用超出原生 Git 所支持的功能，来任意的管理、同步我们的 dotfiles。</p>
<ul>
<li><code>yadm</code> 的官方 GitHub 地址位于：<a href="https://github.com/TheLocehiliosan/yadm">TheLocehiliosan/yadm</a></li>
<li><code>yadm</code> 的官方介绍于文档位于：<a href="https://yadm.io/">Yet Another Dotfiles Manager</a></li>
</ul>
<p>首先，实际上 <code>yadm</code> 的底层依旧是 Git，也就是说，我们在用 <code>yadm</code> 管理 dotfiles 的时候，实际上依旧是用 Git 来进行版本控制、远程同步等等操作的。但是，<code>yadm</code> 在 Git 的基础之上，进行了合理的功能拓展：<code>yadm</code> 能够让我们不必被一个 Git 仓库文件夹所限制，不必将 <code>$HOME</code> 目录下的全部非相关文件放入 <code>.gitignore</code>，能够直接管理同步 dotfile 文件。</p>
<p>总之，如果我们使用 <code>yadm</code> 而不是直接用 Git 来管理 dotfiles 的话：</p>
<ul>
<li>我们不必关心当前目录是否是 Git 目录（非 Git 初始化的目录原版 Git 是拒绝工作的：<code>fatal: not a git repository</code>）；</li>
<li>我们不必将原 dotfile 的位置进行移动，也不必将原 dotfile 通过软链接 symlink 到其他位置来统一管理；</li>
<li>但我们依旧可以直接使用 Git 的全部功力，包括 Git 分支、merge、rebase、使用 submodules 等等；</li>
</ul>
<p>不过，<code>yadm</code> 是一个 *NIX 工具，如果你使用 macOS 或者 Linux，那么没问题，你可以直接用 <code>yadm</code> 管理 dotfiles；但是如果你用 Windows，那么很遗憾，<code>yadm</code> 只能在 WSL 里面进行安装。（不过没事！还是有解决办法的！）</p>
<h2 id="用-yadm-管理、同步并自动部署-dotfiles">用 yadm 管理、同步并自动部署 dotfiles</h2>
<p>到这里，我们终于正式开始用 <code>yadm</code> 管理同步 dotfiles 了。首先，如果你还没有安装，那么根据你使用的操作系统，先<a href="https://yadm.io/docs/install">按照官方安装文档</a>安装 <code>yadm</code>。接下来，我按照「我已经有大量 dotfiles，但是还没有统一进行系统管理」的情况，来简单介绍 <code>yadm</code> 的使用和功能。</p>
<h3 id="用-github-仓库同步-dotfiles-文件">用 GitHub 仓库同步 dotfiles 文件</h3>
<p>第一步，在 GitHub 上创建一个<strong>空的私有仓库</strong>（建议除非特别自信，还是用私有仓库比较保险），用作我们整个系统 dotfiles 的同步仓库。</p>
<p>:::caution 🍌 注意
建议这里如果是从头开始，<strong>直接创建完全空白，不带 README 或 LICENSE 的仓库为宜。</strong>
:::</p>
<p>之后，在我们的根目录（即 <code>~/</code>、<code>$HOME</code>）下，直接初始化 <code>yadm</code> 仓库并添加我们需要管理的 dotfile 文件：</p>
<pre><code class="language-bash"># 初始化 yadm 仓库（在哪里都一样噢！）
yadm init

# 用 yadm 添加 dotfile 文件至仓库
yadm add &lt;某个 dotfile 文件&gt;

# 继续添加……

# 最后，将添加好的 dotfile 签入 Git
yadm commit -m &quot;&lt;写个 commit 信息&gt;&quot;</code></pre>
<p>这样，本地的 <code>yadm</code> 仓库就已经用 Git 存储好了我们 dotfile 文件的全部信息，以及最重要的：相对于 <code>$HOME</code> 的文件路径（实际上就是绝对路径）。接下来，我们将本地仓库跟刚刚 GitHub 上面创建好的远程仓库进行关联，并将本地更新推送至远程：</p>
<pre><code class="language-bash">yadm remote add origin &lt;刚刚建立的 GitHub 远程仓库地址&gt;
yadm push -u origin master</code></pre>
<p>至此，我们就像平日里用 Git 一样将我们分散在系统各个地方的 dotfiles 统一签入了 <code>yadm</code> 管理的「专属 dotfiles 仓库」里面。我们也可以直接将 <code>yadm</code> 理解成一个在任意目录下都可以直接使用的 dotfiles 专属 Git 版本控制工具。用 <code>yadm list</code> 可以看到我们使用 <code>yadm</code> 管理的全部 dotfiles 列表，当然，<code>yadm status</code> 跟 <code>git status</code> 一样可以查看每个 dotfile 文件的修改情况。</p>
<p><img src="https://cdn.spencer.felinae98.cn/blog/2020/07/20200724-202658.png" alt="命令：yadm list 和 yadm status"></p>
<h3 id="借助-yadm-的-bootstrap-功能自动配置环境">借助 yadm 的 bootstrap 功能自动配置环境</h3>
<p>很多情况下，光用 <code>yadm</code> 将 dotfiles 同步到本地是不够的，拿到新电脑我们还得安装许多工具。这时候，我们即可借助 <code>yadm</code> 的 bootstrap 功能，自动将任务脚本的执行 hook 在 <code>yadm</code> 克隆之后，完成环境的全自动部署。不过，<code>yadm</code> 的 bootstrap 脚本是需要我们自己撰写的，默认位于 <code>$HOME/.config/yadm/bootstrap</code>，这里不论是 Bash 脚本、Python 脚本还是什么别的，只要是可执行文件就可以。</p>
<p>这里我给我自己简单写了一个 Bash 脚本，自动安装 Zsh、<a href="https://github.com/zplug/zplug">zplug</a> 和 Neovim 三个软件 / 插件。具体代码如下：</p>
<pre><code class="language-bash">#!/bin/sh

system_type=$(uname -s)

if [ &quot;$system_type&quot; = &quot;Linux&quot; ]; then
  # install zsh, zplug, neovim
  if ! command -v zsh &gt; /dev/null 2&gt;&amp;1; then
    echo &quot;Installing zsh...&quot;
    sudo apt install zsh
  elif [ ! -f ${HOME}/.zplug/init.zsh ]; then
    echo &quot;Installing zplug...&quot;
    curl -sL --proto-redir -all,https https://cdn.jsdelivr.net/gh/zplug/installer/installer.zsh | zsh
  elif ! command -v nvim &gt; /dev/null 2&gt;&amp;1; then
    echo &quot;Installing neovim...&quot;
    sudo apt install neovim
    # sudo apt install python-neovim
    # sudo apt install python3-neovim
    echo &quot;Installing neovim plugins with vim-plug...&quot;
    nvim &quot;+PlugUpdate&quot; &quot;+PlugClean!&quot; &quot;+PlugUpdate&quot; &quot;+qall&quot;
  else
    echo &quot;All packages are installed.&quot;
  fi
fi</code></pre>
<p>这里必须注意的是：</p>
<ul>
<li>我们所编写的自动 bootstrap 脚本最好是「幂等」的，简单来讲就是有对软件当前存在状态的检测，以免发生重复安装、覆盖原始配置的问题；</li>
<li>我们最好先对当前操作系统进行检测，比如 macOS 和 Linux 实际上环境有很大不同，需要区别处理；</li>
</ul>
<p>我们可以用 <code>yadm bootstrap</code> 命令来手动执行与测试这一脚本。当然，我们编写的 bootstrap 可执行文件，同样可以用 <code>yadm</code> 签入并进行同步，和普通 dotfile 文件一样处理即可。</p>
<p>更多有关 <code>yadm</code> 的 bootstrap 功能请参考：<a href="https://yadm.io/docs/bootstrap">yadm Docs - Bootstrap</a>.</p>
<h3 id="其他功能">其他功能</h3>
<p>除了上面简单的 dotfiles 同步与版本管理、bootstrap 自动环境部署等功能外，<code>yadm</code> 还可以：</p>
<ul>
<li>对敏感文件（比如 SSH 密钥、SSH <code>config</code> 文件……）进行加密、解密，提供私有仓库以外的额外一层保护：<a href="https://yadm.io/docs/encryption">yadm Docs - Encryption</a>；</li>
<li>针对不同的操作系统、不同的环境以及不同的电脑维护不同种类的同一软件 / 插件的 dotfile 文件：<a href="https://yadm.io/docs/alternates">yadm Docs - Alternate Files</a>、<a href="https://yadm.io/docs/templates">yadm Docs - Templates</a>；</li>
</ul>
<p>这两个功能我还有待体验与开发，有兴趣的同学可以前往官方文档进行研究使用。</p>
<h2 id="faq">FAQ</h2>
<h3 id="但是我想让-yadm-管理-home-以外的文件怎么办？">但是我想让 yadm 管理 <code>$HOME</code> 以外的文件怎么办？</h3>
<p>前面的表述中，我们可能已经发现了，<code>yadm</code> 默认情况下仅管理 <code>$HOME</code> 目录下的文件，也就是我们个人用户的文件。<code>yadm</code> 不会去管理系统文件，比如 <code>/etc</code>、<code>/mnt</code> 下的文件。这里的 <code>$HOME</code> 实际上就是 <code>yadm</code> 的默认工作树，其真实文件路径位于 <code>/home/&lt;你的用户名&gt;</code>，<code>yadm</code> 也「根据设计」仅管理这一目录下的文件，而大部分情况下这也是完全合理、够用的。</p>
<p>但是，如果我们想要继续管理 <code>/home/xxx</code> 以外地方的文件，那么我们需要「扩大文件树的范围」，或者「选择使用其他目录作为主文件树」。首先，其他地方的文件 <code>yadm</code> 很可能没有读写权限，所以我们需要先设置一个专门用来管理系统文件的 <code>yadm</code> alias 命令：</p>
<pre><code class="language-bash"># 使用 /etc/yadm 作为 yadm 系统文件管理的目录
alias sysyadm=&quot;sudo yadm -Y /etc/yadm&quot;</code></pre>
<p>之后，我们可以直接使用 <code>sysyadm</code> 命令，并使用一个单独的远程仓库，来单独管理系统文件：</p>
<pre><code class="language-bash"># 将 / 目录初始化为 sysyadm 的文件树
sysyadm init -w /

# 和 yadm 一样，添加文件、签入版本控制系统、同步远程仓库
sysyadm add &lt;某个 / 路径下的系统文件&gt;
sysyadm commit -m &quot;&lt;啊，我签入了一个系统文件耶&gt;&quot;
# ……</code></pre>
<p>当然，你仅使用 <code>sysyadm</code> 管理整个系统里面的全部 dotfiles 也可以，但是你可能每次都需要申请 sudo 权限，不太优雅。</p>
<p>更多使用细节请见：<a href="https://yadm.io/docs/faq#unconventional-cases">yadm Docs - FAQ #Unconventional Cases</a>.</p>
<h3 id="我们这些-windows-用户怎么办？">我们这些 Windows 用户怎么办？</h3>
<p>什么时候我们 Windows 用户才能站起来！气抖冷。</p>
<p>实际上，如果你像我一样使用 WSL，那么我们用上文的方法其实也可以直接进行 Windows 的 dotfiles 管理。在 WSL 环境下，我们 <code>yadm</code> 拿到的 Windows 文件一般位于 <code>/mnt/c/Users/&lt;你的用户名&gt;</code> 里面。因此，这一情况需要我们将 <code>yadm</code> 的工作树设定在 <code>/</code> 或 <code>/mnt</code> 目录下，才可以有效管理 Windows 文件。</p>
<p>但是，我还是觉得有点别扭，特别是当我需要给 <code>yadm</code> 权限直接管理整个 <code>/</code> 目录下的文件的时候。因此，为了方便（并不方便），我还是在 WSL 的 <code>$HOME</code> 里面创建了一个 <code>~/Dotfiles/Windows</code> 目录，用传统办法，手动复制粘贴 Windows 里面的 dotfiles 作为「配置文件参考」。</p>
<p><img src="https://cdn.spencer.felinae98.cn/blog/2020/07/20200724-211554.png" alt="在 WSL 里面手动管理 Windows 中的 Dotfiles"></p>
<p>当然，还好 Windows 需要同步的文件不算太复杂，而且主要为了同步一下 Windows Terminal 的自定义图标，还好还好。</p>
<h2 id="小结">小结</h2>
<p>说到这里，我其实发现自己已经算是不太管曾经那个用笨方法管理的 <a href="https://github.com/spencerwooo/dotfiles">spencerwooo/dotfiles</a> 仓库了，即使标星的人那么多。（啊这，写着写着还发现<a href="https://github.com/spencerwooo/dotfiles/issues/9">有个老哥发了个 issue</a>，人傻了。）好了，本文到这里就结束啦，感谢阅读。(＠_＠;)</p>
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Hit count：用 Google Analytics + Vercel Serverless 为文章添加浏览量统计]]></title>
            <link>https://blog.spencerwoo.com/2020/06/serverless-ga-hit-count-api/</link>
            <guid>https://blog.spencerwoo.com/2020/06/serverless-ga-hit-count-api/</guid>
            <pubDate>Thu, 25 Jun 2020 15:00:00 GMT</pubDate>
            <content:encoded><![CDATA[<p>:::note 🍍 编者按
本文灵感和部分方法极大程度来源于 @printempw 的文章：<a href="https://printempw.github.io/google-analytics-api-page-views-counter/">使用 Google Analytics API 实现博客阅读量统计</a>，感谢。(　o=^•ェ•)o
:::</p>
<p>静态网站是没有后端服务的，仅有一个前端页面用来渲染网站的全部内容。虽然从部署、管理和访问速度的角度来说，静态网站还是有点优势的，但是没有后端就意味着没有「评论系统」、「浏览量统计」、「登录鉴权」等等功能。如果想要实现这些功能，就必须依赖第三方的服务，才能实现类似的需要。许多同学都像我一样：在自己的博客网站里使用 Google Analytics 用来统计访问量和阅读数，因此对于「浏览量统计」这个功能来说，我们其实可以借助 Google Analytics API 来将我们在管理后台看到的部分数据显示在网站前端里面，从而实现「文章访问、阅读数量」显示的功能。</p>
<h2 id="工作原理">工作原理</h2>
<p>Google Analytics 非常强大，能够从非常多的维度来解读你网站的访客来源、浏览量、浏览设备等多种数据。这里当我们进入 Google Analytics 管理后台，在首页我们就可以看到我们网站每个路径在特定时间段之中的浏览数量。</p>
<p><img src="https://cdn.spencer.felinae98.cn/blog/2020/07/20200722-213128.png" alt="Google Analytics 管理后台中我的博客上周 7 天的每个路径浏览数"></p>
<p>实际上我们需要的就是这个数据。幸好，Google Analytics 提供了类似的 API，可以让我们根据页面路径、时间起止等参数来查询浏览数量。不过 Google Analytics 的原始 API 其实还是比较复杂的，而且其本身在国内访问还是不太顺畅，所以为了减轻我们静态网站前端的负担，<strong>我们可以在 Vercel 上面用 Serverless 方案部署一个 API 中转站</strong>，方便我们静态网站调用，从而实现「文章浏览量显示」的功能。</p>
<h2 id="开启-google-analytics-api">开启 Google Analytics API</h2>
<p>我们在 Google Analytics 中调用自己网站的分析数据时，需要首先开启 Google Analytics API，获取到鉴权密钥，才可以正常调用 API。我们可以根据 Google 官方教程：<a href="https://developers.google.com/analytics/devguides/reporting/core/v4/quickstart/service-py#1_enable_the_api">Analytics Reporting API v4 - Enable the API</a>，或按照下面的办法来开启我们账户的 Google Analytics API：</p>
<ul>
<li>首先，前往官方 API 的 <a href="https://console.developers.google.com/start/api?id=analyticsreporting.googleapis.com&amp;credential=client_key">setup tools</a> 并根据提示进行设置，选择一个项目（或创建新的项目，比如 <code>ga-hit-count</code>），之后选择 Continue，就可以为我们这一项目开启 Google API 了；
<img src="https://cdn.spencer.felinae98.cn/blog/2020/07/20200722-213128-1.png" alt="为我们的项目开启 Google API"></li>
<li>接下来，我们会进入 Google API 的 Credentials 设置页面，这里我们首先设置 API 为 Analytics Reporting API，并选择 API 调用方为 Web server，再选择调用数据类型为 Application data，最后选择「不会使用 App Engine 或 Compute Engine」即可；
<img src="https://cdn.spencer.felinae98.cn/blog/2020/07/20200722-213128-2.png" alt="设置 Google API 类型"></li>
<li>最后，我们设置基本信息，获取 Credentials 文件。我们设置 Service account name 的名字（比如 <code>blog-analytics</code>），设置 Role 为 <code>Service Account User</code>，选择 Key type 为 JSON，即可获取 API 凭证，点击 Continue 之后你就可以下载到这一 JSON 格式的 API 凭证文件了。 <img src="https://cdn.spencer.felinae98.cn/blog/2020/07/20200722-213128-3.png" alt="设置 API 凭证信息"></li>
</ul>
<p>我们获取到的 JSON 文件里面应该包含有以下的重要信息：</p>
<pre><code class="language-json">{
  // ...
  &quot;project_id&quot;: &quot;ga-hit-count&quot;,
  &quot;private_key&quot;: &quot;-----BEGIN PRIVATE KEY-----\nxxx-----END PRIVATE KEY-----\n&quot;,
  &quot;client_email&quot;: &quot;blog-hit-count@ga-hit-count.iam.gserviceaccount.com&quot;,
  // ...
}</code></pre>
<p>其中，我们重点关注的就是这三个 API 凭证信息：项目 ID <code>project_id</code>、凭证私钥 <code>private_key</code> 以及客户邮箱 <code>client_email</code>。其中 <code>private_key</code> 是我们 API 访问的重要凭证，需要妥善保管，也一定不能签入 <code>git</code>。另外，我们需要将 <code>client_email</code> 定义的邮箱<strong>作为新用户加入 Google Analytics 后台</strong>，从而让这一邮箱访问到我们 Google Analytics 的数据。详见：<a href="https://support.google.com/analytics/answer/1009702">Add, edit, and delete users and user groups</a>。</p>
<p><img src="https://cdn.spencer.felinae98.cn/blog/2020/07/20200722-213128-4.png" alt="将 client_email 的邮箱加入 Google Analytics 后台"></p>
<h2 id="使用-vercel-自己部署-serverless-api-用于前端显示">使用 Vercel 自己部署 Serverless API 用于前端显示</h2>
<p>最后，我们就可以借助 Google Analytics API 在 Vercel 上部署中转 API 用于前端静态网站的调用。这里我使用 Node.js 和 Typescript 写好了一个非常简单的基础 Serverless API 项目，位于：<a href="https://github.com/spencerwooo/ga-hit-count-serverless">spencerwooo/ga-hit-count-serverless</a>，同学们可以直接 Fork 我的这一项目用来自己部署。其中，如果自己没有特殊需要，那么 Fork 项目之后我们仅需要修改 <code>api/config.ts</code> 里面的配置即可导入 Vercel 一键部署。</p>
<h3 id="修改-ga-hit-count-serverless-的配置">修改 <code>ga-hit-count-serverless</code> 的配置</h3>
<p>同学们将这一项目 Fork 至自己的 GitHub 账户上后，进入 <code>api/config.ts</code> 即可看到我自己的 API 配置，大致如下：</p>
<pre><code class="language-typescript">export default {
  viewId: &#39;{Google Analytics view ID}&#39;,
  auth: {
    projectId: &#39;{Google API project ID}&#39;,
    privateKey: process.env.PRIVATE_KEY,
    clientEmail: &#39;{Google API client email}&#39;,
  },
  allFilter: [&#39;{Post path filter}&#39;],
  startDate: &#39;{Google API query start date}&#39;,
}</code></pre>
<p>其中，这些内容我们都需要一一进行设置：</p>
<ul>
<li><code>viewId</code>：是你的 Google Analytics 视图 ID，可以在 Google Analytics 后台的 Admin » View » View Settings 中找到；
<img src="https://cdn.spencer.felinae98.cn/blog/2020/07/20200722-213128-5.png" alt="Google Analytics 视图 ID 的设定位置"></li>
<li><code>projectId</code>：是刚刚凭证 JSON 文件中的 <code>project_id</code>，直接按照刚刚的凭证填写即可；</li>
<li><code>privateKey</code>：是通过 Vercel 环境变量获取到的 API 凭证私钥，<strong>这里不要更改</strong>；</li>
<li><code>clientEmail</code>：是刚刚凭证 JSON 文件中的 <code>client_email</code>，直接按照刚刚的凭证填写即可；</li>
<li><code>allFilter</code>：是通过 Google API 查询时的前缀过滤器，比如你的网站中文章路径以 <code>/post</code> 开头，那么就可以设置为 <code>[&#39;/post&#39;]</code>。默认为 <code>[&#39;/20&#39;]</code>（因为我的文章路径是以 <code>/2020</code> 或 <code>/2019</code> 开头的）；</li>
<li><code>startDate</code>：是通过 Google API 查询时设定时间段的开始时间，设定一个比较久远的时间即可，默认为 <code>2010-01-01</code>。</li>
</ul>
<p>:::warning 🚨 请注意！
这里千万千万不要直接将刚刚凭证中获取到的私钥直接粘贴进入 <code>privateKey</code> 一项之中，因为这样当我们将 <code>config.ts</code> 签入 <code>git</code> 之后，<code>privateKey</code> 将以明文形式保存，非常危险。
:::</p>
<h3 id="将项目导入-vercel">将项目导入 Vercel</h3>
<p>Vercel（曾经的 ZEIT Now）是一个专注于部署 Jamstack 静态网页和 Serverless API 的服务，其官网位于 <a href="https://vercel.com/">Develop. Preview. Ship. - Vercel</a>。我们使用 GitHub 注册登录 Vercel 之后，仅需将刚刚 Fork 并修改好配置文件版本的 <code>ga-hit-count-serverless</code> 导入 Vercel 即可。Vercel 会自动的识别我们项目的环境，生成合适的编译、部署命令，自动将我们的 API 部署到 Vercel 的全球 CDN 上面，方便全世界随时随地的访问。</p>
<p>但是此时我们并不能正常的使用我们自己部署的 API，因为 <code>privateKey</code> 尚未设置。我们需要进入刚刚在 Vercel 上部署好的项目设置中，选择 General » Environment Variables，向其中新增一个环境变量 <code>PRIVATE_KEY</code>。之后，我们将刚刚的 Google API JSON 凭证文件里面的私钥，<strong>复制其中的字符串部分，将 <code>\n</code> 全部删掉并更换为换行</strong>，得到类似如下的多行私钥形式：</p>
<pre><code>-----BEGIN PRIVATE KEY-----
dageWvAIBADANBAokdP8WgkqhkiGkk
...
afROdsafbliOjPA==1Hk3mdsafEdBa
-----END PRIVATE KEY-----</code></pre><p>我们复制这一私钥，再粘贴进入刚刚新建的 <code>PRIVATE_KEY</code> 的值。</p>
<p><img src="https://cdn.spencer.felinae98.cn/blog/2020/07/20200722-213128-6.png" alt="在 Vercel 项目的设置中添加环境变量 PRIVATE_KEY，并存入我们的私钥凭证"></p>
<p>之后，我们需要重新触发一次部署（比如随便向 GitHub 仓库中 commit 并 push 一些东西），完成后我们即可通过 Vercel 给我们提供的域名 <code>https://{VERCEL_DOMAIN_NAME}.vercel.app</code> 访问我们的 API。</p>
<h3 id="使用-vercel-serverless-版-api">使用 Vercel Serverless 版 API</h3>
<p>默认情况下，当我们直接访问 <code>https://{VERCEL_DOMAIN_NAME}.vercel.app</code> 时，因为没有设定 <code>index.html</code>，所以 Vercel 会将当前列表下的文件列出。</p>
<p><img src="https://cdn.spencer.felinae98.cn/blog/2020/07/20200722-213128-7.png" alt="默认情况下直接访问 Vercel 上部署的 Serverless API 域名"></p>
<p>我们 API 的根域名实际上就是 <code>https://{VERCEL_DOMAIN_NAME}.vercel.app</code>。我们可以通过下面的默认请求（添加在 API 根域名后面）来访问 API：</p>
<pre><code>/api/ga</code></pre><p>没有添加任何参数的情况下，默认这一 API 会将你的 Google Analytics 里面全部路径与访问量拉取并给出。以我自己博客为例，访问这一 API 会得到类似下面的 JSON response：</p>
<pre><code class="language-json">[
  {
    &quot;page&quot;: &quot;/2019/11/tiny-tiny-rss/&quot;,
    &quot;hit&quot;: &quot;698&quot;
  },
  {
    &quot;page&quot;: &quot;/2019/11/weibo-to-twitter/&quot;,
    &quot;hit&quot;: &quot;531&quot;
  },
  {
    &quot;page&quot;: &quot;/2020/03/ttrss-noteworthy/&quot;,
    &quot;hit&quot;: &quot;357&quot;
  },
  // ...
]</code></pre>
<p>其中，我们可以看到返回的 response 中包含有我们网站中所有路径的 <code>hit</code> 阅读量数据，并且数据是按照阅读量递减来排列的。</p>
<p>当我们需要直接请求网站中某一页面或某个特定路径的数据时，我们可以用这样的语法构造我们的请求：</p>
<pre><code>/api/ga?page={WEBSITE_PAGE_PATH}</code></pre><p>其中请求的 field 为 <code>page</code>，参数为目标路径。比如这样的请求：</p>
<pre><code>/api/ga?page=/2020/03/substats/</code></pre><p>将直接返回路径 <code>/2020/03/substats/</code> 的阅读量：</p>
<pre><code>[
  {
    &quot;page&quot;: &quot;/2020/03/substats/&quot;,
    &quot;hit&quot;: &quot;311&quot;
  }
]</code></pre><p>那么在此基础上，我们即可借助自己在 Vercel 上面部署的 API，来请求 Google Analytics 给我们当前路径的浏览量记录了。利用 <code>axios</code> 或者类似的前端 HTTP 请求库，我们可以非常轻松的请求我们部署的 Serverless API，并将结果进行处理，显示在我们的网站里面。你正在浏览的我的博客，就是利用这样的原理实现访问数据的显示。</p>
<p><img src="https://cdn.spencer.felinae98.cn/blog/2020/07/20200722-213128-8.png" alt="用这样的方法在我自己博客上面显示文章的浏览量"></p>
<p>到此，我们借助 Google Analytics 和 Vercel Serverless 为文章添加浏览量统计的功能就大功告成了。希望大家能用上本文的办法，为自己的静态博客网站快速添加上文章阅读量统计。感谢阅读 🍒</p>
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Eureka！对，正是那个时刻]]></title>
            <link>https://blog.spencerwoo.com/2020/06/that-eureka-moment/</link>
            <guid>https://blog.spencerwoo.com/2020/06/that-eureka-moment/</guid>
            <pubDate>Tue, 23 Jun 2020 16:10:00 GMT</pubDate>
            <content:encoded><![CDATA[<p>昨天晚上我们松散的大学班级在腾讯会议上结束了大学最后一次「班会」，很多人也可能是最后一次「见面」。昨天的活动也让我在这个毕业季终于有了一点毕业的感觉。情感不能像纯理性的论文或者科普教程一样能掰开了揉碎了来介绍讲解，我也不知道纯情感的「牢骚」适不适合发出来让大伙一起看看，甚至不知道直接发在我这个想要专注写技术的博客上面会不会太一言难尽。不管了，总之它来了。</p>
<p>虽说是情感，其实主要是流水账，我主要想谈谈我大学里的 Eureka moment 对我来说是多么的重要。实话说，我刚入大学时确实一腔热血投入 CS 的学习，但是当时我对计算机的的确确一无所知。虽然高中我参加过有限的几次 C 语言培训，也获得过省里的名次，但是刚上大一的我，可能也就比身边的同学多知道 C 语言需要写头文件才能编译以及 Dev C++ 这个古老的 IDE 到底怎么使用。这段时间我学业上其实非常迷茫，处于一种自信和卑微共存的心理状态。我是一个非常需要内容来证明自己的人，所以学业上的低迷让我一身心投入我校记者团和摄影协会的摄影事业。我相信也正是这段经历，让我不论是审美还是专业软件的使用直到今天都比我的 peer 们稍好一些。（我也担任了一段时间的记者团执行团长。）</p>
<p>进入大学一段时间之后，我第一次选择了一个比较需要专业技能的课程。课程主要用大作业项目进行考核，需要结对编程。CS 嘛，编程经历必不可少，所以这也成为了我第一次开发一个「项目」的经历。当时我不知道 <code>git</code>，不懂得版本控制，不会用 Eclipse 这些 IDE，也不懂具体的系统究竟应该如何设计。一个完全空白的孩子就这样摸爬滚打完成了第一个作业，当然最终成绩也并不理想。</p>
<p>之后，我参加了暑期集训，当时一个学长作为高年级的讲师给我们培训算法，他将部分题解以博文的形式发在了 GitHub Pages 上面，于是我因为 <code>github.io</code> 的域名，第一次发现了 GitHub 以及私有部署的博客。我也从那时候开始捣鼓自己的博客，研究 GitHub 是什么，看命令行如何使用。<strong>这段经历确实为我接下来的 Eureka moment 打下了基础。</strong></p>
<p>随后我从偏僻的良乡校区搬到了中关村，从小学期到一门门的选修课，我的编程工作也越来越多。一次又一次的开发任务让我发现了非常有潜力的身边同学。（也发现了很 emmm 的同学……如果有人看到的话，请不要对号入座，我还是爱你的。）当然，「欣赏」是互相的，所以这个过程中我也和这些同学完成「绑定」，组成了随后课程大作业的得力团队。</p>
<p>在课程之外，我还在即刻上非常活跃。当时，我分享了一个用 Photoshop（是的，当时我还在用 Photoshop （；´д｀）ゞ）设计的即刻名片，许多即友们非常喜欢，纷纷表示想要，我自己手动用 Photoshop 来制作太浪费时间，于是在给几个相对熟悉的即友制作之后，我萌发了自己动手开发一个名片生成网站的想法。一个类似功能网站必须包含有生成名片的「前端」部分，还需要搞定一个中转即刻 API 的「后端」部分，所以当时啥都不懂的我，开始了人生中第一次全栈开发。现在回看当时的代码，不仅设计有很大问题，安全方面也简直布满了漏洞（我当时甚至明文用 JSON 存储了 accessKey…），不过，生成器还是能用。后端我委托 TenkeySeven 部署在他的阿里云上面，结果在我发出即刻动态之后，Tenkey 的服务器直接被挤爆，为了修 bug 当时甚至都没去吃晚饭，从 4 点发布之后我留在教室搞到晚上 9 点，才买了个煎饼果子回宿舍。</p>
<p>昨天演讲的时候我提到了我经历了 Eureka moment，让我的专业技能得到升华。同学会后跟我说：「是不是有一次大作业去 GitHub 上找能跑的代码让我一下 Aha! 💡」，当时我没反驳，不过其实我觉得并不是。现在站在我如今的高度进行回溯，我觉得开发名片生成器的这段经历才是真正的 Eureka moment：正是这之后，我开始一发不可收拾<del>（没有，夸张了）</del>，不仅学会了自己完成项目开发设计，还能协调其他同学跟我进行配合，顺滑完美的完成任务。也正是因为项目开发这个稍微「软实力象征」的任务我拿捏得当，让我在大四上学期这个强调大作业的学期实现弯道超车，幸运的拿到了专业第五、一等奖学金（还有进步奖学金 😄）的成绩。</p>
<p>回想整个四年，从专业技能的角度来说，Eureka moment 之前我几乎就在原地踏步，而之后我就像换了一个人一样，开始对自己的专业能力拥有极强自信，成为几乎所有大作业的中坚力量。GitHub 贡献榜也佐证了这一点，仅仅从去年 2019 年到今天，我的 contribution 记录就已经达到了 2k+，2020 年上半年几乎全绿。这不仅因为我所有的大作业都在 GitHub 上进行协同开发，还因为我许多开源项目也在 GitHub 上进行维护。</p>
<p>我是个不擅长进行情感表达的人，也是一个不擅长推心置腹的人。如果你读到了这里，那我真的非常感谢你愿意看完我的流水账。这完全不是我大学的总结，如果要总结大学，那我可以写上两天两夜，这仅仅是跟大家分享一下我的 Eureka moment 以及这个 Eureka moment 对我之后的路有着多么大的影响。从学业的角度来说，我完全算不上是成功的一个，<strong>但是从我自己现在来回看，我的 Eureka moment 确实让我 CS 之路走上正轨。</strong>希望阅读这篇文章的你，不论是跟我同级的同学，还是比我小的高中生、大学生，都能尽快找到自己的 Eureka moment，共勉。</p>
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Netlify or VuePress：大型悬疑推理篇之——报错到底是谁的锅？]]></title>
            <link>https://blog.spencerwoo.com/2020/05/debugging-netlify-static-site/</link>
            <guid>https://blog.spencerwoo.com/2020/05/debugging-netlify-static-site/</guid>
            <pubDate>Fri, 08 May 2020 16:23:00 GMT</pubDate>
            <content:encoded><![CDATA[<p>Dev on Windows with WSL 是我目前维护比较频繁的一个文档，它介绍了在 Windows 上使用 WSL 进行开发的环境配置、注意事项以及高阶操作等，我们称它为 <code>dowww</code>。我使用的是非常优秀的基于 Vue 的静态文档生成器：VuePress，来构建 <code>dowww</code> 的文档网站，并（曾经）借助于 Netlify 来将文档的静态页面部署在网络上，让大家可以访问参考。</p>
<p>随着 WSL 的不断更新，我们的 <code>dowww</code> 文档也在不断迭代，为了让 <code>dowww</code> 文档网站能够回溯历史版本，我参考了在 VuePress 项目的 issue 区中 <a href="https://github.com/vuejs/vuepress/issues/1018">Docs versioning mechanism</a> 这一 issue 提到的一种方案，自定义实现了「多版本文档」这一 VuePress 尚未实现的功能。随后，在 Netlify 上部署的静态网站便出现了非常奇特的 bug。</p>
<h2 id="问题复现">问题复现</h2>
<p>为了方便各位同学来亲身经历（<del>不是</del>）这一奇妙的 bug，我并没有删除 Netlify 当时的部署版本，同学们可以直接访问：<a href="https://deploy-preview-54--dowww.netlify.app">Netlify Deploy Preview 54/dowww</a>，进入任意一个文档内部页面（比如：<a href="https://deploy-preview-54--dowww.netlify.app/1.1/4-Advanced/4-2-LxRunOffline.html">Dev on Windows with WSL - LxRunOffline</a>），刷新页面，然后我们就可以看到浏览器 Console 中出现了那红红的报错信息：</p>
<pre><code>DOMException: Failed to execute &#39;appendChild&#39; on &#39;Node&#39;: This node type does not support this method.
    at Object.appendChild (https://deploy-preview-54--dowww.netlify.app/a...</code></pre><p><img src="https://cdn.spencer.felinae98.cn/blog/2020/07/20200722-215425.png" alt="刷新页面，即可看到 Console 报错"></p>
<p>另外，在这一状态下，点击左侧的导航侧边栏，VuePress 不仅无法渲染出顺滑滚动的效果，我们甚至无法直接进行页面导航，点不开下拉菜单，偶尔也根本打不开任何其他页面，我们只能手动再次进入文档主页，重新定位刚刚的位置才能正常阅读。🎃</p>
<p>其实，我在今年 1 月初更新完成大部分 1.1 版本的 <code>dowww</code> 之后，就发现了这一问题。同时，@InsulationJustf 大佬在 2 月左右也跟我反馈了侧边栏无法点击的问题。拖延症的我，在搜索了全网的相关问题之后，发现：</p>
<ul>
<li>有可能是 VuePress 自己的问题</li>
<li>也有可能是我手动给 VuePress 添加 versioning 功能导致的 bug</li>
<li>还有可能是我部署到 Netlify 上面之后，Netlify 做了些 undocumented 的操作</li>
<li>甚至有可能是 Cloudflare 在将网站 serve 出来的时候动了动静态文件，压缩了空白字符等等</li>
</ul>
<p>最终我解决问题无果，在 4 月的时候去 Netlify Community 寻求帮助，各位可以去 <a href="https://community.netlify.com/t/vuepress-deployment-on-netlify-succeeds-but-experience-errors-when-reloading-specific-pages/12606">VuePress deployment on Netlify succeeds, but experience errors when reloading specific pages</a> 这一问题下进行围观。</p>
<h2 id="定位-bug">定位 bug</h2>
<p>其实，在去 Netlify Community 寻求帮助之前，我就已经自己在本地和远程环境中进行多次 debug 尝试。我用 Chrome Debugger 寻找到报错的具体原因，并将前面的猜想一一进行了排除。</p>
<h3 id="使用-chrome-debugger-找到报错根源">使用 Chrome Debugger 找到报错根源</h3>
<p>在 Production 环境下进行 debug 异常艰辛，因为 Production 环境中所有的相关文件都已经被 minify 了，即使在 Chrome 的 console 中报出错误，我们点击进入报错代码行，映入眼帘的也只有以 <code>e</code>、<code>t</code>、<code>p</code> 等为名的反人类变量。我们只能寄希望于 Chrome Debugger 能够捕获 Exception，并给我们有用的信息。</p>
<p>报错只发生于部署得到的 Netlify 网站上面，所以我们直接进入文档内部一页，点击刷新，待页面加载成功之后开启 Chrome Debugger 捕获 Exception。在这一状态下，bug 的复现非常简单，只需要点击任意一个侧边栏链接，我们就可以看到 Chrome Debugger 捕获到了相应的错误：</p>
<p><img src="https://cdn.spencer.felinae98.cn/blog/2020/07/20200722-215425-1.png" alt="使用 Chrome Debugger 捕获出错位置"></p>
<p>可以看到，出错的代码是：</p>
<pre><code class="language-javascript">appendChild: function(t, e) {
  t.appendChild(e)
}</code></pre>
<p>当 <code>t</code> 是一个 <code>#comment</code> 类型的节点（node）时，强行给这一节点添加子节点（即调用 <code>appendChild()</code> 方法）就会报出 <code>DOMException: Failed to execute &#39;appendChild&#39; on &#39;Node&#39;</code> 的错误。我们这里的 <code>t</code> 刚好是一段空注释 <code>&lt;!----&gt;</code>，自然不是合法的 HTML 节点，报错合情合理。</p>
<h3 id="排除是-vuepress-本身的原因">排除是 VuePress 本身的原因</h3>
<p>为了排除是 VuePress 本身的问题，我首先确认了网站在本地的 Dev 环境下是没有问题的，也就是 <code>yarn docs:dev</code> 开启 development server 后的网站时没问题的。当然，也确实没有问题，我完全无法复现在服务器上部署出现的 bug。</p>
<p>接下来，我尝试在本地 build 网站的静态文件，并用一个简单的 http server 在本地部署进行尝试。我使用 <code>yarn docs:build</code> 将全站进行编译，生成静态文件位于 VuePress 编译后静态文件的默认位置（也就是 <code>docs/.vuepress/dist</code> 目录下）。之后，我简单使用 Python 的 <code>http.server</code> 来 serve 得到的静态文件夹：</p>
<pre><code class="language-bash"># 进入编译得到的静态文件目录
cd docs/.vuepress/dist

# 运行 Python http.server 模块来 serve 整个网站
python -m http.server</code></pre>
<p>这样我们就可以在 <code>localhost:8000</code> 访问 Production 环境下的网站，如果我们直接将这一网站部署到自己的服务器上，得到的效果也是如此。但是，我依旧没能够复现在 Netlify 上面部署出现的 bug。所以我们基本可以确定我们自己的网站本身是没有问题的，问题应该出现在第三方静态网站托管服务上，也就是 Netlify 或者 Cloudflare 上面。</p>
<h3 id="罪魁祸首竟然是">罪魁祸首竟然是……</h3>
<p>上面我们找到了报错的原因，也排除了 VuePress 本身导致的报错，因此，前两个推断都被我们一一排除掉，出错只可能是由于 Netlify 或者 Cloudflare 导致。为了验证，我将网站重新用和 Netlify 类似的 Vercel（前身是 ZEIT）进行了部署，整个部署非常简单，项目导入之后几乎零配置就完成了网站的迁移。</p>
<p>之后，令人震惊的事情发生了：<strong>在 Vercel 上部署的 <code>dowww</code> 竟然没有任何问题，和本地部署的版本近乎一致，bug 也销声匿迹！</strong></p>
<p>:::important 罪魁祸首
<strong>🌚 Netlify！想不到你这个浓眉大眼的！竟然是因为你！</strong>
:::</p>
<h2 id="解决方案">解决方案</h2>
<p>好吧，确实是因为 Netlify 出了点偏差，但是虽然我已经找到了问题是 Netlify 导致的，网站迁移到 Vercel 就没有问题了，但是我还是没有找到到底为什么 Netlify 部署的网站会和本地不一致。</p>
<p>就在我在 Telegram 频道发布迁移投票之后，<a href="https://twitter.com/geekdada">@geekdada</a> 通过 Twitter 私信找到了我，一针见血，精确定位问题所在！（<a href="https://twitter.com/geekdada">@geekdada</a> 是 Surgio 的作者，<a href="https://github.com/geekdada/surgio">Surgio</a> 是<strong>支持自部署的一站式规则解析生成器</strong>，身处国内的你一定用得到 🚀）</p>
<p>@geekdada 告诉我，Netlify 有一个非常蛋疼的设定：所有包含有大写字母的 URL 路径都会被处理成小写，这一规则是默认添加且不能 opt out 的，也就是所有在 Netlify 上部署的静态网站，如果包含有 case-sensitive 的路径，就有可能出错。</p>
<blockquote>
<p>This is an intended feature of our platform from which you cannot opt out. We’ll serve any combination of case (e.g. <code>FiLe.HtMl</code> or <code>file.html</code> or <code>FILE.HTML</code>) correctly, though the mixed-case ones will be redirected to the “canonical” lowercase definition.</p>
<p>We found that most people deploy from a non-case-sensitive filesystem (Windows/Mac) and this was the best way to make things work as most folks intended.</p>
<p>🚩 <em>Source: <a href="https://community.netlify.com/t/my-url-paths-are-forced-into-lowercase/1659/2">My URL paths are forced into lowercase</a></em></p>
</blockquote>
<p>Netlify 强制使用小写 URL 路径的方法是将直接访问包含有大写字母的 http 请求通过 301 转发至小写 URL。比如，我们尝试直接请求 <code>https://deploy-preview-54--dowww.netlify.app/1.1/4-Advanced/4-2-LxRunOffline.html</code> 这一包含有大写字母的 URL：</p>
<pre><code class="language-bash">curl -I https://deploy-preview-54--dowww.netlify.app/1.1/4-Advanced/4-2-LxRunOffline.html</code></pre>
<p><img src="https://cdn.spencer.felinae98.cn/blog/2020/07/20200722-215425-2.png" alt="请求 Netlify 上包含有大写字母 URL 的路径"></p>
<p>可以看到 Netlify 是返回了一个 HTTP 301，并转发到了相应的小写字母 URL 对应的 location。而当我们直接请求小写字母版本的 URL 时：</p>
<pre><code class="language-bash">curl -I https://deploy-preview-54--dowww.netlify.app/1.1/4-advanced/4-2-lxrunoffline.html</code></pre>
<p><img src="https://cdn.spencer.felinae98.cn/blog/2020/07/20200722-215425-3.png" alt="请求对应只有小写字母的 URL 路径"></p>
<p>一切都恢复正常了。由于 VuePress 生成的静态文件的文件名都是按照我 Markdown 文件名来的，所以当我们直接的访问一个包含大写字母 URL 的内部页面时，由于 Netlify 的处理，我们实际上进入了一个全为小写字母 URL 的页面，此时 VuePress 自己就迷惑了，也就出现导航至其他页面时可能触发的 bug。</p>
<p>所以……看来如果我将我项目中所有的路径中的大写字母改为小写，问题在 Netlify 平台上就解决了？事实上也确实如此。上面刚刚提到，VuePress 在进行 Markdown 文件解析时，会按照 Markdown 文件本身的文件名进行处理，每一个 Markdown 文件都是一个单独的路径，因此我需要将所有的 Markdown 文件名称、包含 Markdown 文件的路径、以及在 VuePress 配置文件 <code>config.js</code> 与我自己自定义 versioning 实现中所使用的 <code>sidebar.js</code> 里定义的全部文件名、路径名中的大写字母更换成小写字母。</p>
<p>看似比较复杂的问题，其实也不过是一行命令的事情：</p>
<pre><code class="language-bash">find my_root_dir -depth -exec rename &#39;s/(.*)\/([^\/]*)/$1\/\L$2/&#39; {} \;</code></pre>
<p>上面这一命令非常巧妙，用 <code>rename</code> 工具先重命名目录中包含的文件，再重命名目录本身，有效的避免了命令错误导致文件路径失效的问题[^1]。我直接在项目的 <code>/docs</code> 目录下运行上面这一命令，之后手动修改了 <code>config.js</code>、<code>sidebar.js</code> 中内容，重新部署到 Netlify 上面……问题解决！</p>
<p>那么，历时四个月（<del>并没有，只是因为我拖延</del>）的 bug 总算解决了，我们 <code>dowww</code> 文档也正式支持上了多版本 versioning，非常开心。不过，虽然 Netlify 上面确实问题解决了，但是由于 Vercel 的管理后台实在太好看，所以我还是把我的大部分网站都迁移了过去。喜新厌旧本人了 🤭</p>
<p>这篇文章主要想记录一下我艰（tuo）难（yan）的 debug 经历，为同在使用 Netlify 和 Vercel 等第三方静态网站托管平台的你提供一些参考，不再重蹈覆辙。希望本文能够帮到你解决问题，感谢阅读。</p>
<p>[^1]: 这一命令来自 Stack Overflow 的这一问题：<a href="https://stackoverflow.com/questions/152514/how-do-i-rename-all-folders-and-files-to-lowercase-on-linux">How do I rename all folders and files to lowercase on Linux?</a></p>
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Norm：简单介绍如何衡量机器学习中向量的「大小」]]></title>
            <link>https://blog.spencerwoo.com/2020/04/norm/</link>
            <guid>https://blog.spencerwoo.com/2020/04/norm/</guid>
            <pubDate>Wed, 08 Apr 2020 00:00:00 GMT</pubDate>
            <content:encoded><![CDATA[<p>定量的衡量一个向量的长度或者大小往往是机器学习向量运算、矩阵运算中非常必要的一个任务，我们往往将「向量的长度」称为向量的范数：Vector&#39;s Norm。</p>
<blockquote>
<p><strong>范数（norm）</strong>：是具有「长度」概念的函数。在线性代数、泛函分析及相关的数学领域，是一个函数，<strong>其为向量空间内的所有向量赋予非零的正长度或大小。</strong>[^1]</p>
</blockquote>
<p>最近，我在对抗样本攻击的研究中，需要定量的衡量「对抗样本」和「原图」之间的「扰动大小」。事实上，在机器学习里，不论是「对抗样本」还是其他的图片，它们本质上都可以用向量来表示，在 Python 中使用 Numpy 矩阵来存储和运算。这篇文章简单介绍（记录）一下一些 $\ell_p$ 范数的计算方法以及代码实现。</p>
<p>:::note 对抗样本的概念
对抗样本（Adversarial Examples）是神经网络模型中的一种 Vulnerability，其中面向图像分类模型的对抗样本指的是：对模型输入图片上添加一个微小的「扰动」，使得模型对输入图片进行错误的分类的一种问题。
:::</p>
<h2 id="ell_p-范数">$\ell_p$ 范数</h2>
<p>$\ell_p$ 范数实际上是向量空间中的「一组范数」[^2]。在我的研究方向上，$\ell_p$ 范数也经常用于定量的衡量对抗样本的「扰动」幅度。我们将 $\ell_p$ 范数定义如下：</p>
<p>$$
\ell_p: L_p(\vec{x})=(\sum_{i=1}^n |x_i|^p)^{1/p}
$$</p>
<p>其中，$p$ 的取值可以是：$1$、$2$ 以及 $\infty$ 等，分别表示 $\ell_1$ 范数、$\ell_2$ 范数以及 $\ell_{\infty}$ 范数。</p>
<h3 id="ell_0-范数">$\ell_0$ 范数</h3>
<p>严格意义上来说，$\ell_0$ 范数并不是「范数」（所以定义中 $1/p$ 的 $p$ 也不能取值为 0），它表示的是<strong>向量中非零元素的个数</strong>。那么在对抗样本中，$\ell_0$ 范数表示的就是「扰动」中非零元素的个数。</p>
<h3 id="ell_1-范数">$\ell_1$ 范数</h3>
<p>$\ell_1$ 范数表示的是<strong>向量空间中所有向量长度之和</strong>，一个比较形象的形容就是：在向量空间中，你需要从一个向量的起点走到另一个向量的终点，那么你行程的距离（经过向量的总长度）就是向量的 $\ell_1$ 范数。</p>
<p><img src="https://cdn.spencer.felinae98.cn/blog/2020/07/20200722-215637.png" alt=""></p>
<p>如上图，$\ell_1$ 即可以按照下式计算得到：</p>
<p>$$
\ell_1(\vec{v})=|\vec{v}|_1=|a|+|b|
$$</p>
<p>就像出租车（Taxicab）沿着路线行驶全程所经过的距离。因而，$\ell_1$ 范数也被称为 Taxicab 范数或者 Manhattan 范数。（因为曼哈顿的街道大部分都是矩形直角分布的！🤣）</p>
<h3 id="ell_2-范数">$\ell_2$ 范数</h3>
<p>$\ell_2$ 范数是机器学习领域更为常用的一种向量大小的衡量方法。$\ell_2$ 范数也被称为欧式范数（Euclidean norm），表示的是从一点到另一点所需的最短距离。</p>
<p><img src="https://cdn.spencer.felinae98.cn/blog/2020/07/20200722-215637-1.png" alt=""></p>
<p>按照如上图所示的例子，$\ell_2$ 范数就是按照下式进行计算得到的：</p>
<p>$$
\ell_2(\vec{v})=|\vec{v}|_2=\sqrt{(|a|^2+|b|^2)}
$$</p>
<h3 id="ell_infty-范数">$\ell_\infty$ 范数</h3>
<p>$\ell_\infty$ 范数理解起来就非常简单，即为向量元素里面绝对值最大的元素的长度（大小）：</p>
<p>$$
\ell_\infty(\vec{v})=|\vec{v}|_\infty=\max(|a|,|b|)
$$</p>
<p>比如给定一个向量 $\vec{v}=[-10,3,5]$，那么向量的 $\ell_\infty$ 范数就是 $10$。</p>
<h2 id="代码实现">代码实现</h2>
<p>在我的研究中，我往往使用 $\ell_p$ 范数衡量对抗样本的扰动大小。很遗憾，我所使用的 Foolbox 框架并没有直接给出所有 $\ell_p$ 范数计算的距离值（distance），所有我往往需要用 Numpy 进行计算。</p>
<p>对于一个图片 <code>img</code>，以及其对抗样本 <code>adv</code>，我们可以轻松的计算得到「扰动」<code>perturb</code>：</p>
<pre><code class="language-python"># perturb is a numpy array
perturb = adv - img</code></pre>
<p>那么，我们就可以使用 Numpy 来计算扰动 <code>perturb</code> 的各项 $\ell_p$ 范数：</p>
<pre><code class="language-python"># import numpy and relevant libraries
import numpy as np
from numpy.linalg import norm

# L0
_l0 = norm(perturb, 0)
# L1
_l1 = norm(perturb, 1)
# L2
_l2 = norm(perturb)
# L∞
_linf = norm(perturb, np.inf)</code></pre>
<p>事实上，<code>numpy.linalg.norm</code> 的内部实现中，就是利用 $\ell_p$ 的定义进行的向量运算。比如 $\ell_1(x)$ 就是：</p>
<pre><code class="language-python">_l1_x = np.sum(np.abs(x))</code></pre>
<p>$\ell_\infty(x)$ 就是：</p>
<pre><code class="language-python">_linf_x = np.max(np.abs(x))</code></pre>
<p>等等（上面是对于向量来说的两种简单算法，没有考虑特殊情况，生产环境中尽量使用 <code>numpy.linalg.norm</code> 提供的方法）。在 Numpy 的官方文档中，Numpy 也给出了 $\ell_p$ 范数在矩阵和向量不同情况下的计算方法。</p>
<h2 id="延伸阅读">延伸阅读</h2>
<ul>
<li><a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.norm.html">Numpy 文档 - <code>numpy.linalg.norm</code></a></li>
<li><a href="https://medium.com/@montjoile/l0-norm-l1-norm-l2-norm-l-infinity-norm-7a7d18a4f40c">Medium - L0 Norm, L1 Norm, L2 Norm &amp; L-Infinity Norm</a></li>
<li><a href="https://machinelearningmastery.com/vector-norms-machine-learning/">Gentle Introduction to Vector Norms in Machine Learning</a></li>
</ul>
<p>[^1]: <a href="https://zh.wikipedia.org/wiki/%E8%8C%83%E6%95%B0">Wikipedia - 范数</a>
[^2]: <a href="https://zh.wikipedia.org/wiki/Lp%E8%8C%83%E6%95%B0">Wikipedia - $\ell_p$ 范数</a></p>
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Substats：快速统计你在各个平台的关注者！]]></title>
            <link>https://blog.spencerwoo.com/2020/03/substats/</link>
            <guid>https://blog.spencerwoo.com/2020/03/substats/</guid>
            <pubDate>Mon, 16 Mar 2020 00:00:00 GMT</pubDate>
            <content:encoded><![CDATA[<p>:::important SUBSTATS
Serverless Function to Count How Many People are Subscribed to You in Your Favorite Services.
<strong>你只管调用，我们来帮你找订阅者！</strong>
:::</p>
<p>在 <a href="https://blog.spencerwoo.com/2020/03/ttrss-noteworthy/">上一篇文章</a> 里面，我在开头用 Feedly 的 API 和 Shields.io 制作了显示我 RSS 订阅数量的 Badge。这个 Badge 不仅是实时更新、动态加载的，还能轻松嵌入各个网页里面。</p>
<p><a href="https://feedly.com/i/subscription/feed%2Fhttps%3A%2F%2Fblog.spencerwoo.com%2Fposts%2Findex.xml"><img src="https://img.shields.io/badge/dynamic/json?color=2bb24c&amp;label=subscribers&amp;query=%24.source.subscribers&amp;url=https%3A%2F%2Ffeedly.com%2Fv3%2Frecommendations%2Ffeeds%2Ffeed%252Fhttps%253A%252F%252Fblog.spencerwoo.com%252Fposts%252Findex.xml&amp;logo=feedly&style=for-the-badge" alt="" style="display: inline; margin: 0 0.1rem 0 0; width: auto;"></a></p>
<p>但是，RSS 订阅服务不仅仅有 Feedly 一家，还有 Inoreader 和 NewsBlur 等等。单一个 Feedly 提供的数据并不能真正显示我们 RSS 链接的订阅人数，于是，我就准备用 Serverless 技术搭建一个「API 中转站」，<strong>提供多个服务商的订阅人数整合的工作。</strong></p>
<p>其实，最初的 Substats 实际上叫做 RSS-stats，也就是集合多个 RSS 服务商提供的订阅人数数据得到的一个 API 服务。但是后来经过我一番思考，既然都是调用 API，那么为什么不把其他的平台和服务，比如微博粉丝、知乎、少数派、以及 GitHub 和 Twitter 的关注者等等，一起支持一下呢？💡 可行！于是 Substats 就这样诞生啦。(≧∇≦)ﾉ</p>
<p>:::tip 相关链接</p>
<ul>
<li><strong>Substats API 地址</strong>：<a href="https://api.spencerwoo.com/substats/">API - Substats</a></li>
<li><strong>Substats GitHub 项目地址</strong>：<a href="https://github.com/spencerwooo/Substats">spencerwooo/Substats</a>
:::</li>
</ul>
<h2 id="功能特性">功能特性</h2>
<p>Substats 是一个非常方便易用的<strong>请求订阅者、粉丝、关注用户数量 API 服务</strong>。目前，Substats 平台支持了包括 Feedly、GitHub、Twitter、知乎和少数派在内的五个平台和网站，并使用 Serverless 技术部署到了 Cloudflare 的 CDN 上，全球部署，飞速响应。Substats 将复杂的原平台 API 请求进行了隐藏、简化和集成，让用 Substats 的你只需要关注<strong>两个参数：平台名称、用户名称</strong>，一波访问，即可得到对应的关注数量。</p>
<p>得益于强大的 Cloudflare 全球 CDN 网络，Substats 不仅部署方便、维护轻松，<strong>还有着极强的可拓展性、极快的访问速度和极小的请求时延</strong>。甚至在你懂得的地方，你都可以轻松访问到 Twitter 的粉丝数量！🥂</p>
<h3 id="api-endpoint">API Endpoint</h3>
<p>Substats 的请求非常简单，基础 API Endpoint 位于：</p>
<pre><code>https://api.spencerwoo.com/substats/</code></pre><p>接下来，我们只需要关注前文提到的平台名 <code>source</code> 和用户名称（或 RSS 链接、用户 slug 等标识）<code>queryKey</code> 即可构造一个基本的请求。为了更好的和 Shields.io 整合，Substats 仅支持 <code>GET</code> 请求，并使用查询字符串（Query String）来添加请求参数。</p>
<h3 id="基础请求">基础请求</h3>
<p>一个最基础的请求参数类似：</p>
<pre><code class="language-http">GET /?source={SOURCE}&amp;queryKey={QUERY}</code></pre>
<p>其中，我们只需要填入平台名称 <code>{SOURCE}</code> 和请求参数 <code>{QUERY}</code> 即可。</p>
<h3 id="平台串联请求">平台串联请求</h3>
<p>我们可以用下面的语法构建单个请求 query 并列请求多个平台的 API，只需要将平台之间用 <code>|</code> 分隔即可：</p>
<pre><code class="language-http">GET /?source={SOURCE_1}|{SOURCE_2}|{SOURCE_3}&amp;queryKey={QUERY}</code></pre>
<p>其中，这一请求格式特别适合 RSS 订阅的请求，比如当我们想统计同一个 RSS 链接在 Feedly、Inoreader 以及 NewsBlur 三个平台的订阅者数量，即可使用这一语法进行 API 请求。（详见下文例子）</p>
<h3 id="多个平台和用户名的串联请求">多个平台和用户名的串联请求</h3>
<pre><code class="language-http">GET /?source={SOURCE}&amp;queryKey={QUERY}&amp;source={SOURCE}&amp;queryKey={QUERY} ....</code></pre>
<p>如果我们每个平台的请求参数（也就是用户名）不一样，没关系，我们也可以用上面的语法组织各个 <code>[平台, 参数]</code> 二元组，依次请求，得到最终数据。在这一过程中，平台、参数的顺序在请求和内部 API 处理的过程中是完全一致的。（你也就不必担心请求的错位。）</p>
<h2 id="一些例子">一些例子</h2>
<p>将 Substats 和 <a href="https://shields.io/">Shields.io</a> 配合起来，我们可以构造稳定可用的关注者数量实时显示 Badge，嵌入包括 GitHub README、博客文章等等网站的各个位置。我来举个栗子。🌰</p>
<h3 id="单个请求">单个请求</h3>
<p>Substats 最初就是为了请求 RSS 订阅者数量，我们先来请求一波 Feedly 的订阅数量。我自己博客（也就是本博客）的 RSS 订阅链接是 <code>https://blog.spencerwoo.com/feed.xml</code>，那么，我们就可以用下面的 URL 构造请求：</p>
<pre><code class="language-http">GET /?source=feedly&amp;queryKey=https://blog.spencerwoo.com/feed.xml</code></pre>
<p>这一请求会返回如下的数据：</p>
<pre><code class="language-json">{
  &quot;status&quot;: 200,
  &quot;data&quot;: {
    &quot;totalSubs&quot;: 14,
    &quot;subsInEachSource&quot;: { &quot;feedly&quot;: 14 },
    &quot;failedSources&quot;: {}
  }
}</code></pre>
<p>我们所需要的数据即位于：<code>data.totalSubs</code>。在 <a href="https://shields.io/">Shields.io</a> 官网，我们即可借助 Dynamic Badge 构建一个自定义的 Badge：</p>
<ul>
<li><code>data type</code> 选择：<code>json</code></li>
<li><code>label</code> 填入：Feedly RSS Subscribes</li>
<li><code>data url</code> 填入：<code>https://api.spencerwoo.com/substats/?source=feedly&amp;queryKey=https://blog.spencerwoo.com/feed.xml</code></li>
<li><code>query</code> 填入：<code>$.data.totalSubs</code></li>
<li><code>color</code> 填入：<code>2bb24c</code>（Feedly 的强调色）</li>
</ul>
<p>点击 Make badge，即可生成如下的 Feedly RSS 订阅 Badge：</p>
<p><a href="https://feedly.com/i/subscription/feed%2Fhttps%3A%2F%2Fblog.spencerwoo.com%2Fposts%2Findex.xml"><img src="https://img.shields.io/badge/dynamic/json?color=2bb24c&label=Feedly%20RSS%20Subscribes&query=%24.data.totalSubs&url=https%3A%2F%2Fapi.spencerwoo.com%2Fsubstats%2F%3Fsource%3Dfeedly%26queryKey%3Dhttps%3A%2F%2Fblog.spencerwoo.com%2Fposts%2Findex.xml" alt="" style="display: inline; margin: 0 0.1rem 0 0; width: auto;"></a></p>
<pre><code>https://img.shields.io/badge/dynamic/json?color=2bb24c&amp;label=Feedly%20RSS%20Subscribes&amp;query=%24.data.totalSubs&amp;url=https%3A%2F%2Fapi.spencerwoo.com%2Fsubstats%2F%3Fsource%3Dfeedly%26queryKey%3Dhttps%3A%2F%2Fblog.spencerwoo.com%2Fposts%2Findex.xml</code></pre><p>在这一请求链接的结尾，再手动添加上 Feedly 的 logo 请求参数 <code>&amp;logo=feedly</code>，即可将 Badge 添加上图标：</p>
<p><a href="https://feedly.com/i/subscription/feed%2Fhttps%3A%2F%2Fblog.spencerwoo.com%2Fposts%2Findex.xml"><img src="https://img.shields.io/badge/dynamic/json?color=2bb24c&label=Feedly%20RSS%20Subscribes&query=%24.data.totalSubs&url=https%3A%2F%2Fapi.spencerwoo.com%2Fsubstats%2F%3Fsource%3Dfeedly%26queryKey%3Dhttps%3A%2F%2Fblog.spencerwoo.com%2Fposts%2Findex.xml&logo=feedly" alt="" style="display: inline; margin: 0 0.1rem 0 0; width: auto;"></a></p>
<pre><code>https://img.shields.io/badge/dynamic/json?color=2bb24c&amp;label=Feedly%20RSS%20Subscribes&amp;query=%24.data.totalSubs&amp;url=https%3A%2F%2Fapi.spencerwoo.com%2Fsubstats%2F%3Fsource%3Dfeedly%26queryKey%3Dhttps%3A%2F%2Fblog.spencerwoo.com%2Fposts%2Findex.xml&amp;logo=feedly</code></pre><p>另外，我们还可以指定生成超大 For The Badge 风格的 Badge，在上面请求末尾再手动添加参数 <code>&amp;style=for-the-badge</code> 即可：</p>
<p><a href="https://feedly.com/i/subscription/feed%2Fhttps%3A%2F%2Fblog.spencerwoo.com%2Fposts%2Findex.xml"><img src="https://img.shields.io/badge/dynamic/json?color=2bb24c&label=Feedly%20RSS%20Subscribes&query=%24.data.totalSubs&url=https%3A%2F%2Fapi.spencerwoo.com%2Fsubstats%2F%3Fsource%3Dfeedly%26queryKey%3Dhttps%3A%2F%2Fblog.spencerwoo.com%2Fposts%2Findex.xml&logo=feedly&style=for-the-badge" alt="" style="display: inline; margin: 0 0.1rem 0 0; width: auto;"></a></p>
<pre><code>https://img.shields.io/badge/dynamic/json?color=2bb24c&amp;label=Feedly%20RSS%20Subscribes&amp;query=%24.data.totalSubs&amp;url=https%3A%2F%2Fapi.spencerwoo.com%2Fsubstats%2F%3Fsource%3Dfeedly%26queryKey%3Dhttps%3A%2F%2Fblog.spencerwoo.com%2Fposts%2Findex.xml&amp;logo=feedly&amp;style=for-the-badge</code></pre><h3 id="多个平台串联请求">多个平台串联请求</h3>
<p>当然，我们可以用 <code>|</code> 串联多个请求，比如我同时请求 Feedly、Inoreader 中订阅我 RSS 链接的用户数量：</p>
<pre><code class="language-http">GET /?source=feedly|inoreader&amp;queryKey=https://blog.spencerwoo.com/feed.xml</code></pre>
<p>我们会得到如下数据（截至发文 Inoreader 的 API 尚未实现，我正在咨询 Inoreader 平台方是否提供 API 接口）：</p>
<pre><code class="language-json">{
  &quot;status&quot;: 200,
  &quot;data&quot;: {
    &quot;totalSubs&quot;: 14,
    &quot;subsInEachSource&quot;: { &quot;feedly&quot;: 14, &quot;inoreader&quot;: 0 },
    &quot;failedSources&quot;: {
      &quot;inoreader&quot;: &quot;Not implemented&quot;
    }
  }
}</code></pre>
<p><a href="https://feedly.com/i/subscription/feed%2Fhttps%3A%2F%2Fblog.spencerwoo.com%2Fposts%2Findex.xml"><img src="https://img.shields.io/badge/dynamic/json?label=RSS%20subs&query=%24.data.totalSubs&url=https%3A%2F%2Fapi.spencerwoo.com%2Fsubstats%2F%3Fsource%3Dfeedly%257Cinoreader%26queryKey%3Dhttps%3A%2F%2Fblog.spencerwoo.com%2Fposts%2Findex.xml&color=ffa500&logo=rss&style=for-the-badge" alt="" style="display: inline; margin: 0 0.1rem 0 0; width: auto;"></a></p>
<h3 id="多平台多请求参数串联请求">多平台多请求参数串联请求</h3>
<p>当每个平台的请求参数（用户名）不一样时，我们可以串联多个请求参数并行请求，比如我希望统计「少数派」平台和「Twitter」平台的粉丝，但是我在这两个平台上面的用户名分别是 <code>spencerwoo</code> 和 <code>realSpencerWoo</code>，我们即可用下面的方法构造请求：</p>
<pre><code class="language-http">GET /?source=sspai&amp;queryKey=spencerwoo&amp;source=twitter&amp;queryKey=realSpencerWoo</code></pre>
<p>我们会得到如下数据：</p>
<pre><code class="language-json">{
  &quot;status&quot;: 200,
  &quot;data&quot;: {
    &quot;totalSubs&quot;: 756,
    &quot;subsInEachSource&quot;: { &quot;sspai&quot;: 636, &quot;twitter&quot;: 120 },
    &quot;failedSources&quot;: {}
  }
}</code></pre>
<p>这样，我们即可非常轻松的构造这样的三个 Badge：</p>
<p><a href="https://api.spencerwoo.com/substats/?source=sspai&queryKey=spencerwoo&source=twitter&queryKey=realSpencerWoo"><img src="https://img.shields.io/badge/dynamic/json?label=Social%20media&query=%24.data.totalSubs&url=https%3A%2F%2Fapi.spencerwoo.com%2Fsubstats%2F%3Fsource%3Dsspai%26queryKey%3Dspencerwoo%26source%3Dtwitter%26queryKey%3DrealSpencerWoo&color=brightgreen&style=for-the-badge" alt="" style="display: inline; margin: 0 0.1rem 0 0; width: auto;"></a>
<a href="https://api.spencerwoo.com/substats/?source=sspai&queryKey=spencerwoo&source=twitter&queryKey=realSpencerWoo"><img src="https://img.shields.io/badge/dynamic/json?label=%E5%B0%91%E6%95%B0%E6%B4%BE&query=%24.data.subsInEachSource.sspai&url=https%3A%2F%2Fapi.spencerwoo.com%2Fsubstats%2F%3Fsource%3Dsspai%26queryKey%3Dspencerwoo%26source%3Dtwitter%26queryKey%3DrealSpencerWoo&color=d71a1b&style=for-the-badge" alt="" style="display: inline; margin: 0 0.1rem 0 0; width: auto;"></a>
<a href="https://api.spencerwoo.com/substats/?source=sspai&queryKey=spencerwoo&source=twitter&queryKey=realSpencerWoo"><img src="https://img.shields.io/badge/dynamic/json?label=Twitter&query=%24.data.subsInEachSource.twitter&url=https%3A%2F%2Fapi.spencerwoo.com%2Fsubstats%2F%3Fsource%3Dsspai%26queryKey%3Dspencerwoo%26source%3Dtwitter%26queryKey%3DrealSpencerWoo&color=1da1f2&logo=twitter&style=for-the-badge" alt="" style="display: inline; margin: 0 0.1rem 0 0; width: auto;"></a></p>
<h2 id="小结">小结</h2>
<p>这些就是 Substats 的特别之处，Substats 不仅整合了原服务复杂的 API，还拥有方便的请求构建方法。与 <a href="https://shields.io/">Shields.io</a> 配合，我们可以及其方便的构造自定义 Badge。虽然当前 Substats 支持的服务平台还比较少，但是整合其他服务 API 的方法还是相当方便的，欢迎同学们帮我来共同整合其他平台，一起将 Substats 发扬壮大 ( •̀ ω •́ )✧</p>
<p><strong>最后，如果你觉得 Substats 非常棒，请不要吝啬你的 Star！你们的支持是我输出的最大动力 φ(*￣0￣)</strong></p>
<p><a href="https://github.com/spencerwooo/Substats"><img src="https://img.shields.io/github/stars/spencerwooo/Substats?logo=github&style=social" alt="" style="display: inline; margin: 0 0.1rem 0 0; width: auto;"></a></p>
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Tiny Tiny RSS：部署中的普遍问题与注意事项总结]]></title>
            <link>https://blog.spencerwoo.com/2020/03/ttrss-noteworthy/</link>
            <guid>https://blog.spencerwoo.com/2020/03/ttrss-noteworthy/</guid>
            <pubDate>Fri, 13 Mar 2020 00:00:00 GMT</pubDate>
            <content:encoded><![CDATA[<p>最近一直在折腾 Tiny Tiny RSS，包括升级、添加样式和持久化。由于我是直接使用了 Awesome TTRSS 项目提供的 <code>docker-compose.yml</code> 部署的 Docker 版本 Tiny Tiny RSS，所以一些经验还是具有普遍性、值得分享的。这里我总结一下我在部署 Tiny Tiny RSS 过程中遇到的一些「普遍问题」和「注意事项」。</p>
<p>关于如何在自己的服务器上面部署 Tiny Tiny RSS：</p>
<ul>
<li><a href="https://blog.spencerwoo.com/2019/11/tiny-tiny-rss/">Tiny Tiny RSS | 最速部署私有 RSS 服务器</a>：我的博客</li>
<li><a href="https://sspai.com/post/57498">找不到满意的 RSS 服务？你可以自己搭建一个</a>：我的少数派文章</li>
</ul>
<p>:::warning RSS 订阅
另外，订阅我博客 RSS 的同学（来自 Feedly 的数据）已经达到了惊人的 10 个！谢谢各位的关注，没有用 RSS 订阅的同学可以点击下方徽章直接在 Feedly 中订阅。🙇‍♂️</p>
<p><a href="https://blog.spencerwoo.com/feed.xml"><img src="https://img.shields.io/badge/subscribe%20via-RSS-ffa500?logo=rss&style=for-the-badge" alt="" style="display: inline; margin: 0 0.1rem 0 0; width: auto;"></a>
<a href="https://feedly.com/i/subscription/feed%2Fhttps%3A%2F%2Fblog.spencerwoo.com%2Fposts%2Findex.xml"><img src="https://img.shields.io/badge/dynamic/json?color=2bb24c&amp;label=subscribers&amp;query=%24.source.subscribers&amp;url=https%3A%2F%2Ffeedly.com%2Fv3%2Frecommendations%2Ffeeds%2Ffeed%252Fhttps%253A%252F%252Fblog.spencerwoo.com%252Fposts%252Findex.xml&amp;logo=feedly&style=for-the-badge" alt="" style="display: inline; margin: 0 0.1rem 0 0; width: auto;"></a>
:::</p>
<h2 id="如何直接更新最新版本的-tiny-tiny-rss">如何直接更新最新版本的 Tiny Tiny RSS</h2>
<p>首先明确一下，更新 Awesome TTRSS 至最新版时，实际上更新了全部组件，包括 Tiny Tiny RSS 本体、主题、插件等等。比如这次更新（2020 年 2 月更新）就将 <a href="https://github.com/levito/tt-rss-feedly-theme">Feedly 主题</a> 最新的更新中加入的 <code>feedly-cozy.css</code>、<code>feedly-sepia.css</code> 等等主题全部加入了。</p>
<p><img src="https://cdn.spencer.felinae98.cn/blog/2020/07/20200722-220455.png" alt="更新之后加入的最新的 Feedly 主题"></p>
<p>我推荐大家手动执行更新。更新 Tiny Tiny RSS 本体：</p>
<pre><code class="language-bash">docker pull wangqiru/ttrss:latest</code></pre>
<p>更新 Mercury 和 OpenCC 服务：</p>
<pre><code class="language-bash">docker pull wangqiru/mercury-parser-api:latest
docker pull wangqiru/opencc-api-server:latest</code></pre>
<p>之后，重启 Tiny Tiny RSS 服务即可：</p>
<pre><code class="language-bash">docker-compose up -d</code></pre>
<p>Awesome TTRSS 项目中提供了 Watch Tower 项目，但是我并不推荐大家通过这一工具进行自动更新，因为 Watch Tower 会将你的全部 Docker 容器更新，可能会导致其他环境的不兼容。</p>
<h2 id="如何在编辑-docker-compose-文件之后重启-tiny-tiny-rss">如何在编辑 docker-compose 文件之后重启 Tiny Tiny RSS</h2>
<p>无论是重新配置了何种环境，是重新映射的 Volume 还是添加了 <code>docker-compose.yml</code> 设置，我都推荐直接用这一命令重启 Tiny Tiny RSS 服务：</p>
<pre><code class="language-bash">docker-compose up -d</code></pre>
<p>如果你先运行停止 Docker 容器（<code>docker-compose down</code>、<code>docker-compose rm</code>）的命令，我都遇到过数据库丢失的情况，因此只要没有特殊需要，<strong>我们都可以只使用上面的命令重启 Tiny Tiny RSS 服务。</strong></p>
<h2 id="如何正确的配置-fever-emulation-api">如何正确的配置 Fever Emulation API</h2>
<p>通常，为了适配第三方 RSS 阅读器比如 Reeder，我们需要使用 Fever Emulation API 进行「登录」。首先我们必须开启 Tiny Tiny RSS 外部 API 访问的权限。在 Preference » General » Enable API 处开启：</p>
<p><img src="https://cdn.spencer.felinae98.cn/blog/2020/07/20200722-220455-1.png" alt="设置允许通过外部 API 访问 Tiny Tiny RSS"></p>
<p>Fever Emulation 的 API 包含有三个部分：</p>
<ul>
<li>你的 Fever API 地址（通常是你的服务器域名 + <code>/plugins/fever/</code>）</li>
<li>你的登录用户名</li>
<li>你的 Fever API 密码</li>
</ul>
<p>在 Preferences » Personal data / Authentication » Personal data » Full name 处设置的用户名就是你 <strong>Fever API 的用户名</strong>：</p>
<p><img src="https://cdn.spencer.felinae98.cn/blog/2020/07/20200722-220455-2.png" alt="找到你的用户名"></p>
<p>在 Preferences » Fever Emulation 处你可以找到：</p>
<ol>
<li>Fever API 地址</li>
<li>设置 Fever API 密码的地方</li>
</ol>
<p><img src="https://cdn.spencer.felinae98.cn/blog/2020/07/20200722-220455-3.png" alt="获取 Fever API 地址并设置 Fever Emulation 密码"></p>
<p>要知道，设置 Fever API 的密码时点击 Set Password 并不会提示「成功」，不过你可以从 XHR 请求中看到 Password saved 的 response，<strong>所以其实你的密码已经保存啦</strong>。用这里设置的 API 地址、用户名和密码，我们就能成功登录 Reeder 等 RSS 阅读器了。</p>
<h2 id="如何设置分栏视图">如何设置分栏视图</h2>
<p>「分栏视图」就是类似笔记应用的一个侧边栏显示订阅文章列表，另一侧显示文章内容的视图。你可以：</p>
<ol>
<li><p>在 Preferences » Preferences » Articles 中关闭 Combined mode 的设置：</p>
<p><img src="https://cdn.spencer.felinae98.cn/blog/2020/07/20200722-220455-4.png" alt="关闭 Combined mode"></p>
</li>
<li><p>回到主界面，在右上角的汉堡键 » Toggle widescreen mode 处点击打开宽屏模式：</p>
<p><img src="https://cdn.spencer.felinae98.cn/blog/2020/07/20200722-220455-5.png" alt="打开宽屏模式"></p>
</li>
</ol>
<p>这样 Tiny Tiny RSS 就会以分栏视图显示文章列表和文章内容了。</p>
<p><img src="https://cdn.spencer.felinae98.cn/blog/2020/07/20200722-220455-6.png" alt="Tiny Tiny RSS 分栏视图"></p>
<p>另外，我的基于 Feedly 修改的主题配置文件：<a href="https://gist.github.com/spencerwooo/7a373a3c921a50953ec12f329452ee27">GitHub - Gist</a></p>
<h2 id="如何正确设置-mercury-全文抓取和-opencc-繁简转换-api">如何正确设置 Mercury 全文抓取和 OpenCC 繁简转换 API</h2>
<p>如果你使用了 Awesome TTRSS 中包含的 Mercury 全文抓取和 OpenCC 繁简转换 API，那么你应该在 <code>docker ps</code> 的输出中看到这两个服务的身影：</p>
<p><img src="https://cdn.spencer.felinae98.cn/blog/2020/07/20200722-220455-7.png" alt="docker ps 的输出"></p>
<p>如果你这两个服务的配置和原配置一致：</p>
<pre><code class="language-yaml">service.mercury:
  image: wangqiru/mercury-parser-api:latest
  container_name: mercury
  expose:
    - 3000
  restart: always

service.opencc:
  image: wangqiru/opencc-api-server:latest
  container_name: opencc
  environment:
    NODE_ENV: production
  expose:
    - 3000
  restart: always</code></pre>
<p>那么，你只需要在 Tiny Tiny RSS 的 Preferences 中开启这两个插件，并将 API 地址依次设置为如下即可。（Docker 会自动探索相应的服务 API 地址。）</p>
<table>
<thead>
<tr>
<th align="center">Mercury</th>
<th align="center">OpenCC</th>
</tr>
</thead>
<tbody><tr>
<td align="center"><code>service.mercury:3000</code></td>
<td align="center"><code>service.opencc:3000</code></td>
</tr>
<tr>
<td align="center"><img src="https://cdn.spencer.felinae98.cn/blog/2020/07/Snipaste_2020-07-22_22-13-39.png" alt="Mercury 全文抓取"></td>
<td align="center"><img src="https://cdn.spencer.felinae98.cn/blog/2020/07/Snipaste_2020-07-22_22-14-10.png" alt="OpenCC 繁简转换"></td>
</tr>
</tbody></table>
<p>注意，你需要在每一个订阅源中明确指定使用 Mercury 或 OpenCC 服务（右键编辑），才可以真正保证服务的准确运行。</p>
<p><img src="https://cdn.spencer.felinae98.cn/blog/2020/07/20200722-220455-9.png" alt="编辑订阅源，开启 Mercury 或 OpenCC 服务"></p>
<h2 id="如何调试、查看-tiny-tiny-rss-与其他容器服务的-log">如何调试、查看 Tiny Tiny RSS 与其他容器服务的 log</h2>
<p>Docker 容器的 log 查看非常简单。如果我们想用 <code>docker-compose</code> 查看整个 Awesome TTRSS 的 log：</p>
<pre><code class="language-bash"># 查看 docker-compose 的 log 后 5 条
docker-compose logs --tail 5</code></pre>
<p><img src="https://cdn.spencer.felinae98.cn/blog/2020/07/20200722-220455-10.png" alt="查看 docker-compose 启动的全部服务的 log"></p>
<p>我们可以用下面的命令查看单个服务的 log：</p>
<pre><code class="language-bash"># 查看 Mercury 服务后 5 条
docker logs mercury --tail 5</code></pre>
<p><img src="https://cdn.spencer.felinae98.cn/blog/2020/07/20200722-220455-11.png" alt="查看 Mercury 全文抓取的 log"></p>
<p>另外，我们也可以用下面的命令查看实时更新的 log：</p>
<pre><code class="language-bash"># 查看实时更新的 Mercury 服务（显示 10 条）
docker logs mercury --tail 10 -f</code></pre>
<p><img src="https://cdn.spencer.felinae98.cn/blog/2020/07/20200722-220455-12.png" alt="查看实时更新的 Mercury 服务"></p>
<h2 id="如何持久化-tiny-tiny-rss-图标">如何持久化 Tiny Tiny RSS 图标</h2>
<p>升级更新了 Tiny Tiny RSS 服务之后，每次订阅源的图标都会丢失。我们可以通过将 Tiny Tiny RSS 的图标文件夹挂载到 Docker 容器之外来「持久化」图标存储。</p>
<p>在 <code>docker-compose.yml</code> 中，将 Tiny Tiny RSS 的 Docker 容器配置 <code>service.rss</code> 中添加如下的配置：</p>
<pre><code class="language-yaml">volumes:
  - ~/awesome-ttrss/feed-icons/:/var/www/feed-icons/ # mount feed icons to local machine</code></pre>
<p>这样即可将 Docker 容器里面 <code>/var/www/feed-icons/</code> 文件夹里面的内容挂载到外面服务器上的 <code>~/awesome-ttrss/feed-icons/</code> 文件夹。</p>
<p>另外，我们还需要给 <code>~/awesome-ttrss/feed-icons/</code> 文件赋予合适的权限：</p>
<pre><code class="language-bash">sudo chmod -R 777 feed-icons</code></pre>
<p>这样，Tiny Tiny RSS 下载的 favicon 就全部挂载到本机，更新 Docker 容器也不会对这些内容造成任何影响了。</p>
<p><img src="https://cdn.spencer.felinae98.cn/blog/2020/07/20200722-220455-13.png" alt="将订阅源的 icon 挂载到容器外部"></p>
<p>感谢阅读。🙇‍♂️</p>
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Remote Jupyter Lab：如何用 Jupyter Lab 将远程服务器资源最大化利用]]></title>
            <link>https://blog.spencerwoo.com/2020/03/remote-jupyter-lab/</link>
            <guid>https://blog.spencerwoo.com/2020/03/remote-jupyter-lab/</guid>
            <pubDate>Thu, 12 Mar 2020 00:00:00 GMT</pubDate>
            <content:encoded><![CDATA[<blockquote>
<p>🎃 <strong>编者按：</strong></p>
<p>本文原文以英文发表于 Medium - SpencerWeekly，本处仅为援引。本文将不会被翻译，也不会被转载至此中文博客。</p>
</blockquote>
<p><strong>Spencer Weekly</strong>: <a href="https://medium.com/spencerweekly/remote-jupyter-lab-how-to-utilize-jupyter-lab-to-its-fullest-on-a-remote-server-2a359159d2f6">Remote Jupyter Lab: how to utilize Jupyter Lab to its fullest on a remote server?</a></p>
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[「输出」开销：为什么我们向 stdout 输出时那么慢？]]></title>
            <link>https://blog.spencerwoo.com/2020/02/stdout-overhead/</link>
            <guid>https://blog.spencerwoo.com/2020/02/stdout-overhead/</guid>
            <pubDate>Sat, 22 Feb 2020 00:00:00 GMT</pubDate>
            <content:encoded><![CDATA[<blockquote>
<p>本文发布于 Medium: <a href="https://medium.com/spencerweekly/console-output-overhead-why-is-writing-to-stdout-so-slow-b0cc7c88704c">Console output overhead: why is writing to stdout so slow?</a></p>
</blockquote>
<p>Generally, when we code through our projects, we need to output certain values in order to understand what the program is currently doing and what results it shows. Normally we do this with a simple print function:</p>
<pre><code class="language-python">print(&#39;Hey look, this is the result.&#39;)</code></pre>
<p>And for most of our use cases, this would be more than enough. However, when we are outputting a huge amount of data, like printing a file with 10k lines of text onto the terminal, we will eventually suffer from IO bottlenecks. This is something I discovered when trying to find a nice progress bar library for PyTorch in order to monitor the model’s training progress. Let me explain.</p>
<h2 id="what-did-i-discover">What did I discover?</h2>
<p>Now, if you are familiar with modern neural network libraries, you may have heard of Keras. Keras is a highly encapsulated library that takes care of data output, model compilation, backward propaganda and more for you under the hood, meaning that it offers less agility than other low-level libraries like PyTorch. The one feature I really appreciate about Keras is its nice little training progress bar that shows the current training progress, iteration duration, ETA, loss and other relevant statistics. Below is an example.</p>
<p><img src="https://cdn.spencer.felinae98.cn/blog/2020/07/20200722-215810.png" alt="Visualizing training progress with Keras"></p>
<p>However, PyTorch offers far more customizable features when it comes to designing, compiling and training your model, but it also means you have to take care of outputting progress, training details and others by yourself. This is when I discovered IO bottlenecks when outputting directly to <code>stdout</code>, that is, your terminal.</p>
<p>The library I first tried is called <a href="https://github.com/yueyericardo/pkbar"><strong>pkbar</strong> — a progress bar library intended to bring Keras style progress monitoring to the PyTorch ecosystem</a>. <strong>(Don’t use it!)</strong> It’s outputs are like this:</p>
<p><img src="https://cdn.spencer.felinae98.cn/blog/2020/07/20200722-215810-1.png" alt="pkbar: Keras style progress bar for PyTorch"></p>
<p>Most progress bar libraries are implemented by wrapping itself around an iterative object, in our case, a PyTorch Data Loader. When building a progress bar, we need to refresh our progress, statistics and other parameters on each iteration. This means every time we come to the end of an iteration, we need to output to our terminal with a delay when the program waits for the writing to return. This delay is often unnoticed when we are simply writing a single message, but when we are iterating through thousands of images, we need to write the latest status on each iteration. A small delay multiplied by a thousand iterations gives us a huge time difference, and results in bottlenecks we encounter now.</p>
<p><img src="https://cdn.spencer.felinae98.cn/blog/2020/07/20200722-215810-2.png" alt="Each iteration comes with a time cost that adds up to be a huge delay"></p>
<p>What happened to me when I was using pkbar was exactly what I described above: <strong>each iteration was accompanied with a delay which eventually builds up into a time difference I simply can’t ignore.</strong> To put numbers into perspective, I was applying a DeepFool adversarial attack on a pre-trained ResNet18 CNN. It took about 5 minutes on my laptop running on GPU without pkbar, and when I added pkbar to visualize progress, it gave me an ETA of nearly 20 minutes! Holy crap, I removed pkbar almost immediately.</p>
<h2 id="so-why-were-there-delays">So why were there delays?</h2>
<p>In order to find out what was causing this huge delay and how we could avoid it, I turned for help on Stack Overflow. A particular question gave me the answer: <a href="https://stackoverflow.com/questions/3857052/why-is-printing-to-stdout-so-slow-can-it-be-sped-up">Why is printing to stdout so slow? Can it be sped up?</a></p>
<p>In this question, the author compared the following speeds:</p>
<ul>
<li>Writing output to the terminal, which is <code>stdout</code></li>
<li>Writing output to a file</li>
<li>Writing output to <code>stdout</code>, but with <code>stdout</code> redirected to <code>/dev/null</code></li>
</ul>
<p><img src="https://cdn.spencer.felinae98.cn/blog/2020/07/20200722-215810-3.png" alt="Time cost for printing to stdout, file and /dev/null"></p>
<p>Wow, that shows a huge difference for printing to <code>stdout</code> and redirecting <code>stdout</code> to <code>/dev/null</code>. Even writing output to file is faster than directly writing to the terminal. So…, why?</p>
<blockquote>
<p>Congratulations, you have just discovered the importance of I/O buffering. :-)</p>
</blockquote>
<p>So it turns out that I/O buffering is what made even “writing to file” faster than “writing to <code>stdout</code>”. When we are directly writing outputs to our terminal, each writing operation is being done “synchronously”, which means our programs waits for the “write” to complete before it continues to the next commands.</p>
<p><img src="https://cdn.spencer.felinae98.cn/blog/2020/07/20200722-215810-4.png" alt="Synchronous writing to stdout"></p>
<p>Each time our programs writes something to <code>stdout</code>, we are met with this delay. However, writing to files are not of the same case. When we are writing to files, we have what is known as “I/O buffering”, which means the program outputs whatever it needs to write to the file, and the OS catches all these contents to write where it then stores in a file <strong>as a bulk</strong> afterwards. <strong>All our “write” functions return before anything is actually written to a file.</strong></p>
<p><img src="https://cdn.spencer.felinae98.cn/blog/2020/07/20200722-215810-5.png" alt="Buffered writing output to file"></p>
<p>So what should we do here? Well, in order to avoid I/O overhead, we either have to do training and update output in parallel or asynchronously like we do in GUIs when we have an UI thread and a controller, or we will need to write in bulks to <code>stdout</code> like what we are doing when writing to files. <strong>It’s a trade-off: interactivity versus bulk efficiency.</strong></p>
<h2 id="what-i-did-to-visualize-pytorch-training-with-minimum-overhead">What I did to visualize PyTorch training with minimum overhead?</h2>
<p>After I ditched pkbar, I found a perfect progress bar library: <a href="https://github.com/tqdm/tqdm">tqdm — A Fast, Extensible Progress Bar for Python and CLI</a>. With over 13.4k+ stars, <code>tqdm</code> is easily the best Python library for us to implement training progress visualization.</p>
<p><img src="https://cdn.spencer.felinae98.cn/blog/2020/07/20200722-215810-6.png" alt="tqdm in action"></p>
<p><code>tqdm</code> is simple, efficient and comes with minimal overhead. The author claims that <code>tqdm</code> only delays output for a minimum of 60ns per iteration. That’s impressive. In addition to its low overhead, <code>tqdm</code> uses smart algorithms to predict the remaining time and to skip unnecessary iteration displays, which allows for a negligible overhead in most cases.</p>
<h3 id="installation">Installation</h3>
<p>Either install <code>tqdm</code> using conda directly:</p>
<pre><code class="language-bash">conda install -c conda-forge tqdm</code></pre>
<p>And install <code>ipywidgets</code> with pip to enable Jupyter Notebook integration:</p>
<pre><code class="language-bash">pip install ipywidgets</code></pre>
<p>Or integrate the following code to your <code>environment.yml</code>:</p>
<pre><code class="language-yaml">channels:
  - conda-forge
  - defaults
dependencies:
  - tqdm
  - pip:
      - ipywidgets</code></pre>
<p>Then run:</p>
<pre><code class="language-bash">conda env update</code></pre>
<h3 id="integration">Integration</h3>
<p>We can easily integrate <code>tqdm</code> into our PyTorch project. Basically what you need to do is just to wrap <code>tqdm</code> on an iterable object, and that’s it.</p>
<pre><code class="language-python">from tqdm import tqdm
for i in tqdm(range(10000)):
    ...</code></pre>
<pre><code>76%|████████████████████████████ | 7568/10000 [00:33&lt;00:10, 229.00it/s]</code></pre><p>For PyTorch <code>DataLoader</code>, we can first load our data like so:</p>
<pre><code class="language-python">dataset_loader = torch.utils.data.DataLoader(dataset, batch_size=4)</code></pre>
<p>Then initialize <code>tdqm</code>:</p>
<pre><code class="language-python">pbar = tqdm(dataset_loader)</code></pre>
<p>Here, we can add the description of the progress bar:</p>
<pre><code class="language-python">pbar.set_description(&#39;Validate predictions&#39;)</code></pre>
<p>Also, we can initialize the post-fix dict for the progress bar, so that we can update the post-fix on the fly:</p>
<pre><code class="language-python">pbar.set_postfix(loss=&#39;0.0%&#39;, acc=&#39;0.0%&#39;)</code></pre>
<p>And Voilà! We can directly iterate through <code>tdqm</code>, just like we did with PyTorch <code>DataLoader</code>. (My model is wrapped with Foolbox, so I can make predictions with the <code>.forward()</code> function.)</p>
<pre><code class="language-python">for image, label in pbar:
  # make a prediction
  prob = model.forward(image.numpy())

  ...  # calculate and update loss
  pbar.set_postfix(loss=&#39;{:.2f}%&#39;.format(loss))

  # calculate and update accuracy
  pbar.set_postfix(acc=&#39;{:.2f}%&#39;.format(acc))</code></pre>
<h3 id="features">Features</h3>
<p><code>tqdm</code> can be used directly in a CLI environment with no special configuration whatsoever.</p>
<p><img src="https://cdn.spencer.felinae98.cn/blog/2020/07/20200722-215810-7.gif" alt="tqdm in a CLI enviroment"></p>
<p>Also, we can run <code>tqdm</code> in VS Code’s Python Interactive console, or Jupyter Notebook. In this case, <code>tqdm</code> will output progress asynchronously according to the <a href="https://jupyter-notebook.readthedocs.io/en/stable/examples/Notebook/Running%20Code.html#Output-is-asynchronous">Notebook API</a>.</p>
<p><img src="https://cdn.spencer.felinae98.cn/blog/2020/07/20200722-215810-8.gif" alt="tqdm outputting inside Jupyter Notebook / VS Code Python Interactive"></p>
<p>To kick it up a notch, we can actually output “real” progress bars. Utilizing <code>ipywidget</code>, we can directly draw interactive progress bars inside Jupyter Notebook. <a href="https://ipywidgets.readthedocs.io/en/latest/user_install.html">See here for instructions on how to enable </a><code>ipywidgets</code><a href="https://ipywidgets.readthedocs.io/en/latest/user_install.html">.</a></p>
<p>After enabling these widgets inside Jupyter Notebook, all we have to do is change the way on how we imported <code>tqdm</code>. Change:</p>
<pre><code class="language-python">import tqdm as tqdm</code></pre>
<p>To:</p>
<pre><code class="language-python">import tqdm.notebook as tqdm</code></pre>
<p>Now, run the code once more, if everything goes well, we should be able to see a neat progress bar with colors indicating the current task’s state: pending, success or failure.</p>
<p><img src="https://cdn.spencer.felinae98.cn/blog/2020/07/20200722-215810-9.gif" alt="tqdm integrating directly into Jupyter Notebook with ipywidgets - https://ipywidgets.readthedocs.io/en/latest"></p>
<hr>
<p>This is the end of the article, for more information on how to utilize <code>tqdm</code>, I recommend this article: <a href="https://medium.com/better-programming/python-progress-bars-with-tqdm-by-example-ce98dbbc9697">Python Progress Bars with tqdm by Example</a>. Also, the official documentation of <code>tqdm</code> is quite thorough, covering most cases when using <code>tqdm</code>.</p>
<p>There’s one thing to note, and that is: you should not use “print” to output anything when a <code>tqdm</code> progress bar is running, as this behavior will mess up the <code>tqdm</code> output. If you have to write something, do remember to use:</p>
<pre><code class="language-python">tqdm.write(&lt;context&gt;)</code></pre>
<p>That’s all, thanks for reading.</p>
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[别用 Anaconda：如何搭建一个 decent 的机器学习开发环境？]]></title>
            <link>https://blog.spencerwoo.com/2020/02/dont-use-anaconda/</link>
            <guid>https://blog.spencerwoo.com/2020/02/dont-use-anaconda/</guid>
            <pubDate>Sat, 08 Feb 2020 00:00:00 GMT</pubDate>
            <content:encoded><![CDATA[<blockquote>
<p>🎃 <strong>编者按：</strong></p>
<p>本文原文以英文发表于 Medium - SpencerWeekly，本处仅为援引。英文文章将优先发布于 Medium，如果有机会我将在这里将文章翻译为中文。</p>
</blockquote>
<p><strong>Spencer Weekly</strong>: <a href="https://medium.com/spencerweekly/dont-use-anaconda-how-to-setup-a-decent-machine-learning-environment-a69b19c24918">Don’t use Anaconda: How to setup a decent machine learning environment?</a></p>
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[cwim 背后的故事：Rust 与 Ownership]]></title>
            <link>https://blog.spencerwoo.com/2020/01/cli-app-in-rust/</link>
            <guid>https://blog.spencerwoo.com/2020/01/cli-app-in-rust/</guid>
            <pubDate>Thu, 23 Jan 2020 00:00:00 GMT</pubDate>
            <content:encoded><![CDATA[<p>前几天 Stack Overflow 博客上面有一篇文章[^1]，里面提到了 Rust 已经连续四年位列 Stack Overflow 社区最爱编程语言榜首。的确，Rust 是一门神奇又美丽的语言。Rust 是一门标榜 safe 与 zero-cost abstraction 的语言[^2]，意味着只要你编写的 Rust 代码符合官方标准 —— 能够通过编译 —— 那么你的项目几乎可以肯定地说是内存安全的。</p>
<h2 id="初衷">初衷</h2>
<p>偏向底层的 Rust 让我在之前一直没有机会尝试，毕竟我相信国内高校没有一所是敢直接抛弃 C、C++ 而使用 Rust 作为其主语言进行授课的。最近我重构了 <a href="https://dowww.spencerwoo.com">Dev on Windows with WSL</a>：一个近 2w 字的 WSL 开发配置文档。我前后用了大概半个月的时间，增加了许多内容，因此我在结束编写工作之后，试图找到一个类似 <code>cloc</code>，能帮我统计一个目录下全部 Markdown 文件的命令行工具。很失望，没找到。</p>
<p>我决定自己尝试实现这个命令行工具，当然，我也相信 Python、Node.js、Ruby 等脚本语言一定适合做这些事情，毕竟 <code>cloc</code> 本身就是使用 Perl 实现的。不过，Rust 作为一门高效的、静态的、可以直接编译到三个操作系统的底层语言，还是很有吸引力的。因此我才决定使用 Rust 开新坑。</p>
<p><img src="https://cdn.spencer.felinae98.cn/blog/2020/07/20200722-215236.png" alt="使用 Rust 实现的 cwim - Count words in Markdown"></p>
<p>另外：cwim 的第一个小版本我已经编译并发布 Release 版本，有兴趣的同学可以前往 GitHub 查看：<a href="https://github.com/spencerwooo/cwim">spencerwooo/cwim</a>.</p>
<h2 id="开始一个-rust-项目">开始一个 Rust 项目</h2>
<p>Rust 最 beginner friendly 的地方我觉得在于其<a href="https://www.rust-lang.org/zh-CN/learn/get-started">官方入门文档</a>的简洁易懂。从安装、编译到包管理、打包项目 …… Rust 的官方文档讲解的都比任何其他语言的文档讲解的要易懂不少。我这里简单记录一下 Rust 环境的安装搭建的基本过程。</p>
<h3 id="安装-rust-环境">安装 Rust 环境</h3>
<p>Rust 语言虽然小众，但是其生态相当完善。Rust 借鉴了其他语言的多种环境配置工具，官方直接提供了一整套完善的闭环 Toolkit，基本能满足我们在使用 Rust 时的安装、构建、编译、发布的整套流程：</p>
<ul>
<li>Rustup：Rust 版本管理（类似 Python 的 <code>pyenv</code>、Node.js 的 <code>nvm</code> 等）</li>
<li>Cargo：Rust 构建工具与包管理（类似 Python 的 <code>pip</code>、Node.js 的 <code>yarn</code> 等）</li>
<li>crates.io：Rust 的 Package Registry</li>
</ul>
<p>首先，我们安装 Rustup：Rust 安装器与 Rust 版本控制器。使用 Arch Linux（以及 WSL 的 Arch Linux）的同学可以直接在 AUR 中安装 <code>rustup</code>：</p>
<pre><code class="language-bash">yay rustup</code></pre>
<p>如果使用 WSL 其他 Linux 发行版，我们也可以用下面的命令安装 <code>rustup</code>：</p>
<pre><code class="language-bash">curl --proto &#39;=https&#39; --tlsv1.2 -sSf https://sh.rustup.rs | sh</code></pre>
<p>其他安装方法请参考：<a href="https://www.rust-lang.org/zh-CN/tools/install">Rust | 安装 Rust</a></p>
<p>安装了 <code>rustup</code> 之后，我们就应该已经安装完成了 Cargo：Rust 的构建工具与包管理工具。Cargo 可以做很多事情：</p>
<pre><code class="language-bash">cargo build     # 可以构建项目
cargo run       # 可以运行项目
cargo test      # 可以测试项目
cargo doc       # 可以为项目构建文档
cargo publish   # 可以将库发布到 crates.io</code></pre>
<p>要检查是否安装了 Rust 和 Cargo，可以在终端中运行：</p>
<pre><code class="language-bash">cargo --version</code></pre>
<p>接下来我们就可以使用 Cargo 来创建一个 Rust 项目，并用它来安装我们必须的 Rust 库等内容。</p>
<h3 id="rust--vs-code">Rust / VS Code</h3>
<p>VS Code 是一个通用的文本 / 代码编辑器，能够通过插件支持多种语言环境下代码的编写任务。我们下载 Rust 官方提供的 VS Code 插件：<a href="https://marketplace.visualstudio.com/items?itemName=rust-lang.rust">Visual Studio Code Marketplace | Rust (rls)</a></p>
<p><img src="https://cdn.spencer.felinae98.cn/blog/2020/07/20200722-215236-1.png" alt="Visual Studio Code Marketplace | Rust (rls)"></p>
<p>之后，我们用 Cargo 创建一个新的项目 <code>hello-rust</code>：</p>
<pre><code class="language-bash">cargo new hello-rust</code></pre>
<p>或者在已有文件夹 <code>hello-rust</code> 下，生成新 Rust 项目：</p>
<pre><code class="language-bash">cd hello-rust
cargo init</code></pre>
<p>新的 Rust 项目目录下应该拥有以下内容：</p>
<pre><code class="language-bash">hello-rust      # 根目录
|- Cargo.toml   # Rust 的清单文件，其中包含项目的元数据和依赖库
|- src
  |- main.rs    # 主程序入口</code></pre>
<p>用 VS Code 打开这一目录，我们即可开始 Rust 项目的编写。</p>
<p><img src="https://cdn.spencer.felinae98.cn/blog/2020/07/20200722-215236-2.png" alt="使用 VS Code 撰写 Rust 项目"></p>
<p>使用下面命令即可运行项目：</p>
<pre><code class="language-bash">cargo run</code></pre>
<h2 id="rust-语言特性">Rust 语言特性</h2>
<p>在用 Rust 编写 <code>cwim</code> 的时候，有两个令我印象深刻的地方：一个是 Rust 语言实际上非常清晰易懂，有 C、C++ 等强类型语言的严谨，也有脚本语言的易读；另一个就是 Rust 编译器非常严格，但给我们的问题提示也非常清晰，方便追溯问题所在，容易 debug。严苛的 Rust 编译器让我们必须考虑「内存分配」，也正因如此，使得 <strong>Rust 在并未实现「垃圾回收」的前提下，确保了任何 Rust 程序都生来具有「内存安全」特性。</strong></p>
<blockquote>
<p>🧲 <strong>Rust Playground</strong></p>
<p>下面涉及到的代码内容可以在 Rust 在线 Playground 中自己尝试。链接位于：<a href="https://play.rust-lang.org">Rust Playground</a></p>
</blockquote>
<h3 id="ownership">Ownership</h3>
<p>Rust 的 Ownership（所有权）是保证 Rust 程序「内存安全」的重点特性。什么是「内存安全」？保证「内存安全」就是指或语言本身，或使用语言的开发者，在其程序运行时管理系统的「内存分配」的过程中保证内存没有浪费、没有泄露。我们一般的程序都需要实施「静态」与「动态」两种形式的内存分配，其中前者指已知变量所需空间，直接在内存中划分一部分不变的区域给变量；后者为在程序运行过程中动态地给变量分配内存空间，使得变量能够在程序运行时变化地占用内存大小。</p>
<h3 id="数据存储方式">数据存储方式</h3>
<p>Rust 是使用「栈」和「堆」这两种数据结构来对这两种内存分配形式进行划分的。为了更好的理解 Rust 的 Ownership 的工作机制，我们首先看看 Rust 是如何利用「栈」和「堆」进行内存分配。</p>
<p><img src="https://cdn.spencer.felinae98.cn/blog/2020/07/20200722-215236-3.png" alt="使用「栈」和「堆」进行内存分配"></p>
<p>首先，「栈」从实现上来说是一种效率非常高的数据结构，因为「栈」拥有「后进先出」的数据存储特点（LIFO），使得最后压入栈顶的元素会被最先从栈顶移出。这种数据结构的优势在于：当我们用「栈」来维护内存数据时，<strong>我们只需要维护「栈顶」元素的信息即可</strong>。同时，Rust 内存管理的「栈」在编译时即可知道其具体大小，静态分配内存空间即可[^3]。</p>
<p>而 Rust 的「堆」则不一样，「堆」是一个动态分配内存空间的数据结构。当我们使用「堆」分配内存空间时，我们实际上是在「堆」上寻找对应的内存地址，将之标记，并返回与之相对映的指针。这一过程跟我们 C、C++ 中的 allocate memory 的原理是一致的。</p>
<h3 id="所有权如何保证内存安全">所有权如何保证内存安全</h3>
<p>为什么 Rust 需要引入「所有权」的机制？因为 Rust 保证「内存安全」的方法是：追踪第二种「堆」结构中哪部分数据被哪部分代码使用，从而尽量减少「堆」中的重复数据，保证「堆」中不出现有未使用的数据等问题。</p>
<blockquote>
<p>Keeping track of what parts of code are using what data on the heap, minimizing the amount of duplicate data on the heap, and cleaning up unused data on the heap so you don’t run out of space are all problems that ownership addresses. —— <a href="https://doc.rust-lang.org/book/ch04-01-what-is-ownership.html">The Rust Book</a></p>
</blockquote>
<p>Rust 正是使用基于「所有权」理念的一系列规定来保证 Rust 程序的「内存安全」。这其中的机制包括：</p>
<ul>
<li>Rust 中的每个「值」都有一个被叫做 owner 的变量（所有者）</li>
<li>同一时间只能有一个 owner</li>
<li>当 owner 离开我们程序段的 scope 之后，这一「值」就会被释放掉</li>
</ul>
<p>我们来看下面的几个例子，来具体看看 Rust 如何保证「内存安全」的。</p>
<p>我们以 Rust 中字符串（String Literal）为例子，Rust 中字符串有 <code>&amp;str</code> 的静态字符串变量，以及 <code>String</code> 的动态字符串变量。我们来看看 Rust 是如何分别利用「栈」来存储 <code>&amp;str</code>、用「堆」来存储 <code>String</code> 的。</p>
<p>首先来看一个 <code>&amp;str</code> 的例子：</p>
<pre><code class="language-rust">fn main() {
  // &amp;str 用「栈」存储
  let s1 = &quot;Hello&quot;;    // 字符串 &quot;Hello&quot; 赋值给变量 s1
  let s2 = s1;         // 将变量 s1 复制并赋值给 s2
  println!(&quot;{}&quot;, s1);  // 这样做没有问题！
}</code></pre>
<p>可以发现，当我们直接使用 <code>&amp;str</code> 存储字符串时，Rust 是将前一个变量 <code>s1</code> 的值<strong>直接复制</strong>给后一个变量 <code>s2</code>，前一个变量 <code>s1</code> 并没有变化。此时我们访问前一个变量 <code>s1</code> 没有任何问题。</p>
<p>接下来，我们来看一个 <code>String</code> 的例子：</p>
<pre><code class="language-rust">fn main() {
  // String 用「堆」存储
  let s1 = String::from(&quot;Hello again&quot;); // String 变量 &quot;Hello again&quot; 赋值给 s1
  let s2 = foo;                         // 将 s1 变量中的内容「移动」到 s2 中
  println!(&quot;{}, {}&quot;, s1, s2);           // 出错了！s1 中内容被 moved，无法被 reference
}</code></pre>
<p>此时，编译器会报出 <code>error[E0382]: borrow of moved value: &#39;s1&#39;</code> 的错误。可以发现，当我们使用 String 存储字符串时，Rust 不会将变量的值「复制」，而是会将变量「移动」到目标变量中。这种情况下，Rust 会认为前一个变量 <code>s1</code> 已经不再有效（no longer valid），因此，当我们在之后试图访问 <code>s1</code> 时，Rust 就会报出这一错误。</p>
<p><img src="https://cdn.spencer.felinae98.cn/blog/2020/07/20200722-215236-4.png" alt="error[E0382] 报错"></p>
<p>为什么 Rust 在使用「堆」进行动态内存分配时，会 move 而不 copy 呢？一方面是因为 copy 的消耗是比 move 大得多的；另一方面，Rust 这一设计恰好帮助我们避免了 C 语言中非常可能遇到的一种内存泄露的问题：<strong>double free 异常</strong>[^4]。</p>
<p>Double free 异常是如何发生的？当我们使用 <code>String</code> 类型来存储字符串时，我们实际上存储了以下三个 field 的值：<code>ptr</code>、<code>len</code>、<code>capacity</code>。<code>ptr</code> 指向存储字符串内容的内存空间。比如 <code>let s1 = String::from(&quot;Hello&quot;);</code> 即声明了如下的存储方式：</p>
<p><img src="https://cdn.spencer.felinae98.cn/blog/2020/07/20200722-215236-5.png" alt="String 的存储方式"></p>
<p>我们如果使用 <code>copy</code> 将 String <code>s1</code> 复制给 <code>s2</code>，我们实际上就将三个 field 的值 <code>ptr</code>、<code>len</code> 和 <code>capacity</code> 全部复制，也就是我们的 <code>ptr</code> 指针实际上指向上一块地址空间，如下图所示：</p>
<p><img src="https://cdn.spencer.felinae98.cn/blog/2020/07/20200722-215236-6.png" alt="使用 copy 将 s1 复制给 s2"></p>
<p>前面我们提到了，_对于一个「值」来说，当程序离开「值」的 owner 所在的 scope 之后，这一「值」就会被释放掉_，那么当我们离开 <code>s1</code> 和 <code>s2</code> 所在的 scope 之后，程序则会试图将这两个「值」的内存空间全部释放，而此时 <code>s1</code> 和 <code>s2</code> 指向同一块地址空间，<strong>这种情况下就会出现 double free 异常的情况</strong>。</p>
<blockquote>
<p>Freeing memory twice can lead to memory corruption, which can potentially lead to security vulnerabilities.</p>
</blockquote>
<p>而我们 Rust 就通过「所有权」规避了这一问题，如下图所示，Rust 在上述过程中，实际上是将 <code>s1</code> 的值移动到了 <code>s2</code> 上，在 <code>s2</code> 的指针指向对应的内存空间时，Rust 会认为 <code>s1</code> 此时已经无用了，从而直接 invalidate 掉 <code>s1</code>，那么当我们程序离开当前 scope 后，valid 的「值」只有 <code>s2</code>，Rust 就只会将 <code>s2</code> 释放，从而避免出现 double free 异常的情况。</p>
<p><img src="https://cdn.spencer.felinae98.cn/blog/2020/07/20200722-215236-7.png" alt="Rust 中 s1 move 给 s2 之后，s1 被认为 invalid"></p>
<p>不过，如果我们此时一定要访问 <code>s1</code> 的内容怎么办？Rust 有一个专门的方法，让 <code>s2</code> 创建时，不 move 而是深度拷贝 <code>s1</code> 的全部内容，如下图所示：</p>
<p><img src="https://cdn.spencer.felinae98.cn/blog/2020/07/20200722-215236-8.png" alt="Rust 中将 s1 deep copy 给 s2"></p>
<p>这里 Rust 所做的事情类似于其他语言中的 deep copy —— 花费更大的开销，将 <code>s1</code> 字符串对应的「堆」复制一个，再分配内存空间存储复制出来的 <code>s1</code> 并将之赋给 <code>s2</code>。在 Rust 中我们可以用 <code>&lt;VARIABLE&gt;.clone()</code> 来表示这一功能：</p>
<pre><code class="language-rust">fn main() {
  let s1 = String::from(&quot;hello&quot;);
  let s2 = s1.clone();

  println!(&quot;s1 = {}, s2 = {}&quot;, s1, s2);
}</code></pre>
<p>上面的代码就不会出现类似的错误了。Rust 语言<strong>对「动态」数据结构</strong>都有类似的功能安排：利用 Ownership 的设计思想，在没有垃圾回收的基础之上，避免内存错误。</p>
<h2 id="小结">小结</h2>
<p>Rust 的确是一门神奇的语言，不仅拥有 C、C++ 等系统级别语言的高效迅速，还利用 Ownership 的设计思想保证了内存安全。上面仅仅是 Rust 语言中一个小小的独特之处，由于这一特性所保证的功能我在其他语言中也有过类似的体验（比如 Python 的 deep copy 与 shallow copy[^5]），因此拿来和大家分享。Rust 还有更多有趣的设计与内容等待大家发掘。感谢阅读。</p>
<p>[^1]: <a href="https://stackoverflow.blog/2020/01/20/what-is-rust-and-why-is-it-so-popular/">What is Rust and why is it so popular? - Stack Overflow Blog</a>
[^2]: <a href="https://www.reddit.com/r/rust/comments/5lg3ih/what_do_rusts_buzzwords_like_safe_and_zerocost/">What do Rust&#39;s buzzwords like &quot;safe&quot; and &quot;zero-cost abstraction&quot; mean?</a>
[^3]: <a href="https://medium.com/@thomascountz/ownership-in-rust-part-1-112036b1126b">Ownership in Rust, Part 1 - Medium</a>
[^4]: <a href="https://doc.rust-lang.org/book/ch04-01-what-is-ownership.html">The Rust Programming Language - Understanding Ownership</a>
[^5]: <a href="https://www.geeksforgeeks.org/copy-python-deep-copy-shallow-copy/">copy in Python (Deep Copy and Shallow Copy) - GeeksforGeeks</a></p>
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[BIT-Web Automation：如何利用 iOS 快捷指令自动化登录 BIT-Web 校园网]]></title>
            <link>https://blog.spencerwoo.com/2019/12/bitweb-auto-login/</link>
            <guid>https://blog.spencerwoo.com/2019/12/bitweb-auto-login/</guid>
            <pubDate>Sun, 29 Dec 2019 00:00:00 GMT</pubDate>
            <content:encoded><![CDATA[<p>BIT-Web 是北京理工大学校园 Wi-Fi，专门用于笔记本等桌面设备，另外还有 BIT-Mobile 用于移动设备。但是，BIT-Mobile 有时候并没有 BIT-Web 稳定，自动登录不是那么靠谱，我们也不能通过 BIT-Mobile 登录使用免费运营商宽带，这些场景下我们都需要在移动设备上连接至 BIT-Web 进行手动登录。</p>
<p><img src="https://cdn.spencer.felinae98.cn/blog/2020/07/20200722-215042.png" alt="BIT-Web 和 BIT-Mobile 的对比"></p>
<p>最近我的同学跟我说，BIT-Web 的登录页面在移动端（尤其是 Android 平台）上不能正常的显示「密码管理器」，也就不能直接填充密码，每次都需要手动输入。这令人非常烦恼，如何才能实现在连接到 BIT-Web 上之后自动发送登录认证请求来连接至校园网呢？</p>
<h2 id="实现思路">实现思路</h2>
<p>对于我的学校来说，登录至校园网的基本操作就是：</p>
<ul>
<li>连接到 BIT-Web</li>
<li>在浏览器中打开网址 <code>t.cn</code> 来重定向至登录页面 <code>10.0.0.55</code></li>
<li>输入账号密码并点击登录</li>
</ul>
<p>在这背后，我们事实上是给学校校园网登录认证服务器发送了一个带有我们「账号」和「密码」的登录请求（实际来说可能是账号密码组合出的加密认证令牌），之后校园网认证服务器核实我们的身份，并反馈我们认证结果，给予上网权限。</p>
<p><img src="https://cdn.spencer.felinae98.cn/blog/2020/07/20200722-215042-1.png" alt="连接至 BIT-Web 并进行认证过程发送的网络请求"></p>
<p>这样来说，我们事实上就只需要在每次连接至 BIT-Web 网络时，自动发送这一请求，即可实现自动登录校园网的功能。在 iOS 平台，我们有相当方便的工具来制作发送请求的脚本：快捷指令 Shortcuts，利用 Shortcuts 我们可以定制一个「动作」，实现自动登录的功能。</p>
<p>另外，iOS 13 里面的 Shortcuts 加入了全新的 <strong>Automation</strong>：基于场景的动作自动化执行功能。利用这一特性，我们就可以定义「连接到 Wi-Fi 名称为 BIT-Web 的网络」这一「触发器」，从而实现自动触发动作的能力。</p>
<p>好啦，万事俱备，我们开始实现吧～</p>
<h2 id="操作步骤">操作步骤</h2>
<p>接下来，我会以 iOS 的 Shortcuts 中「自动登录校园网」的实现为例，详细介绍我们具体如何实现这样的自动化操作功能。</p>
<p>首先需要说明的是，iOS 的 Shortcuts 里面能够执行的算法有限，但是就今天（2019.12.25）来说，我校校园网服务器的登录认证接口已经升级，需要进行加密运算生成登录令牌才能正确认证。考虑到我们的脚本仅仅在校园网内部可控环境下执行，这里我们退而求其次，使用旧接口：<strong>直接发送明文账号密码进行认证的 API 来登录校园网。</strong></p>
<h3 id="明确网络请求参数">明确网络请求参数</h3>
<p>BIT-Web 的旧登录请求接口是如下配置的：</p>
<pre><code class="language-http">POST /include/auth_action.php HTTP/1.1
Content-Type: application/x-www-form-urlencoded
Host: 10.0.0.55:801
Content-Length: 75

action=login&amp;username={YOUR_USERNAME}&amp;password={YOUR_PASSWORD}&amp;ac_id=8&amp;save_me=1&amp;ajax=1</code></pre>
<p>其中 <code>{YOUR_USERNAME}</code> 以及 <code>{YOUR_PASSWORD}</code> 均为明文账密，我校曾经就是这样简单粗暴。简单在终端中用 cURL 工具进行测试，在连接 BIT-Web 且尚未登录的情况下，在终端中输入如下的命令：</p>
<pre><code class="language-bash">curl --request POST \
  --url http://10.0.0.55:801/include/auth_action.php \
  --header &#39;content-type: application/x-www-form-urlencoded&#39; \
  --data action=login \
  --data username={YOUR_USERNAME} \
  --data password={YOUR_PASSWORD} \
  --data ac_id=8 \
  --data save_me=1 \
  --data ajax=1</code></pre>
<p>将你自己的账号密码带入其中，如果得到类似下面的包含有 <code>login_ok</code> 的结果，同时你可以连接互联网，那么说明你的认证成功。</p>
<blockquote>
<p><strong>💡 注意：</strong></p>
<p>这里如果登录失败，校园网认证服务器会直接返回登录失败的原因。比如：已欠费、密码错误等。按照错误信息进行相应的调试即可。</p>
</blockquote>
<p><img src="https://cdn.spencer.felinae98.cn/blog/2020/07/20200722-215042-2.png" alt="cURL 测试登录 BIT-Web 效果"></p>
<p>上面的 HTTP 请求我来简单分解介绍一下。首先，请求是 <code>POST</code> 的方法，格式为 <code>application/x-www-form-urlencoded</code>，请求地址（即 url 地址）为 <code>http://10.0.0.55:801/include/auth_action.php</code>，参数分别为：</p>
<table>
<thead>
<tr>
<th align="left">参数</th>
<th align="left">值</th>
<th align="left">功能</th>
</tr>
</thead>
<tbody><tr>
<td align="left"><code>action</code></td>
<td align="left"><code>login</code></td>
<td align="left">设置动作为登录</td>
</tr>
<tr>
<td align="left"><code>username</code></td>
<td align="left"><code>{YOUR_USERNAME}</code></td>
<td align="left">发送账户（即学号）</td>
</tr>
<tr>
<td align="left"><code>password</code></td>
<td align="left"><code>{YOUR_PASSWORD}</code></td>
<td align="left">发送校园网密码</td>
</tr>
<tr>
<td align="left"><code>ac_id</code></td>
<td align="left">8</td>
<td align="left">代表登录 BIT-Web（BIT-Mobile 登录请求的 <code>ac_id</code> 不一样）</td>
</tr>
<tr>
<td align="left"><code>save_me</code></td>
<td align="left">1</td>
<td align="left">保存当前登录 session</td>
</tr>
<tr>
<td align="left"><code>ajax</code></td>
<td align="left">1</td>
<td align="left">（猜测）表示异步发送请求</td>
</tr>
</tbody></table>
<p>搞清楚我们具体的发送信息之后，接下来我们开始着手在 iOS 上面制作 Shortcuts 动作。</p>
<h3 id="制作-shortcuts-动作">制作 Shortcuts 动作</h3>
<p>首先，在 Shortcuts 里面创建新动作，并搜索加入模块「Get contents of URL」。点击模块下部的 Show More，在其中按下图进行配置：</p>
<!-- ![BIT-Web 登录认证请求模块](https://cdn.spencer.felinae98.cn/blog/2020/07/20200722-215042-3.png) -->

<p><img src="https://cdn.spencer.felinae98.cn/blog/2020/07/20200722-215042-3.png" alt="BIT-Web 登录认证请求模块" width="500px"></p>

<ul>
<li>URL 设置为：<code>http://10.0.0.55:801/include/auth_action.php</code></li>
<li>Method 设置为：<code>POST</code></li>
<li>之后会出现 Request Body 的设置字段：<ul>
<li>选择 Request Body 为 <strong>Form</strong></li>
<li>点击 Add new field，选择 Text 类型。将 Key 字段设置为 <code>action</code>，Text 设置为字段设置为 <code>login</code></li>
<li>继续点击 Add new field 并选择 Text 类型。将 Key 字段设置为 <code>username</code>，Text 设置为字段设置为<strong>你的校园网用户名（学号）</strong></li>
<li>继续点击 Add new field 并选择 Text 类型。将 Key 字段设置为 <code>password</code>，Text 设置为字段设置为<strong>你的校园网账户密码</strong></li>
<li>继续点击 Add new field 并选择 Text 类型。将 Key 字段设置为 <code>ac_id</code>，Text 设置为字段设置为 8</li>
<li>继续点击 Add new field 并选择 Text 类型。将 Key 字段设置为 <code>save_me</code>，Text 设置为字段设置为 1</li>
<li>继续点击 Add new field 并选择 Text 类型。将 Key 字段设置为 <code>ajax</code>，Text 设置为字段设置为 1</li>
</ul>
</li>
</ul>
<p>在模块「Get contents of URL」下方添加模块「Text」，<strong>将「Text」的值设置为「Contents of URL」</strong>，也就是上一步网络请求的返回结果。</p>
<!-- ![将返回数据用 Text 模块规格化](https://cdn.spencer.felinae98.cn/blog/2020/07/20200722-215042-4.jpg) -->

<p><img src="https://cdn.spencer.felinae98.cn/blog/2020/07/20200722-215042-4.jpg" alt="将返回数据用 Text 模块规格化" width="500px"></p>

<p>继续，在下面添加模块「If」，用来判断我们登录成功与否。将 If 模块的判断条件设置为「contains」，包含字符设置为 <code>login_ok</code>：</p>
<ul>
<li>如果匹配成功：说明登录 BIT-Web 成功，发送登录成功通知</li>
<li>如果匹配失败（进入 Otherwise 部分）：说明登录 BIT-Web 失败，发送登录失败通知以及失败的请求返回的数据</li>
</ul>
<!-- ![BIT-Web 登录返回数据处理模块](https://cdn.spencer.felinae98.cn/blog/2020/07/20200722-215042-5.jpg) -->

<p><img src="https://cdn.spencer.felinae98.cn/blog/2020/07/20200722-215042-5.jpg" alt="BIT-Web 登录返回数据处理模块" width="500px"></p>

<p>之后，我们测试。将手机连接至校园网 BIT-Web，尝试执行这一 Shortcuts 动作。如果一切顺利，那么你应该可以登录成功，得到如下通知：</p>
<!-- ![BIT-Web 登录成功](https://cdn.spencer.felinae98.cn/blog/2020/07/20200722-215042-6.jpg) -->

<p><img src="https://cdn.spencer.felinae98.cn/blog/2020/07/20200722-215042-6.jpg" alt="BIT-Web 登录成功" width="500px"></p>

<h3 id="定义动作触发条件">定义动作触发条件</h3>
<p>接下来，我们在 Shortcuts 中设置连接至 BIT-Web 之后自动触发这一动作的功能。在今年秋天 iOS 13 的更新中，Shortcuts 同样更新了其 Automation 的功能。这里我们所要做的就是设置一个「触发器」使得 iPhone 能够自动连接到 WiFi SSID 为 BIT-Web 的网络之后提醒我们执行上面创建的 Shortcuts 动作。</p>
<p>我们点击 Shortcuts 中间菜单「Automation」，点击上方加号，选择 Create Personal Automation。之后，在菜单中选择 WLAN，在下方菜单中点击 Choose 并选择 BIT-Web。</p>
<p><img src="https://cdn.spencer.felinae98.cn/blog/2020/07/20200722-215042-7.png" alt="配置连接至 BIT-Web 的自动触发器"></p>
<p>之后，点击右上角 Next，在添加动作模块的页面点击加号，添加一个「Run Shortcut」的模块。</p>
<p><img src="https://cdn.spencer.felinae98.cn/blog/2020/07/20200722-215042-8.jpg" alt="添加 Run Shortcut 的模块"></p>
<p>接下来，将「Run Shortcut」模块的执行动作设置为我们刚刚制作的 BIT-Web Shortcut 动作。其他内容无需改动。</p>
<p><img src="https://cdn.spencer.felinae98.cn/blog/2020/07/20200722-215042-9.png" alt="配置自动执行上一步制作的 BIT-Web Shortcut 动作"></p>
<p>之后，点击 Next &gt; Done 保存动作。</p>
<h2 id="效果">效果</h2>
<p>如果一切顺利，那么你的 iPhone 连接至 BIT-Web 之后，Shortcuts 应用就会自动弹出提醒请求执行 BIT-Web 登录的动作。在通知提醒上面点击执行，我们就可以直接登录至 BIT-Web。</p>
<blockquote>
<p><strong>🤔 注意：</strong></p>
<p>这一快捷指令以及利用快捷指令登录 BIT-Web 是上个月的思路和想法，在上个月这一功能一直完好可用，但是最近我注意到 Wifi 触发器总是无法正确的触发动作的运行，同时在 Reddit 社区里面也有不下 5 条抱怨 WiFi 触发器不工作的内容。因此我怀疑确实是 iOS 出现的 bug 导致的。</p>
</blockquote>
<p>另外，如果上面的触发总是无法成功，那么你也可以直接将 BIT-Web Shortcut 动作固定在主屏幕，每次连接到 BIT-Web 之后手动点击执行快捷指令即可。这肯定比跳转登录认证页面输入账号密码登录方便许多。</p>
<p><img src="https://cdn.spencer.felinae98.cn/blog/2020/07/20200722-215042-10.jpg" alt="直接将 BIT-Web 登录快捷指令固定到主屏幕"></p>
<p>自动化的操作比人力重复无效劳动要方便许多，打卡、签到，日复一日的登录、提醒，都可以利用「自动化」的思路进行完成。本文就介绍到这里，感谢阅读。</p>
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[IFTTT x Integromat：微博 to Twitter 自动转发的最佳实践]]></title>
            <link>https://blog.spencerwoo.com/2019/11/weibo-to-twitter/</link>
            <guid>https://blog.spencerwoo.com/2019/11/weibo-to-twitter/</guid>
            <pubDate>Tue, 26 Nov 2019 00:00:00 GMT</pubDate>
            <content:encoded><![CDATA[<p>IFTTT 网络自动化平台实际上是一个缩写，完全展开的 IFTTT 是「IF this THEN that」- 如果「这」，那么「那」。既然如此，我们利用 IFTTT 就可以实现非常完善的自动化事件处理流程，比如这篇文章我要介绍的就是一个例子：「利用 IFTTT 自动将微博发布的信息同步到 Twitter 上面」的最佳实践。</p>
<p>:::note 附注 🎫
本文介绍方法高度借鉴于这篇文章：<a href="https://sspai.com/post/51942">微博同步至 Twitter，这里有更好的方式 - 少数派</a>，但是本文的介绍方式更为清晰易懂，配置简洁明了，同时也解决了一些如果按照原文直接配置的话会出现的意外问题。
:::</p>
<h2 id="背景与工作原理">背景与工作原理</h2>
<p>首先，IFTTT 上面已经有非常多的微博、Twitter 互相自动转发分享的 Applet（就是 IFTTT 的动作），官方甚至都有一个专门的分类，包含了一些基本的微博 → Twitter、Twitter → 微博的动作：<a href="https://ifttt.com/connect/sina_weibo/twitter">Connect Sina Weibo to Twitter to unlock powerful experiences</a>。</p>
<p><img src="https://cdn.spencer.felinae98.cn/blog/2020/07/20200722-220834.png" alt="IFTTT 官方微博 Twitter 同步 Applet"></p>
<p>但是单独由 IFTTT 进行「微博 → Twitter」的自动转发有着非常的局限性。IFTTT 无法区分：纯文本原创微博、带图原创微博、以及转发微博这三种微博。如果我们只依赖于 IFTTT 来帮我们进行「微博 → Twitter」转发的话：要么我们只能转发文字 + 原微博链接；要么我们转的带图，但是对于纯文本微博 IFTTT 会发送一张「找不到原图」的 Twitter。因此，今天我们所要介绍的方法，就是利用 Integromat 对 IFTTT 获取到的微博进行路由分流，实现对纯文本微博自动发 IFTTT 纯文本微博转发 Applet、对带图微博自动发 IFTTT 带图微博转发 Applet，并自动过滤掉非原创微博的内容。</p>
<p><img src="https://cdn.spencer.felinae98.cn/blog/2020/07/20200722-220834-1.png" alt="仅靠 IFTTT 只能单独执行某个转发策略，这不够"></p>
<h2 id="利用-ifttt-和-integromat-配合实现路由转发">利用 IFTTT 和 Integromat 配合实现路由转发</h2>
<h3 id="流程原理">流程原理</h3>
<p>目前和微博平台整合最好、最方便的自动化平台就是 IFTTT，因此我们不能丢掉 IFTTT。利用 IFTTT 获取到微博的信息包括：微博文本、微博原链接以及微博图片链接。因此，我们需要做的就是：</p>
<ol>
<li>先利用 IFTTT 获取最新发送的微博，包括微博文本、微博原链接和微博图片这三项参数</li>
<li>通过 HTTP 请求在 Integromat 中触发 Webhook，利用 Integromat 解析 IFTTT 发送来的数据，根据「图片的有无」进行路由分配，调用合适的 IFTTT 动作</li>
<li>Integromat 调用合适的 IFTTT 动作之后，IFTTT 执行发送 Twitter 的 Applet</li>
<li>结束 👍</li>
</ol>
<p><img src="https://cdn.spencer.felinae98.cn/blog/2020/07/20200722-220834-2.png" alt="利用 IFTTT 和 Integromat 配合实现微博 Twitter 转发流程"></p>
<p>我们接下来的基本工作就是按步骤对上面介绍的功能在相应的平台上一一进行实现。</p>
<h3 id="准备工作">准备工作</h3>
<p>在一切开始之前，我们需要在 IFTTT 和 Integromat 两个平台上做一些准备工作。</p>
<h4 id="在-integromat-平台创建-scenario">在 Integromat 平台创建 Scenario</h4>
<p>在 <a href="https://www.integromat.com">Integromat</a> 平台注册登录，在 Scenarios 处点击创建：Create a new scenario，搜索 Webhook 并选择，之后点击 Continue 即可。</p>
<p><img src="https://cdn.spencer.felinae98.cn/blog/2020/07/20200722-220834-3.png" alt="Integromat 创建新 Scenario 并选择 Webhook"></p>
<p>进入创建 Scenario 的界面，我们首先点击中心的问号，选择 Webhooks &gt; Custom Webhooks 作为我们 Scenario 的起点。之后我们配置 Webhook：</p>
<ul>
<li><p>点击 Webhook 设置窗口中的 Add，在弹出的 Add a hook 界面将 Webhook 名称设置为 <strong>IFTTT weibo webhook</strong></p>
</li>
<li><p>点击左下角的 Show advanced settings，在 Data structure 处添加抓取自微博的数据结构：点击 Add，在弹出的窗口中将数据结构名称命名为 <strong>Weibo data structure</strong>。这就是我们从 IFTTT 获取到的微博博文的三个关键数据的存储方式（微博文本、微博链接和微博图片）</p>
<p><img src="https://cdn.spencer.felinae98.cn/blog/2020/07/20200722-220834-4.png" alt="配置 Webhook、添加 Data structure"></p>
</li>
<li><p>下面我们开始设置微博数据结构：</p>
<ul>
<li>点击右侧 Generator，在弹出的窗口中将 Content type 选择为 <strong>Query String</strong></li>
<li>在下方 Sample Data 中填入：<code>text=weiboText&amp;url=weiboUrl&amp;image=imageUrl</code>，点击保存之后，我们就得到了一个合适的 Data Structure</li>
<li>可以看到，上面 Sample Data 实际上就是我们利用 IFTTT 获取到微博数据的一个传递，其中 <code>text</code> 字段保存「微博文本内容：weiboText」、<code>url</code> 字段保存「微博链接：weiboUrl」、<code>image</code> 字段保存「微博图片：imageUrl」</li>
</ul>
<p><img src="https://cdn.spencer.felinae98.cn/blog/2020/07/20200722-220834-5.png" alt="配置微博 Data structure"></p>
</li>
<li><p>将上面步骤配置的内容全部保存，我们得到这样的一个界面：</p>
<p><img src="https://cdn.spencer.felinae98.cn/blog/2020/07/20200722-220834-6.png" alt="Integromat Webhook API 地址"></p>
</li>
<li><p>上图里面我框出来的地方就是 Integromat 的 Webhook API 地址。接下来，将上图中的 API 地址复制，在后面添加上我们刚刚声明的数据结构的请求，并在浏览器中访问一下，让 Integromat 对收到的请求进行判断，自动确定数据结构的数据类型</p>
</li>
<li><p>需要注意的是，我们需要让这一步中的 URL 跟我们 IFTTT 发送给 Integromat 的请求一致，也就是在刚刚复制的 Integromat 请求地址后面，需要正确的拼接上我们的请求数据，这里给出一个示范：</p>
<ul>
<li><p>如果我们的 Integromat Webhook API 地址为：<code>https://hook.integromat.com/{integromat_api_key}</code></p>
</li>
<li><p>我们请求的微博文本内容为：<code>TestingMyWebhook</code></p>
</li>
<li><p>我们请求的微博原文链接为：<code>https://m.weibo.cn/detail/4444027372221130</code></p>
</li>
<li><p>我们请求的微博图片链接为：<code>https://wx4.sinaimg.cn/orj360/63e5c1e1ly1g9f2lmag8hj20k00qoac8.jpg</code></p>
</li>
<li><p>那么我们最终拼接出来的 URL 请求为（没有换行，换行方便阅读）：</p>
<pre><code>https://hook.integromat.com/{integromat_api_key}
?text=TestingMyWebhook
&amp;url=https://m.weibo.cn/detail/4444027372221130
&amp;image=https://wx4.sinaimg.cn/orj360/63e5c1e1ly1g9f2lmag8hj20k00qoac8.jpg</code></pre></li>
</ul>
</li>
<li><p>将上面的 URL 复制到浏览器中访问，加载完成之后，如果我们得到了正确的数据结构，那么 Integromat 那边会自动显示 Successfully Determined，同时浏览器中会显示 Accepted 字样</p>
<p><img src="https://cdn.spencer.felinae98.cn/blog/2020/07/20200722-220834-7.png" alt="成功确定数据结构"></p>
</li>
</ul>
<p>到这里，我们先将 Integromat 的全部配置保存妥当，将上面 Integromat 的 URL 请求保存，不要关闭标签页，我们继续准备 IFTTT 的配置项目。</p>
<p>:::caution 注意 🐖
如果这里 Integromat 平台未提示 Successfully Determined，或浏览器中未显示 Accepted，那么说明 URL 请求拼接有问题，请再次尝试保证数据结构确定无误，否则会对接下来的配置造成很大影响。
:::</p>
<h4 id="在-ifttt-平台找到-webhook-接口">在 IFTTT 平台找到 Webhook 接口</h4>
<p>在 IFTTT 平台，我们进入 <a href="https://ifttt.com/maker_webhooks">Webhook 的动作界面</a>，点击右侧的 Documentation，不出意外的话，你会看到专属于你自己的 Webhook Key，我们在 Integromat 中最后就会向这个地址发出 HTTP 请求，传递相应的微博博文数据，触发正确的 IFTTT 动作。因此，请记下这一请求的具体方式，包括请求 URL 以及请求 body 格式：</p>
<pre><code class="language-json">{ &quot;value1&quot;: &quot;...&quot;, &quot;value2&quot;: &quot;...&quot;, &quot;value3&quot;: &quot;...&quot; }</code></pre>
<p><img src="https://cdn.spencer.felinae98.cn/blog/2020/07/20200722-220834-8.png" alt="Webhook Documentation"></p>
<p>接下来，我们在刚刚对两个平台的配置的基础之上，对两个平台进行连接，实现自动化的微博 → Twitter 转发过程。</p>
<h3 id="ifttt-触发-webhook，调用-integromat">IFTTT 触发 Webhook，调用 Integromat</h3>
<p>首先，我们在 IFTTT 平台创建一个新的 Applet，作为检测到新微博的起始动作。IFTTT 在检测到我们发送一条新微博之后，会向 Integromat 发送一个 HTTP 请求，告知 Integromat 我们的新微博的文本消息、图片内容和原文链接。</p>
<p><img src="https://cdn.spencer.felinae98.cn/blog/2020/07/20200722-220834-9.png" alt="This 选择 Sina Weibo 触发"></p>
<p>在 IFTTT 上面，点击右上角 <a href="https://ifttt.com/create">头像 &gt; Create</a>，进入创建 Applet 的界面。在 This 处选择 Sina Weibo，选择 New post by you 的触发器。如果要求登录微博那么选择链接即可。</p>
<p><img src="https://cdn.spencer.felinae98.cn/blog/2020/07/20200722-220834-10.png" alt="That 选择 Webhook 服务"></p>
<p>在 That 处选择 Webhooks，选择 Make a web request，之后进入配置 Webhook 的界面。<strong>我们在 URL 处填写刚刚 Integromat 的 Webhook API 地址（不包含测试用的拼接部分，即只填入 Integromat 显示的 URL），在 Method 处选择 POST，在 Content Type 处选择 <code>application/x-www-form-urlencode</code>，最后在 Body 处填写：</strong></p>
<pre><code>text={{Text}}&amp;url={{WeiboURL}}&amp;image={{PhotoURL}}</code></pre><p><img src="https://cdn.spencer.felinae98.cn/blog/2020/07/20200722-220834-11.png" alt="填写 Webhook 请求地址，触发 Integromat 动作"></p>
<p>点击 Create action 保存动作，这样，我们 IFTTT 的起手触发动作就制作完成了。</p>
<h3 id="ifttt-自动发送-twitter-动作">IFTTT 自动发送 Twitter 动作</h3>
<p>接下来，我们在 IFTTT 平台继续创建两个不同的 Applet，分别用来处理带图微博和纯文本微博的转发。我们和之前一样，选择 Create，配置 This 和 That。</p>
<p>对带图微博：</p>
<ul>
<li>This 处选择 Webhooks，选择 Receive a web request，并给 Event name 起名为 <code>image_weibo</code></li>
<li>That 处选择 Twitter，选择 Post a tweet，并将 Tweet text 设置为：<code>{{Value1}} via Weibo: {{Value2}}</code></li>
</ul>
<p>对纯文本微博：</p>
<ul>
<li>This 处选择 Webhooks，选择 Receive a web request，并给 Event name 起名为 <code>text_weibo</code></li>
<li>That 处选择 Twitter，选择 Post a tweet with image，并将 Tweet text 设置为：<code>{{Value1}} via Weibo: {{Value2}}</code>，将 Image URL 设置为：<code>{{Value3}}</code></li>
</ul>
<p>:::important 注意 🍧
这里，我们将从 Integromat 中收到即将发送的推特的三个参数，分别为原微博的文本内容、原微博链接以及（如果有）原微博的图片。在 IFTTT 中，我们分别设置这三个参数：</p>
<ul>
<li><code>{{Value1}}</code>：原微博文本</li>
<li><code>{{Value2}}</code>：原微博链接</li>
<li><code>{{Value3}}</code>：原微博图片（如果有）</li>
</ul>
<p>这里的三个 Key 就是 <code>{{ ... }}</code> 所包含的内容，这里的配置会和接下来 Integromat 的请求一致。
:::</p>
<p>这样，我们 IFTTT 平台的配置就基本完成了。接下来我们对 Integromat 的动作进行完善。</p>
<h3 id="integromat-路由判断树">Integromat 路由判断树</h3>
<p><img src="https://cdn.spencer.felinae98.cn/blog/2020/07/20200722-220834-12.png" alt="Integromat 路由判断树"></p>
<p>在我们刚刚创建的 Scenario 里面，Webhook 后面，添加一个路由选择：Router。（点击右侧突出的半圆即可添加新的元素，搜索 Router，点击 Flow control 并选择 Router 即可添加。）</p>
<p>在 Router 中第一条分支上面点击，选择设置 Filter，在 Label 处为分支起名为：ImageWeibo，用来专门处理带图片的微博。<strong>在 Condition 里配置 Image contains <code>http</code> 的判断条件</strong>，用来正确的调用合适的 IFTTT 动作。</p>
<p><img src="https://cdn.spencer.felinae98.cn/blog/2020/07/20200722-220834-13.png" alt="第一条分支的 Filter 过滤器"></p>
<p>:::caution 注意 🍳
在最初少数派的文章介绍之中，原作者用「Image exists」作为判断条件。经过我的调试发现：即使是并没有包含图片的纯文本微博，在 IFTTT 抓取微博信息并传到 Integromat 之后，Integromat 判断树并不能正确的判断这条微博没有图片，而是会自动触发默认路径，导致我们依旧发出一条带有「IFTTT 找不到图片」的推特。因此，这里我使用的判断条件为：「Image contains <code>http</code>」。
:::</p>
<p>在分支末端点击问号，搜索 HTTP，选择 Make a request。之后：</p>
<ul>
<li>在 URL 中填入 IFTTT 带图微博转发的 Applet Webhook 请求地址：<code>https://maker.ifttt.com/trigger/image_weibo/with/key/{IFTTT API Key}</code>（将 <code>{IFTTT API Key}</code> 更换为上面在 IFTTT Webhook Documentation 中配置的 Key。）</li>
<li>在下面的 Method 中选择 POST</li>
<li>Body type 中选择 <code>application/x-www-form-urlencode</code></li>
<li>在 Fields 中点击 Add item：<ul>
<li>Key 填入 <code>value1</code>，Value 选择 <code>text</code></li>
<li>Key 填入 <code>value2</code>，Value 选择 <code>url</code></li>
<li>Key 填入 <code>value3</code>，Value 选择 <code>image</code></li>
</ul>
</li>
</ul>
<p><img src="https://cdn.spencer.felinae98.cn/blog/2020/07/20200722-220834-14.png" alt="触发 IFTTT 带图微博转发 Applet"></p>
<p>在第二条分支上面点击，选择设置 Filter 并填入 Label 为 PureTextWeibo，在 Condition 处设置筛选条件为：</p>
<ul>
<li>Image <em>Does not contain</em> <code>http</code> <strong>AND</strong></li>
<li>Text <em>Does not match pattern (case insensitive)</em> <code>(Repost)|(转发微博)|(\/\/)|(轉發微博)</code></li>
</ul>
<p>两个判断条件进行「与」运算，保证满足两条内容才能触发本条规则。</p>
<p><img src="https://cdn.spencer.felinae98.cn/blog/2020/07/20200722-220834-15.png" alt="第二条分支的 Filter 过滤器"></p>
<p>在分支末端点击问号，搜索 HTTP，选择 Make a request。之后：</p>
<ul>
<li>在 URL 中填入 IFTTT 纯文本微博转发的 Applet Webhook 请求地址：<code>https://maker.ifttt.com/trigger/text_weibo/with/key/{IFTTT API Key}</code>（将 <code>{IFTTT API Key}</code> 更换为上面在 IFTTT Webhook Documentation 中配置的 Key。）</li>
<li>在下面的 Method 中选择 POST</li>
<li>Body type 中选择 <code>application/x-www-form-urlencode</code></li>
<li>在 Fields 中点击 Add item：<ul>
<li>Key 填入 <code>value1</code>，Value 选择 <code>text</code></li>
<li>Key 填入 <code>value2</code>，Value 选择 <code>url</code></li>
</ul>
</li>
</ul>
<p><img src="https://cdn.spencer.felinae98.cn/blog/2020/07/20200722-220834-16.png" alt="触发 IFTTT 纯文本微博转发 Applet"></p>
<p>到这里，保存 Scenario，我们基本配置任务就完成了。( •̀ ω •́ )y</p>
<h2 id="最终的转发效果以及局限性">最终的转发效果以及局限性</h2>
<p>如果一些顺利，那么我们就应该可以直接让 IFTTT 和 Integromat 配合在云端默默监控我们的最新微博，并自动的根据合适的方式帮我们转发至 Twitter。</p>
<p><img src="https://cdn.spencer.felinae98.cn/blog/2020/07/20200722-220834-17.png" alt="最终的微博 to 推特转发效果"></p>
<p>但是，这种转发方式还是有一定的限制，其中最为致命的实际上就是：IFTTT 只能捕获微博的第一张图片，多于一张的图片我们就只能将第一张图片转发到 Twitter。同时 Integromat 的同步次数也有一定的限制，每个月相当于最多能转发 500 条原创微博。不过这些对我来说还是足够的，如果有更多的需求，我推荐大家购买完整版的「奇点」微博客户端，能够自动每次发微博的同时发送推特，更为方便。这篇文章的介绍就到这里，感谢阅读。</p>
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Reverse Proxy | 反向代理是什么？]]></title>
            <link>https://blog.spencerwoo.com/2019/11/reverse-proxy/</link>
            <guid>https://blog.spencerwoo.com/2019/11/reverse-proxy/</guid>
            <pubDate>Fri, 22 Nov 2019 00:00:00 GMT</pubDate>
            <content:encoded><![CDATA[<p><a href="https://blog.spencerwoo.com/2019/11/tiny-tiny-rss/">Tiny Tiny RSS | 最速部署私有 RSS 服务器</a> 这篇文章里面，我们提到了「利用 Nginx 作为反向代理」来为 Tiny Tiny RSS 服务加上 SSL 支持。事实上，我经常都能在各个和 Nginx、Apache Server 等相关文档里面看到「反向代理」这个术语。今天我们就来看看，到底什么是反向代理？「反向代理」的工作原理，以及我们都可以用「反向代理」来干什么？</p>
<h2 id="proxy：什么是代理？">Proxy：什么是代理？</h2>
<p>首先，反向代理（Reverse Proxy）是一种代理（Proxy）服务。为了搞清楚「反向代理」，我们首先来说一说「代理 - Proxy」。</p>
<p>相信阅读这篇文章的同学一定对 Proxy 这个名词不陌生，<del>Mainland China 互联网现状让大部分同学的计算机网络知识突飞猛进</del>😂。简单来说，Proxy 服务器的主要功能就是在客户端 Client 和服务端 Server 之间搭建一个桥梁，从客户端访问服务端的网络流量、以及从服务端返回客户端的网络流量都会经由这一 Proxy 服务器的转发。[^1]</p>
<p><img src="https://cdn.spencer.felinae98.cn/blog/2020/07/20200722-214045.png" alt="功能示意"></p>
<p>为了方便表述，我们就叫我们的 Proxy 服务器：咕咕，一只鸽子。🐦</p>
<h2 id="正向代理和反向代理">正向代理和反向代理</h2>
<h3 id="forward-proxy：正向代理">Forward Proxy：正向代理</h3>
<p>飞鸽传书嘛，信鸽主要功能就是通风报信，我们这里的咕咕也不例外。咕咕在正常情况下是我们自己（客户端）的，也就意味着：咕咕会在 Client 前面等待送信。当 Client 准备发送一个请求的时候，咕咕会拿着这个请求，在公共互联网上面，将请求准确送达至对应的 Server。同理，Server 返回 response 给 Client，response 也会先被咕咕拿到，之后再转交给 Client。</p>
<p><img src="https://cdn.spencer.felinae98.cn/blog/2020/07/20200722-214045-1.png" alt="Forward Proxy 的工作原理"></p>
<p>上面就是「正向代理」的基本工作流程，咕咕就是我们这个例子里面的正向代理服务器，负责转发和接受从 Client 发出或收到的网络请求。我们用正向代理（Forward Proxy），<del>除了大家人尽皆知的目的以外</del>😂，还可以：</p>
<ul>
<li>有效屏蔽广告、追踪脚本等有害请求。咕咕可以选择性的将 Server 发来的内容进行屏蔽，也就是：咕咕知道这次发来的是个广告，不给我们看也无妨大碍，那为了让我们浏览体验更加纯净，咕咕就非常体贴的丢掉了这一广告。爱了 ❤️ [^2]</li>
<li>有效的隐藏我们的身份。因为咕咕在你没有用到它的时候，可以并行的为其他同学传递请求。这时候，Server 是无从知道请求的真正来源的，只知道是由咕咕转手的，从而保证我们身份匿名性。</li>
</ul>
<h3 id="reverse-proxy：反向代理">Reverse Proxy：反向代理</h3>
<p>那么反向代理是怎样工作的呢？在反向代理中，我们的咕咕现在由服务器 Server 养活（部署），也就是说，咕咕在服务器端通风报信。每个从 Client 经由互联网发过来的请求会先到达咕咕这里，咕咕再将每个请求分发给相应的服务器。反之亦然。这就是「反向代理」的基本工作原理，我们在这个例子里面的「咕咕」，就是我们的反向代理服务器。（Reverse Proxy Server）</p>
<p><img src="https://cdn.spencer.felinae98.cn/blog/2020/07/20200722-214045-2.png" alt="Reverse Proxy 的工作原理"></p>
<p>为什么我们服务端也需要一个这样的咕咕呢？因为我们的咕咕不仅勤劳，还很坚强。要知道，不是所有的目标服务器都像我们咕咕那么坚强，目标服务器很多时候会因为直接收到的信件过多（收到过多的请求）而被淹没，甚至宕机。如果有咕咕的帮助，目标服务器就不会因为请求过多而无法处理，同时如果咕咕发现一个服务的请求太多，我们可以将这一服务增加多个服务器共同处理，咕咕这时候就可以将服务的请求进行分流，从而减轻单个服务器的处理负担。<strong>这也就是「反向代理」在「负载均衡」方面的应用。</strong></p>
<p>事实上，反向代理服务器有更多的应用，我们接下来就具体说一说我在服务器上面都利用 Nginx 反向代理服务器做了哪些有用的事情。</p>
<h2 id="nginx-反向代理的实际应用">Nginx 反向代理的实际应用</h2>
<p>在我的（<a href="https://blog.tenkeyseven.com">@TenkeySeven</a> 的）服务器上面，有这样的几个服务：</p>
<ul>
<li><a href="https://tt-rss.org/">Tiny Tiny RSS 服务</a></li>
<li><a href="https://github.com/netdata/netdata">Netdata 服务器监控服务</a></li>
<li>Nginx 直接 serve 的一个默认静态页面</li>
</ul>
<p>事实上，这三个服务都运行在这样的一个服务器上面，但是绑定了不同的域名。比如：</p>
<ul>
<li>Tiny Tiny RSS 服务对应域名：<a href="https://ttrss.tenkeyseven.com">https://ttrss.tenkeyseven.com</a>（需要登录）</li>
<li>Netdata 服务对应域名：<a href="https://stats.tenkeyseven.com">https://stats.tenkeyseven.com</a>（可以公开访问）</li>
<li>Nginx 测试静态页面对应域名：<a href="https://tenkeyseven.com">https://tenkeyseven.com</a>（可以公开访问）</li>
</ul>
<p>我们连接到服务器上面，执行下面的命令来查看端口占用情况：</p>
<pre><code class="language-bash">sudo netstat -tulpn | grep LISTEN</code></pre>
<p><img src="https://cdn.spencer.felinae98.cn/blog/2020/07/20200722-214045-3.png" alt="端口占用情况"></p>
<p>通过查看端口占用情况，我们可以非常清晰的看到：</p>
<ul>
<li>Tiny Tiny RSS 运行于 Docker 容器中，对外的 exposed 端口是 181</li>
<li>Netdata 前端服务直接监听的端口是 19999</li>
<li>Nginx 默认 serve 的静态页面直接监听 80 端口（HTTP）</li>
</ul>
<p><a href="https://blog.tenkeyseven.com">@TenkeySeven</a> 将主域名和上面提到的两个子域名的 DNS 解析均设置为我们的服务器，因此，当我们访问上面任意一个域名的时候，请求均直接发给服务器上面运行的 Nginx 反向代理服务。</p>
<p>对 Nginx 来说，只需要识别这些域名对应的请求应该转发给具体哪个服务，就可以让请求被正确处理，这样也就实现了我们多个域名对应一个服务器上的多个服务的需求。</p>
<p>Nginx 全部功能均由配置文件 <code>nginx.conf</code> 来设置，这一配置文件通常位于 <code>/etc/nginx/nginx.conf</code>，我们仔细看一下 Nginx 的配置文件。</p>
<h3 id="对-upstream-服务器的定义">对 upstream 服务器的定义</h3>
<p><img src="https://cdn.spencer.felinae98.cn/blog/2020/07/20200722-214045-4.png" alt="对 upstream 服务器的声明"></p>
<p>首先，我们在 Nginx 的 <code>http</code> 项目下，定义了我们可能需要用到的 upstream 服务器。比如，对 Tiny Tiny RSS 来说，就是 181 端口的服务，用下面的语法进行声明：</p>
<pre><code class="language-nginx">upstream ttrssdev {
    server 127.0.0.1:181;
    keepalive 64;
}</code></pre>
<p>这样，下面 <code>ttrss.tenkeyseven.com</code> 域名的 Proxy 转发规则就可以直接用 <code>http://ttrssdev</code> 的格式进行声明了。</p>
<h3 id="默认-web-root-的定义">默认 Web Root 的定义</h3>
<p><img src="https://cdn.spencer.felinae98.cn/blog/2020/07/20200722-214045-5.png" alt="默认访问 Web Root 的配置"></p>
<p>可以发现，当我们服务并不绑定任何上游服务器，而是直接由 Nginx 来提供服务的时候，Nginx 可以直接 serve 一个目标路径里面的 HTML 文件，比如这里的 <code>/usr/share/nginx/html</code>。此时，直接访问默认主域名 <code>tenkeyseven.com</code>，我们就能直接看到一个默认的静态网页。具体来说，Nginx 就是下面这部分内容的配置，知道当请求匹配到 <code>server_name</code> 定义的域名时，serve <code>root</code> 处定义的 HTML 静态网站。</p>
<pre><code class="language-nginx">server {
   server_name tenkeyseven.com; # managed by Certbot
       root         /usr/share/nginx/html;
       # ...
}</code></pre>
<h3 id="proxy-转发规则的定义">Proxy 转发规则的定义</h3>
<p><img src="https://cdn.spencer.felinae98.cn/blog/2020/07/20200722-214045-6.png" alt="反向代理 Tiny Tiny RSS 转发规则的定义"></p>
<p>当我们需要让 Nginx 反向代理转发我们的请求至 upstream 服务的时候，就是类似这样的配置。此时，当请求匹配到 <code>server_name</code> 定义的 <code>ttrss.tenkeyseven.com</code> 时，Nginx 不 serve <code>root</code> 处定义的页面，而是在 <code>location / { ... }</code> 处找到 <code>proxy_pass</code> 的定义，将请求对应的转发到目标服务那里。可以看到，这里我们直接声明了 <code>proxy_pass http://ttrssdev;</code>，于是，当我们访问 <code>https://ttrss.tenkeyseven.com</code> 的时候，Nginx 反向代理服务会直接将请求转发至我们部署的 Tiny Tiny RSS 服务，在服务器上就是 <code>localhost:181</code> 的服务。</p>
<p>Nginx 反向代理具体的配置如下 <code>location / { ... }</code> 里面的配置：</p>
<pre><code class="language-nginx">server {
   server_name ttrss.tenkeyseven.com; # managed by Certbot
       # root         /usr/share/nginx/html;

       # Load configuration files for the default server block.
       include /etc/nginx/default.d/*.conf;

       location / {
           proxy_redirect off;
           proxy_pass http://ttrssdev;

           proxy_set_header  Host                $http_host;
           proxy_set_header  X-Real-IP           $remote_addr;
           proxy_set_header  X-Forwarded-Ssl     on;
           proxy_set_header  X-Forwarded-For     $proxy_add_x_forwarded_for;
           proxy_set_header  X-Forwarded-Proto   $scheme;
           proxy_set_header  X-Frame-Options     SAMEORIGIN;

           client_max_body_size        100m;
           client_body_buffer_size     128k;

           proxy_buffer_size           4k;
           proxy_buffers               4 32k;
           proxy_busy_buffers_size     64k;
           proxy_temp_file_write_size  64k;
       }

    # ...
}</code></pre>
<p>事实上，这部分的配置非常简单。得益于 Let&#39;s Encrypt 的存在，我们可以利用 Certbot 在签署每个域名的 SSL 证书时，自动生成对应服务的转发配置。因此，事实上我们只需要声明前面介绍的 upstream 服务，并在 Certbot 生成的对应域名下的 location 子项处将服务器对应到反向代理的配置项处即可。</p>
<h2 id="小结">小结</h2>
<p>最后，我们可以看到，经过这样的配置，我们从外界互联网访问我们服务器的请求，就被 Nginx 反向代理分别导向了对应的服务器，从而实现了多个域名对应多个服务，并部署在同一个服务器上面的功能。同时，Nginx 反向代理服务统一帮我们管理了 SSL 证书的签署，因此无论是从外界来访问我们服务器的请求，还是我们服务器里面某个服务返回给外界的请求，都是经过加密的 HTTPS 请求。</p>
<p><img src="https://cdn.spencer.felinae98.cn/blog/2020/07/20200722-214045-7.png" alt="Nginx 反向代理在上文中的功能"></p>
<p>Nginx 反向代理服务器还有更多的功能，比如：</p>
<ul>
<li>前文提到的负载均衡（Load balance）</li>
<li>用反向代理来作为 CDN，cache 一部分资源，加快访问速度</li>
<li>在请求到达目标服务器之前，反向代理服务器事先过滤掉一部分恶意请求，保证提供服务的目标服务器的稳定工作</li>
<li>……</li>
</ul>
<p>抛砖引玉，感谢阅读。</p>
<p>[^1]: <a href="https://www.cloudflare.com/learning/cdn/glossary/reverse-proxy/">What Is A Reverse Proxy? | Proxy Servers Explained - Cloudflare</a></p>
<p>[^2]: <a href="https://www.youtube.com/watch?v=KBXTnrD_Zs4">Block EVERY Online Ad with THIS / Linus Tech Tips</a></p>
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Tiny Tiny RSS | 最速部署私有 RSS 服务器]]></title>
            <link>https://blog.spencerwoo.com/2019/11/tiny-tiny-rss/</link>
            <guid>https://blog.spencerwoo.com/2019/11/tiny-tiny-rss/</guid>
            <pubDate>Mon, 18 Nov 2019 00:00:00 GMT</pubDate>
            <content:encoded><![CDATA[<p>9012 年了，别的小朋友们服务器上面跑着各式各样的东西：博客、云盘、监控脚本……再看看我们那台闲（bai）置（piao）很久的阿里云 Server，里面的 Git 竟然还是 1.8 的上古版本。🤦‍♂️ 不如把那台服务器拿出来，部署一个属于自己的 RSS 服务器，甚至可以跟同学们一起用呢～</p>
<h2 id="rss">RSS</h2>
<p>首先我们来说一说 RSS。RSS 的全称是 Really Simple Syndication（简易信息聚合），它是一种消息来源的格式规范，网站可以按照这种格式规范提供文章的标题、摘要、全文等信息给订阅用户，用户可以通过订阅不同网站 RSS 链接的方式将不同的信息源进行聚合，在一个工具里阅读这些内容。</p>
<p>对于第一次接触 RSS 的同学，推荐大家阅读：<a href="https://sspai.com/post/56391">高效获取信息，你需要这份 RSS 入门指南</a>，进行扫盲。</p>
<p>市面上有非常多的 RSS 聚合服务，来帮助我们统一管理、订阅、更新、筛选 RSS 源推送给我们的更新信息，避免我们被海量的文章淹没，也能保证我们多个设备上 RSS 的阅读进度一致。Feedly、Inoreader 等等都是非常不错的 RSS 服务，但是它们的免费版本都有着一定的限制，有时候无法满足我们的全部功能需求，而动辄一个月数十刀的订阅费用又让人望而却步。不慌，开源的 RSS 服务：Tiny Tiny RSS 可以满足我们 RSS 订阅的全部需求！</p>
<h2 id="tiny-tiny-rss-的搭建">Tiny Tiny RSS 的搭建</h2>
<p><img src="https://cdn.spencer.felinae98.cn/blog/2020/07/20200722-220309.jpg" alt="使用 Feedly 主题的 Tiny Tiny RSS 服务端"></p>
<p>Tiny Tiny RSS 是一个非常优秀的开源免费 RSS 服务引擎，可以直接部署在我们自己的服务器上面，借助于 Docker 优秀方便的容器技术和 Let’s Encrypt 异常简单的 SSL 证书签署机器人 <code>certbot</code>，我们几分钟之内就可以部署上线属于我们自己的 RSS 服务，运行在我们可控的服务器上，环境稳定，刷新及时，并且完全免费。👍（当然，除了服务器需要一定的费用。）</p>
<p>在容器、HTTPS 证书自动签署和虚拟化技术极度发达的今天，整个部署过程非常方便简单。我接下来只利用 iPad 进行讲解演示我们的部署过程。请大家坐和放宽，我们立刻开始。</p>
<h2 id="准备工作">准备工作</h2>
<p>在开始之前，首先我们需要准备一个位于公网的服务器，以及一个可以通过 SSH 连接到服务器上的本地设备。这里我使用我同（bai）学（piao）的已经备案的阿里云服务器作为运行 Tiny Tiny RSS 的服务器，并使用 iPad 和 Blink Shell（一个支持 SSH 协议的 iOS 终端 App）作为我的操作设备。Blink Shell 是 iPad 上面最好用的 SSH/Mosh 工具，推荐大家使用。我们在 Blink Shell 中配置好服务器私钥，通过 SSH 登录服务器。</p>
<p><img src="https://cdn.spencer.felinae98.cn/blog/2020/07/20200722-220309-1.jpg" alt="利用 Blink Shell 登录至服务器"></p>
<h2 id="利用-docker-部署-tiny-tiny-rss">利用 Docker 部署 Tiny Tiny RSS</h2>
<h3 id="安装-docker">安装 Docker</h3>
<p>Docker 是非常优秀的虚拟化容器，借助于 Docker 我们可以方便的部署 Tiny Tiny RSS，首先我们在服务器上安装 Docker 本体。在服务器上面执行下面命令来安装 Docker：</p>
<pre><code class="language-bash">curl -fsSL https://get.docker.com/ | sh</code></pre>
<p>然后启动 Docker 服务：</p>
<pre><code class="language-bash">sudo systemctl start docker</code></pre>
<p>然后，我们检查一下 Docker 是否启动成功。我们执行命令：<code>sudo systemctl status docker</code>：</p>
<p><img src="https://cdn.spencer.felinae98.cn/blog/2020/07/20200722-220309-2.jpg" alt="检查 Docker 服务状态"></p>
<p>看到如上的输出，说明我们 Docker 服务启动成功。</p>
<p><em>参考资料：<a href="https://docs.docker.com/install/linux/docker-ce/centos/">Get Docker Engine - Community for CentOS | Docker Documentation</a></em></p>
<h3 id="安装-docker-compose">安装 docker-compose</h3>
<p>接下来我们安装 <code>docker-compose</code>：一个管理和启动多个 Docker 容器的工具。由于 Tiny Tiny RSS 依赖有 PostgreSQL 的数据库服务以及 <a href="https://github.com/HenryQW/mercury_fulltext">mercury_fulltext</a> 的全文抓取服务等等，这些服务我们都借助于 Docker 部署，因此利用 <code>docker-compose</code> 就会大大降低我们的部署难度。</p>
<p>我们继续，在服务器上面执行下面的命令来安装 <code>docker-compose</code>：</p>
<pre><code class="language-bash">curl -L https://github.com/docker/compose/releases/download/1.25.0/docker-compose-`uname -s`-`uname -m` -o /usr/local/bin/docker-compose</code></pre>
<p>之后给予安装好的 <code>docker-compose</code> 可执行权限：</p>
<pre><code class="language-bash">chmod +x /usr/local/bin/docker-compose</code></pre>
<p><em>参考资料：<a href="https://docs.docker.com/compose/install/">Install Docker Compose | Docker Documentation</a></em></p>
<p>最后我们运行 <code>docker-compose --version</code> 来检查安装是否成功。如果有如下输出，说明我们的 <code>docker-compose</code> 安装成功：</p>
<p><img src="https://cdn.spencer.felinae98.cn/blog/2020/07/20200722-220309-3.jpg" alt="检查 docker-compose 安装情况"></p>
<h3 id="安装-tiny-tiny-rss-及其周边服务">安装 Tiny Tiny RSS 及其周边服务</h3>
<p>准备工作已经全部完成，接下来我们下载由 Awesome-TTRSS 配置的 Tiny Tiny RSS 服务的 docker-compose 配置文件：</p>
<pre><code class="language-bash"># 创建 ttrss 目录并进入
mkdir ttrss &amp;&amp; cd ttrss

# 利用 curl 下载 ttrss 的 docker-compose 配置文件至服务器
curl -fLo docker-compose.yml https://github.com/HenryQW/Awesome-TTRSS/raw/master/docker-compose.yml</code></pre>
<p>修改 docker-compose.yml 里面的内容：</p>
<p><img src="https://cdn.spencer.felinae98.cn/blog/2020/07/20200722-220309-4.png" alt="修改 docker-compose 配置文件"></p>
<ul>
<li>在配置文件的第 7 行和第 23 行，将 PostgreSQL 数据库的默认密码进行修改。暴露在公网的数据库使用默认密码非常危险。</li>
<li>在配置文件的第 18 行，将 Tiny Tiny RSS 服务的部署网址修改。比如我的部署网址是 <code>https://ttrss.tenkeyseven.com/</code><ul>
<li>注意，如果你的部署 URL 包含端口（比如默认部署端口为 181 端口），那么这里的 URL 也需要加上端口号，格式为 <code>{网址}:{端口}</code></li>
<li>不过不必担心，如果你这里的 URL 配置不正确，那么访问 Tiny Tiny RSS 的时候，Tiny Tiny RSS 会提醒你修改这里的值为正确的 URL，按照提醒进行配置即可</li>
</ul>
</li>
</ul>
<p>之后，我们保存配置文件，启动 Tiny Tiny RSS 服务。在刚刚的 <code>ttrss</code> 目录下执行：</p>
<pre><code class="language-bash">docker-compose up -d</code></pre>
<p>等待脚本执行完成，如果一切没有问题，那么接下来输入 <code>docker ps</code>，我们应该看到类似下面的结果：</p>
<p><img src="https://cdn.spencer.felinae98.cn/blog/2020/07/20200722-220309-5.jpg" alt="查看正在运行的 Docker 容器"></p>
<p>上面内容表示我们开启了四个 Docker 容器，分别是：</p>
<ul>
<li>Tiny Tiny RSS 本身，监听端口为 <code>0.0.0.0:181 -&gt; 80</code>，同时暴露给外网</li>
<li>PostgreSQL 数据库，仅供内部使用</li>
<li>Mercury 全文抓取服务，仅供内部使用</li>
<li>OpenCC 简体、繁体中文转换服务，仅供内部使用</li>
</ul>
<p>如果发现问题，修改 docker-compose 配置文件后，需要执行下面的命令重启 Docker 容器们：</p>
<pre><code class="language-bash"># 关闭 Docker 容器们
docker-compose down

# 删除已停止的 Docker 容器
docker-compose rm

# ……
# 修改 docker-compose 配置文件
# ……

# 再次开启 Docker 服务
docker-compose up -d</code></pre>
<h2 id="安装-nginx-作为-docker-容器的反向代理">安装 Nginx 作为 Docker 容器的反向代理</h2>
<p>事实上，到上一步，如果我们访问 <code>{服务器 IP}:181</code>，应该可以直接看到 Tiny Tiny RSS 的 Web 前端，但是 Tiny Tiny RSS 并不能直接配置 SSL 证书，也就没法添加 HTTPS 支持。我们利用 Nginx 作为反向代理服务器，即可方便的给 Tiny Tiny RSS 单独绑定一个我们希望的域名，并利用 Let’s Encrypt 来部署 HTTPS。</p>
<h3 id="安装-nginx">安装 Nginx</h3>
<p>首先我们来安装 Nginx，以 CentOS 为例，我们直接执行下面命令即可：</p>
<pre><code class="language-bash">sudo yum install nginx</code></pre>
<p>之后开启 Nginx 服务：</p>
<pre><code class="language-bash">sudo systemctl start nginx</code></pre>
<p>检查 Nginx 是否启动成功：</p>
<pre><code class="language-bash">sudo systemctl status nginx</code></pre>
<p><img src="https://cdn.spencer.felinae98.cn/blog/2020/07/20200722-220309-6.jpg" alt="检查 Nginx 运行状态"></p>
<h3 id="签署-ssl-证书，部署-https">签署 SSL 证书，部署 HTTPS</h3>
<p>之后，我们利用 Let’s Encrypt 提供的 <code>certbot</code> 直接为 Nginx 配置 SSL 证书。首先，我们执行下面的命令安装 <code>certbot</code>：</p>
<pre><code class="language-bash">sudo yum install certbot python2-certbot-nginx</code></pre>
<p>然后运行 <code>certbot</code> 来签署 SSL 证书并自动配置 Nginx 服务：</p>
<pre><code class="language-bash">sudo certbot --nginx</code></pre>
<p><em>参考资料：<a href="https://certbot.eff.org/lets-encrypt/centosrhel7-nginx">Certbot | Nginx on CentOS/RHEL 7</a></em></p>
<p>在这里，certbot 会要求我们输入我们希望签署 SSL 证书的域名，我们选择为 Tiny Tiny RSS 分配的域名（比如我的就是 <code>ttrss.tenkeyseven.com</code>）即可。另外，如果 certbot 询问是否需要将访问该网址的全部流量重定向至 HTTPS，那么选择「是」即可。我们等待脚本执行签署任务完毕，然后重启 Nginx 服务：</p>
<pre><code class="language-bash">sudo systemctl restart nginx</code></pre>
<p>此时我们如果直接访问这一域名，应该就可以看到带有 HTTPS 的 Nginx 默认网站：</p>
<p><img src="https://cdn.spencer.felinae98.cn/blog/2020/07/20200722-220309-7.jpg" alt="HTTPS 配置成功的 Nginx 默认网站"></p>
<p>接下来，我们修改 Nginx 的配置文件，配置 Nginx 反向代理，将访问 <code>https://ttrss.tenkeyseven.com</code> 的请求指向我们刚刚部署好的 Tiny Tiny RSS 服务，对服务器来说，也就是 <code>127.0.0.1:181</code> 这一地址。（如果你没有更改 Tiny Tiny RSS 的端口号的话。）</p>
<p>Nginx 的配置文件位于 <code>/etc/nginx/nginx.conf</code>，我们打开这一文件：</p>
<p><img src="https://cdn.spencer.felinae98.cn/blog/2020/07/20200722-220309-8.png" alt="Nginx 配置文件"></p>
<ul>
<li><p>在 <code>http</code> 项下，<code>server</code> 项前定义 <code>upstream</code> 服务：</p>
<pre><code>  upstream ttrssdev {
      server 127.0.0.1:181;
      keepalive 64;
  }</code></pre><p>  <img src="https://cdn.spencer.felinae98.cn/blog/2020/07/20200722-220309-9.jpg" alt="Nginx upstream 服务声明"></p>
</li>
<li><p>在刚刚 <code>certbot</code> 为我们生成好的响应域名 <code>server</code> 项下，注释掉第一行定义 <code>root</code> 的内容，并将 <code>location /</code> 项修改为：</p>
<pre><code>  location / {
      proxy_redirect off;
      proxy_pass http://ttrssdev;

      proxy_set_header  Host                $http_host;
      proxy_set_header  X-Real-IP           $remote_addr;
      proxy_set_header  X-Forwarded-Ssl     on;
      proxy_set_header  X-Forwarded-For     $proxy_add_x_forwarded_for;
      proxy_set_header  X-Forwarded-Proto   $scheme;
      proxy_set_header  X-Frame-Options     SAMEORIGIN;

      client_max_body_size        100m;
      client_body_buffer_size     128k;

      proxy_buffer_size           4k;
      proxy_buffers               4 32k;
      proxy_busy_buffers_size     64k;
      proxy_temp_file_write_size  64k;
  }</code></pre><p>  <img src="https://cdn.spencer.felinae98.cn/blog/2020/07/20200722-220309-10.jpg" alt="Nginx 配置文件：反向代理配置"></p>
</li>
</ul>
<p>这样，我们再次执行 <code>sudo systemctl restart nginx</code> 重启 Nginx 服务，一切顺利的话，我们就可以通过我们刚刚签署 SSL 证书的域名访问我们部署好的 Tiny Tiny RSS 服务了！鼓掌 👏</p>
<p>Tiny Tiny RSS 的默认管理员账户密码是 admin 和 password，请在第一时间进行修改。</p>
<h2 id="配置-tiny-tiny-rss">配置 Tiny Tiny RSS</h2>
<p><img src="https://cdn.spencer.felinae98.cn/blog/2020/07/20200722-220309-11.jpg" alt="Tiny Tiny RSS 配置、主题"></p>
<p>如果上面步骤没有问题的话，我们在服务器上面所部署的 Tiny Tiny RSS 本身就已经包含了：</p>
<ul>
<li>Mercury 全文提取服务（默认未开启）</li>
<li>OpenCC 繁简自动转换服务（默认未开启）</li>
<li>Fever 格式输出插件（默认已开启，用来和 Reeder 等客户端进行连接）</li>
<li>包括 Feedly、RSSHub 在内的多款主题</li>
<li>等等……</li>
</ul>
<p>我们不需要多余的配置，开箱即可使用上面的主题和插件，根本不需要操心其他服务的部署和安装。我们登录自己的 Tiny Tiny RSS，在右上角「设置→ 插件」中即可启用上述插件，在「设置 → 主题」处就可以更改我们部署的 Tiny Tiny RSS 所用的主题。这些插件和主题在 <a href="https://sspai.com/post/41302">如何搭建属于自己的 RSS 服务，高效精准获取信息</a> 中已经介绍了使用方法，这里我就不再赘述了。</p>
<p>如果有同学对上面的配置还有问题，请直接参考 <a href="https://ttrss.henry.wang/zh/#%E6%8F%92%E4%BB%B6">Awesome TTRSS 的官方文档：🐋 Awesome TTRSS | 插件</a></p>
<h2 id="小结">小结</h2>
<p>Tiny Tiny RSS 的配置到这里就基本结束了，我相信你通过上面的配置一定已经在自己的服务器上部署成功了 Tiny Tiny RSS 服务，并为它添加了域名和 HTTPS 的支持。另外，Tiny Tiny RSS 还自带了账号系统，可以邀请其他志同道合的朋友们一起使用我们自己部署的 Tiny Tiny RSS。感谢阅读。</p>
<p>📖 关联阅读：</p>
<ul>
<li><a href="https://ttrss.henry.wang/zh/#%E5%85%B3%E4%BA%8E">🐋 Awesome TTRSS</a></li>
<li><a href="https://sspai.com/post/56893">少数派 sspai - Docker 的入门「指北」</a></li>
<li><a href="https://sspai.com/post/41302">少数派 sspai - 如何搭建属于自己的 RSS 服务，高效精准获取信息</a></li>
</ul>
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Working Copy + iA Writer：第二次尝试从我的 iPad 上面更新博客]]></title>
            <link>https://blog.spencerwoo.com/2019/11/update-from-ipad-iawriter/</link>
            <guid>https://blog.spencerwoo.com/2019/11/update-from-ipad-iawriter/</guid>
            <pubDate>Thu, 14 Nov 2019 00:00:00 GMT</pubDate>
            <content:encoded><![CDATA[<p>之前，我就曾经介绍过我是如何利用 iPad 对部署在 GitHub 上面的静态博客进行更新的。这篇文章即使到现在也有一定的借鉴意义，文章归档于：<a href="https://archive.spencerwoo.com/posts/2019/06/09/from-my-ipad.html">使用 Working Copy 在 iPad 上面更新博客 | 一次随缘的更新</a>。</p>
<p>现在，我重新部署了我的静态博客，利用 Hugo 进行构建。宣称全球最快的静态网站渲染引擎 —— Hugo 着实让我博客的编译和部署过程快人一步。借助于方便的 Netlify，只要配置好 CI 的编译命令和环境变量，我们就只需要专注于博客本身的撰写，而不必对博客其他属性进行过多的担心。这样的部署方法，让 iPad 都可以直接发表博文。</p>
<h2 id="前言">前言</h2>
<p>为什么我又换回了静态博客？Listed 确实不错，但是我在给与之配套的笔记应用 Standard Notes 充值一年会员之后，发现 Standard Notes 真的不行。不论是应用本身的响应还是界面的设计，Standard Notes 都和 Bear 等一众笔记应用相距甚远。Listed 博客需要会员才能自定义域名，并且 Listed 本身也没有评论系统，只有一个类似留言板的 Guestbook，难过。</p>
<p>另外，我还专门问了问开发者为什么 Listed 不支持评论：</p>
<p><img src="https://cdn.spencer.felinae98.cn/blog/2020/07/20200722-220619.png" alt="和 Listed/Standard Notes 作者的交流"></p>
<p>// 我猜是开发者被垃圾评论骂怕了 😂</p>
<p>就这样，Hugo 就变成了目前我的博客部署引擎首选。</p>
<h2 id="设备和工具">设备和工具</h2>
<p>更新博客我试用了下面的设备：</p>
<ul>
<li>iPad Air (3rd Gen)</li>
<li>Logitech K380：键盘</li>
<li>Logitech M558：鼠标</li>
</ul>
<p>在 iPad 上面，我尝试使用 iA Writer 来编辑 Markdown 文档，利用 Working Copy 来更新 GitHub 仓库。由于 Working Copy 支持 <strong>Edit in place</strong>，因此我们在 iA Writer 中可以导入 Working Copy 的某个文件夹（比如博文文件夹 <code>posts</code>），从而直接编辑其中的 Markdown 文件。</p>
<p><img src="https://cdn.spencer.felinae98.cn/blog/2020/07/20200722-220619-1.png" alt="在 iA Writer 中直接编辑 Working Copy 文件"></p>
<h2 id="更新流程">更新流程</h2>
<ul>
<li>利用 Working Copy 将博客源文件克隆至 iPad 上面</li>
</ul>
<p><img src="https://cdn.spencer.felinae98.cn/blog/2020/07/20200722-220619-2.png" alt="Working Copy 上面克隆仓库"></p>
<ul>
<li>利用 iA Writer 打开 Working Copy 中的博客 <code>posts</code> 文件夹</li>
<li>在 iA Writer 中创建新文章、编辑旧博客文章</li>
<li>在 iA Writer 中通过快捷键 <code>Command + R</code> 直接预览文章</li>
</ul>
<p><img src="https://cdn.spencer.felinae98.cn/blog/2020/07/20200722-220619-3.png" alt="iA Writer 编辑文章"></p>
<ul>
<li>在 Working Copy 中通过 Git 直接将博客同步至 GitHub</li>
</ul>
<p>iPad 胜在方便，不需要过多的配置即可直接撰写文章。爱了，❤️。</p>
<h2 id="另外">另外</h2>
<p>除了上面的方法，Forestry.io 也是一个可以的选择。Forestry.io 是一个极度完善的静态博客 CMS 统一管理平台，支持 Hugo、VuePress、Gatsby 等诸多博客引擎。利用 Forestry 在线网页版本的后台管理，我们甚至可以直接的撰写文章内容，并在 Forestry 服务器上面渲染文章并预览。</p>
<p><img src="https://cdn.spencer.felinae98.cn/blog/2020/07/20200722-220619-4.png" alt="Forestry 的文章编辑界面"></p>
<p>Forestry 虽然好用，但是 Forestry 在 iPad 上面的编辑体验并不太好，因为相对小的屏幕，Forestry 的文章编辑界面相当狭窄，令人遗憾。</p>
<p>总之，Working Copy 是 iPad 的最佳搭档，而 iA Writer 是 iPad 上面最纯粹的 Markdown 编辑器。这二者结合，确实能让我随时随地发布博客。爱了！❤️</p>
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Batch Git Pull：分享一个维护多个 Git 仓库的小脚本]]></title>
            <link>https://blog.spencerwoo.com/2019/11/batch-git-pull/</link>
            <guid>https://blog.spencerwoo.com/2019/11/batch-git-pull/</guid>
            <pubDate>Wed, 13 Nov 2019 00:00:00 GMT</pubDate>
            <content:encoded><![CDATA[<p>不废话，放脚本：</p>
<pre><code class="language-bash">find . -maxdepth 3 -name .git -type d | rev | cut -c 6- | rev | xargs -I {} git -C {} pull</code></pre>
<p>更为方便的，直接将这部分加入你的 <code>.zshrc</code> 或者 <code>.bash_profile</code>：</p>
<pre><code class="language-bash">alias gpall=&quot;find . -maxdepth 3 -name .git -type d | rev | cut -c 6- | rev | xargs -I {} git -C {} pull&quot;</code></pre>
<p>之后，直接执行 <code>gpall</code> 即可。</p>
<p>好了，我们进入正题。</p>
<h2 id="维护多个-git-仓库的需求">维护多个 Git 仓库的需求</h2>
<p>维护多个 Git 仓库不容易。我在我存放 GitHub 仓库的目录下运行了一下 <code>tree</code>：</p>
<pre><code>.
├── AIP_BackEnd
├── Evaluation_BackEnd
| ... ...
├── SchoolProjects
│   ├── Distance-Vector-Algorithm
│   ├── cartoonize-images
| ... ...
│   ├── zanpress-blog
│   └── zanpress-diagram
| ... ...
└── wechat-format

103 directories</code></pre><p>103 个目录……我自己 <code>Documents/GitHub</code> 文件夹下就有这么多 Git 仓库，一个一个去更新真的很费事情。如何批量更新 GitHub 本地仓库呢？其实就是一个遍历目录，对匹配到的 Git 仓库在其当前分支下执行 <code>git pull</code> 的需求嘛，很简单。</p>
<h2 id="解决方法">解决方法</h2>
<p>在 Medium 上面，我找到了一个相当优雅的脚本。前面这个脚本已经分享给各位了，我们重新看一下：</p>
<pre><code class="language-bash">find . -maxdepth 3 -name .git -type d | rev | cut -c 6- | rev | xargs -I {} git -C {} pull</code></pre>
<p>可以发现，这一长串命令，事实上就是前面的命令执行结果通过「管道」输出至后面的命令作为输入，也就是命令中 <code>|</code> 的功能。我们一段一段看一下这个命令具体都干了什么。</p>
<h3 id="用-find-搜索目录下全部-git-文件夹">用 <code>find</code> 搜索目录下全部 <code>.git/</code> 文件夹</h3>
<p>每个 Git 文件夹里面一定有 <code>.git</code> 的目录，我们只需要找到 <code>.git</code> 文件夹既可以找到 Git 目录。</p>
<pre><code class="language-bash">find . -maxdepth 3 -name .git -type d</code></pre>
<p>这里，我们就使用了 <code>find</code> 的命令，详见：<a href="https://www.gnu.org/software/findutils/manual/html_mono/find.html">GNU - Finding Files</a>。我们将命令分解来看：</p>
<ul>
<li><code>.</code> 表示匹配命令执行路径下的全部文件与文件夹</li>
<li><code>-maxdepth 3</code> 表示向下搜索最多三层级目录</li>
<li><code>-name .git</code> 就是搜索名称为 <code>.git</code> 的内容</li>
<li><code>-type d</code> 则指明了我们搜索的范畴：Directories（目录）</li>
</ul>
<p><img src="https://cdn.spencer.felinae98.cn/blog/2020/07/20200722-214839.png" alt="搜索 .git 文件夹"></p>
<p>一目了然，我们下面就这样对每个命令进行分解和解释。</p>
<h3 id="裁剪出我们要的-git-文件夹所在路径">裁剪出我们要的 <code>.git</code> 文件夹所在路径</h3>
<p>上面我们解析出来的路径，每个路径后面都包含一个 <code>.git</code>，我们需要统一将这个 <code>.git</code> 从字符串中删掉，这样才能一起对给定目录执行 <code>git pull</code>。第二步我们进行目录的裁剪。</p>
<pre><code class="language-bash">... | rev | cut -c 6- | rev | ...</code></pre>
<p>可以发现，这里我们有三部分命令。我们依次对命令进行解析：</p>
<ol>
<li><code>rev</code>：首先对搜索到的目录（字符串）进行反转</li>
<li><code>cut -c 6-</code>：我们利用 <code>cut</code> 工具将路径进行裁剪，<code>-c</code> 表示删减的是字符（Characters），<code>6-</code> 表示我们删去路径的前 6 个字符（即：<code>.git</code>）</li>
<li><code>rev</code>：将处理好的字符串反转回来</li>
</ol>
<p><img src="https://cdn.spencer.felinae98.cn/blog/2020/07/20200722-214839-1.png" alt="裁剪路径"></p>
<h3 id="利用-xargs-执行带参数的-git-pull">利用 <code>xargs</code> 执行带参数的 <code>git pull</code></h3>
<p>上一步，我们已经提取出来所有包含 <code>.git</code> 的文件夹，现在我们需要批量的执行 <code>git pull</code> 来统一拉取仓库。</p>
<pre><code class="language-bash">xargs -I {} git -C {} pull</code></pre>
<p>由于 <code>git</code> 并不支持传入目录等参数，因此我们需要借助于 <code>xargs</code> 来给 <code>git</code> 传入拉取路径。上面的命令简明易懂，就相当于 <code>xargs</code> 告诉 <code>git</code> 拉取以上目录下的全部 Git 仓库。我们来看一看效果：</p>
<p><img src="https://cdn.spencer.felinae98.cn/blog/2020/07/20200722-214839-2.gif" alt="脚本效果"></p>
<p>b(￣▽￣)d 👍 成功~</p>
<h2 id="📚-references">📚 References</h2>
<ul>
<li><a href="https://medium.com/@codenameyau/updating-multiple-repos-with-one-command-9768c8cdfe46">Updating Multiple Repos With One Command</a></li>
<li><a href="http://www.ruanyifeng.com/blog/2019/08/xargs-tutorial.html">xargs 命令教程</a></li>
</ul>
]]></content:encoded>
        </item>
    </channel>
</rss>
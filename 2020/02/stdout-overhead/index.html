<!DOCTYPE html>
<html data-html-server-rendered="true" lang="en" data-vue-tag="%7B%22lang%22:%7B%22ssr%22:%22en%22%7D%7D">
  <head>
    <title>「输出」开销：为什么我们向 stdout 输出时那么慢？ - Spencer&#x27;s Blog</title><meta name="gridsome:hash" content="1a485f6dceb298c3ce36b831edab4aa62783906c"><meta data-vue-tag="ssr" charset="utf-8"><meta data-vue-tag="ssr" name="generator" content="Gridsome v0.7.23"><meta data-vue-tag="ssr" data-key="viewport" name="viewport" content="width=device-width, initial-scale=1, viewport-fit=cover"><meta data-vue-tag="ssr" data-key="format-detection" name="format-detection" content="telephone=no"><meta data-vue-tag="ssr" name="description" content="Console output overhead: why is writing to stdout so slow?"><link data-vue-tag="ssr" rel="icon" href="data:,"><link data-vue-tag="ssr" rel="icon" type="image/png" sizes="16x16" href="/assets/static/favicon.ce0531f.d1135162e600804de5a368c5252e558c.png"><link data-vue-tag="ssr" rel="icon" type="image/png" sizes="32x32" href="/assets/static/favicon.ac8d93a.d1135162e600804de5a368c5252e558c.png"><link rel="preload" href="/assets/css/0.styles.505fe61b.css" as="style"><link rel="preload" href="/assets/js/app.95a869fe.js" as="script"><link rel="preload" href="/assets/js/page--src--templates--blog-post-vue.8c30e663.js" as="script"><link rel="prefetch" href="/assets/js/page--src--pages--404-vue.0121d1c3.js"><link rel="prefetch" href="/assets/js/page--src--pages--archives-vue.2aef679f.js"><link rel="prefetch" href="/assets/js/page--src--pages--friends-vue.eb0e6337.js"><link rel="prefetch" href="/assets/js/page--src--pages--index-vue.933a26a7.js"><link rel="prefetch" href="/assets/js/page--src--templates--tag-vue.11c8826a.js"><link rel="stylesheet" href="/assets/css/0.styles.505fe61b.css"><script data-vue-tag="ssr" src="https://umami.marxchou.com/script.js" defer data-website-id="73b29141-fccf-4d54-9c2f-f7d0d146f86b"></script><noscript data-vue-tag="ssr"><style>.g-image--loading{display:none;}</style></noscript>
  </head>
  <body >
    <script>
      // Add dark / light detection that runs before Vue.js load. Borrowed from overreacted.io
      ;(function() {
        window.__onThemeChange = function() {}
        function setTheme(newTheme) {
          window.__theme = newTheme
          preferredTheme = newTheme
          document.body.setAttribute('data-theme', newTheme)
          window.__onThemeChange(newTheme)
        }

        var preferredTheme
        try {
          preferredTheme = localStorage.getItem('theme')
        } catch (err) {}

        window.__setPreferredTheme = function(newTheme) {
          setTheme(newTheme)
          try {
            localStorage.setItem('theme', newTheme)
          } catch (err) {}
        }

        var darkQuery = window.matchMedia('(prefers-color-scheme: dark)')
        darkQuery.addListener(function(e) {
          window.__setPreferredTheme(e.matches ? 'dark' : 'light')
        })

        setTheme(preferredTheme || (darkQuery.matches ? 'dark' : 'light'))
      })()
    </script>

    <div id="app" data-server-rendered="true"><div id="nprogress-container"><header class="header"><div class="header__left"><div><a href="/" class="logo active"><span class="logo__text">
      ▲   Spencer's Blog
    </span></a></div></div><div class="header__right"><button role="button" aria-label="Toggle dark/light" class="toggle-theme"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-moon"><path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path></svg></button></div></header></div><main class="main"><div><div class="post-title"><h1 class="post-title__text">
      「输出」开销：为什么我们向 stdout 输出时那么慢？
      <!----></h1><div class="post-meta">
  Posted February 22. 2020.
  
    1230 words.
    <strong>6 min read.
      </strong></div></div><div class="post content-box"><div class="post__header"><img alt="Cover image" src="data:image/svg+xml,%3csvg fill='none' viewBox='0 0 1280 400' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3e%3cdefs%3e%3cfilter id='__svg-blur-c5617f729664b6247de180661ab366a7'%3e%3cfeGaussianBlur in='SourceGraphic' stdDeviation='10'/%3e%3c/filter%3e%3c/defs%3e%3cimage x='0' y='0' filter='url(%23__svg-blur-c5617f729664b6247de180661ab366a7)' width='1280' height='400' xlink:href='data:image/png%3bbase64%2cUklGRigBAABXRUJQVlA4IBwBAAAQBgCdASpAABQAPm0wlUakIq4hJAyRwA2JZAC1G3bHgRG5/8WwduWydzuzDzv3xJMckyNgqozQ9YAA/vmXB70fzx//maP3yeu7cLP4SPExOuZ7qGAoedL2/qsnvMAlyPHHaHcB5TIMAD1IOFTda4/pJh/TLujnxy7cBnovrI8Sv9xg52rO2Im%2bwVTttiouKF/9u17p3T78MGu1U3Te%2byRyt/%2b0ajpX%2bBX1kBhCG6Hagx5s8DIZi2U/n9DCTpy3%2bGEUwJ1vRJs%2b3p6Tx/SK9/7oMqP7bszWSEuOUydyUeCe0ubGusnrBosYHydEwe18FnydHNSbTeYY6vyqeWJRJgnGr5zlb4AAyJuTu0ATKmUUzXDcom4V1h498WjgAA==' /%3e%3c/svg%3e" width="1280" data-src="/assets/static/stdout-overhead.a209973.f73a441d9c80efd3bf7fdcbc3c3bb935.png" data-srcset="/assets/static/stdout-overhead.a67b0b2.f73a441d9c80efd3bf7fdcbc3c3bb935.png 480w, /assets/static/stdout-overhead.a209973.f73a441d9c80efd3bf7fdcbc3c3bb935.png 1280w" data-sizes="(max-width: 1280px) 100vw, 1280px" class="g-image g-image--lazy g-image--loading"><noscript><img src="/assets/static/stdout-overhead.a209973.f73a441d9c80efd3bf7fdcbc3c3bb935.png" class="g-image g-image--loaded" width="1280" alt="Cover image"></noscript></div><!----><div class="post__content"><blockquote>
<p>本文发布于 Medium: <a href="https://medium.com/spencerweekly/console-output-overhead-why-is-writing-to-stdout-so-slow-b0cc7c88704c" target="_blank" rel="nofollow noopener noreferrer">Console output overhead: why is writing to stdout so slow?</a></p>
</blockquote>
<p>Generally, when we code through our projects, we need to output certain values in order to understand what the program is currently doing and what results it shows. Normally we do this with a simple print function:</p>
<pre class="language-python"><code class="language-python"><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'Hey look, this is the result.'</span><span class="token punctuation">)</span></code></pre>
<p>And for most of our use cases, this would be more than enough. However, when we are outputting a huge amount of data, like printing a file with 10k lines of text onto the terminal, we will eventually suffer from IO bottlenecks. This is something I discovered when trying to find a nice progress bar library for PyTorch in order to monitor the model’s training progress. Let me explain.</p>
<h2 id="what-did-i-discover"><a href="#what-did-i-discover" aria-hidden="true"><span class="icon icon-link"></span></a>What did I discover?</h2>
<p>Now, if you are familiar with modern neural network libraries, you may have heard of Keras. Keras is a highly encapsulated library that takes care of data output, model compilation, backward propaganda and more for you under the hood, meaning that it offers less agility than other low-level libraries like PyTorch. The one feature I really appreciate about Keras is its nice little training progress bar that shows the current training progress, iteration duration, ETA, loss and other relevant statistics. Below is an example.</p>
<figure><img src="https://cdn.spencer.felinae98.cn/blog/2020/07/20200722-215810.png" alt="Visualizing training progress with Keras"><figcaption>Visualizing training progress with Keras</figcaption></figure>
<p>However, PyTorch offers far more customizable features when it comes to designing, compiling and training your model, but it also means you have to take care of outputting progress, training details and others by yourself. This is when I discovered IO bottlenecks when outputting directly to <code class="language-text">stdout</code>, that is, your terminal.</p>
<p>The library I first tried is called <a href="https://github.com/yueyericardo/pkbar" target="_blank" rel="nofollow noopener noreferrer"><strong>pkbar</strong> — a progress bar library intended to bring Keras style progress monitoring to the PyTorch ecosystem</a>. <strong>(Don’t use it!)</strong> It’s outputs are like this:</p>
<figure><img src="https://cdn.spencer.felinae98.cn/blog/2020/07/20200722-215810-1.png" alt="pkbar: Keras style progress bar for PyTorch"><figcaption>pkbar: Keras style progress bar for PyTorch</figcaption></figure>
<p>Most progress bar libraries are implemented by wrapping itself around an iterative object, in our case, a PyTorch Data Loader. When building a progress bar, we need to refresh our progress, statistics and other parameters on each iteration. This means every time we come to the end of an iteration, we need to output to our terminal with a delay when the program waits for the writing to return. This delay is often unnoticed when we are simply writing a single message, but when we are iterating through thousands of images, we need to write the latest status on each iteration. A small delay multiplied by a thousand iterations gives us a huge time difference, and results in bottlenecks we encounter now.</p>
<figure><img src="https://cdn.spencer.felinae98.cn/blog/2020/07/20200722-215810-2.png" alt="Each iteration comes with a time cost that adds up to be a huge delay"><figcaption>Each iteration comes with a time cost that adds up to be a huge delay</figcaption></figure>
<p>What happened to me when I was using pkbar was exactly what I described above: <strong>each iteration was accompanied with a delay which eventually builds up into a time difference I simply can’t ignore.</strong> To put numbers into perspective, I was applying a DeepFool adversarial attack on a pre-trained ResNet18 CNN. It took about 5 minutes on my laptop running on GPU without pkbar, and when I added pkbar to visualize progress, it gave me an ETA of nearly 20 minutes! Holy crap, I removed pkbar almost immediately.</p>
<h2 id="so-why-were-there-delays"><a href="#so-why-were-there-delays" aria-hidden="true"><span class="icon icon-link"></span></a>So why were there delays?</h2>
<p>In order to find out what was causing this huge delay and how we could avoid it, I turned for help on Stack Overflow. A particular question gave me the answer: <a href="https://stackoverflow.com/questions/3857052/why-is-printing-to-stdout-so-slow-can-it-be-sped-up" target="_blank" rel="nofollow noopener noreferrer">Why is printing to stdout so slow? Can it be sped up?</a></p>
<p>In this question, the author compared the following speeds:</p>
<ul>
<li>Writing output to the terminal, which is <code class="language-text">stdout</code></li>
<li>Writing output to a file</li>
<li>Writing output to <code class="language-text">stdout</code>, but with <code class="language-text">stdout</code> redirected to <code class="language-text">/dev/null</code></li>
</ul>
<figure><img src="https://cdn.spencer.felinae98.cn/blog/2020/07/20200722-215810-3.png" alt="Time cost for printing to stdout, file and /dev/null"><figcaption>Time cost for printing to stdout, file and /dev/null</figcaption></figure>
<p>Wow, that shows a huge difference for printing to <code class="language-text">stdout</code> and redirecting <code class="language-text">stdout</code> to <code class="language-text">/dev/null</code>. Even writing output to file is faster than directly writing to the terminal. So…, why?</p>
<blockquote>
<p>Congratulations, you have just discovered the importance of I/O buffering. :-)</p>
</blockquote>
<p>So it turns out that I/O buffering is what made even “writing to file” faster than “writing to <code class="language-text">stdout</code>”. When we are directly writing outputs to our terminal, each writing operation is being done “synchronously”, which means our programs waits for the “write” to complete before it continues to the next commands.</p>
<figure><img src="https://cdn.spencer.felinae98.cn/blog/2020/07/20200722-215810-4.png" alt="Synchronous writing to stdout"><figcaption>Synchronous writing to stdout</figcaption></figure>
<p>Each time our programs writes something to <code class="language-text">stdout</code>, we are met with this delay. However, writing to files are not of the same case. When we are writing to files, we have what is known as “I/O buffering”, which means the program outputs whatever it needs to write to the file, and the OS catches all these contents to write where it then stores in a file <strong>as a bulk</strong> afterwards. <strong>All our “write” functions return before anything is actually written to a file.</strong></p>
<figure><img src="https://cdn.spencer.felinae98.cn/blog/2020/07/20200722-215810-5.png" alt="Buffered writing output to file"><figcaption>Buffered writing output to file</figcaption></figure>
<p>So what should we do here? Well, in order to avoid I/O overhead, we either have to do training and update output in parallel or asynchronously like we do in GUIs when we have an UI thread and a controller, or we will need to write in bulks to <code class="language-text">stdout</code> like what we are doing when writing to files. <strong>It’s a trade-off: interactivity versus bulk efficiency.</strong></p>
<h2 id="what-i-did-to-visualize-pytorch-training-with-minimum-overhead"><a href="#what-i-did-to-visualize-pytorch-training-with-minimum-overhead" aria-hidden="true"><span class="icon icon-link"></span></a>What I did to visualize PyTorch training with minimum overhead?</h2>
<p>After I ditched pkbar, I found a perfect progress bar library: <a href="https://github.com/tqdm/tqdm" target="_blank" rel="nofollow noopener noreferrer">tqdm — A Fast, Extensible Progress Bar for Python and CLI</a>. With over 13.4k+ stars, <code class="language-text">tqdm</code> is easily the best Python library for us to implement training progress visualization.</p>
<figure><img src="https://cdn.spencer.felinae98.cn/blog/2020/07/20200722-215810-6.png" alt="tqdm in action"><figcaption>tqdm in action</figcaption></figure>
<p><code class="language-text">tqdm</code> is simple, efficient and comes with minimal overhead. The author claims that <code class="language-text">tqdm</code> only delays output for a minimum of 60ns per iteration. That’s impressive. In addition to its low overhead, <code class="language-text">tqdm</code> uses smart algorithms to predict the remaining time and to skip unnecessary iteration displays, which allows for a negligible overhead in most cases.</p>
<h3 id="installation"><a href="#installation" aria-hidden="true"><span class="icon icon-link"></span></a>Installation</h3>
<p>Either install <code class="language-text">tqdm</code> using conda directly:</p>
<pre class="language-bash"><code class="language-bash">conda <span class="token function">install</span> <span class="token parameter variable">-c</span> conda-forge tqdm</code></pre>
<p>And install <code class="language-text">ipywidgets</code> with pip to enable Jupyter Notebook integration:</p>
<pre class="language-bash"><code class="language-bash">pip <span class="token function">install</span> ipywidgets</code></pre>
<p>Or integrate the following code to your <code class="language-text">environment.yml</code>:</p>
<pre class="language-yaml"><code class="language-yaml"><span class="token key atrule">channels</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> conda<span class="token punctuation">-</span>forge
  <span class="token punctuation">-</span> defaults
<span class="token key atrule">dependencies</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> tqdm
  <span class="token punctuation">-</span> <span class="token key atrule">pip</span><span class="token punctuation">:</span>
      <span class="token punctuation">-</span> ipywidgets</code></pre>
<p>Then run:</p>
<pre class="language-bash"><code class="language-bash">conda <span class="token function">env</span> update</code></pre>
<h3 id="integration"><a href="#integration" aria-hidden="true"><span class="icon icon-link"></span></a>Integration</h3>
<p>We can easily integrate <code class="language-text">tqdm</code> into our PyTorch project. Basically what you need to do is just to wrap <code class="language-text">tqdm</code> on an iterable object, and that’s it.</p>
<pre class="language-python"><code class="language-python"><span class="token keyword">from</span> tqdm <span class="token keyword">import</span> tqdm
<span class="token keyword">for</span> i <span class="token keyword">in</span> tqdm<span class="token punctuation">(</span><span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">10000</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span></code></pre>
<pre class="language-text"><code class="language-text">76%|████████████████████████████ | 7568/10000 [00:33&lt;00:10, 229.00it/s]</code></pre>
<p>For PyTorch <code class="language-text">DataLoader</code>, we can first load our data like so:</p>
<pre class="language-python"><code class="language-python">dataset_loader <span class="token operator">=</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data<span class="token punctuation">.</span>DataLoader<span class="token punctuation">(</span>dataset<span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">)</span></code></pre>
<p>Then initialize <code class="language-text">tdqm</code>:</p>
<pre class="language-python"><code class="language-python">pbar <span class="token operator">=</span> tqdm<span class="token punctuation">(</span>dataset_loader<span class="token punctuation">)</span></code></pre>
<p>Here, we can add the description of the progress bar:</p>
<pre class="language-python"><code class="language-python">pbar<span class="token punctuation">.</span>set_description<span class="token punctuation">(</span><span class="token string">'Validate predictions'</span><span class="token punctuation">)</span></code></pre>
<p>Also, we can initialize the post-fix dict for the progress bar, so that we can update the post-fix on the fly:</p>
<pre class="language-python"><code class="language-python">pbar<span class="token punctuation">.</span>set_postfix<span class="token punctuation">(</span>loss<span class="token operator">=</span><span class="token string">'0.0%'</span><span class="token punctuation">,</span> acc<span class="token operator">=</span><span class="token string">'0.0%'</span><span class="token punctuation">)</span></code></pre>
<p>And Voilà! We can directly iterate through <code class="language-text">tdqm</code>, just like we did with PyTorch <code class="language-text">DataLoader</code>. (My model is wrapped with Foolbox, so I can make predictions with the <code class="language-text">.forward()</code> function.)</p>
<pre class="language-python"><code class="language-python"><span class="token keyword">for</span> image<span class="token punctuation">,</span> label <span class="token keyword">in</span> pbar<span class="token punctuation">:</span>
  <span class="token comment"># make a prediction</span>
  prob <span class="token operator">=</span> model<span class="token punctuation">.</span>forward<span class="token punctuation">(</span>image<span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

  <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>  <span class="token comment"># calculate and update loss</span>
  pbar<span class="token punctuation">.</span>set_postfix<span class="token punctuation">(</span>loss<span class="token operator">=</span><span class="token string">'{:.2f}%'</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>loss<span class="token punctuation">)</span><span class="token punctuation">)</span>

  <span class="token comment"># calculate and update accuracy</span>
  pbar<span class="token punctuation">.</span>set_postfix<span class="token punctuation">(</span>acc<span class="token operator">=</span><span class="token string">'{:.2f}%'</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>acc<span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre>
<h3 id="features"><a href="#features" aria-hidden="true"><span class="icon icon-link"></span></a>Features</h3>
<p><code class="language-text">tqdm</code> can be used directly in a CLI environment with no special configuration whatsoever.</p>
<figure><img src="https://cdn.spencer.felinae98.cn/blog/2020/07/20200722-215810-7.gif" alt="tqdm in a CLI enviroment"><figcaption>tqdm in a CLI enviroment</figcaption></figure>
<p>Also, we can run <code class="language-text">tqdm</code> in VS Code’s Python Interactive console, or Jupyter Notebook. In this case, <code class="language-text">tqdm</code> will output progress asynchronously according to the <a href="https://jupyter-notebook.readthedocs.io/en/stable/examples/Notebook/Running%20Code.html#Output-is-asynchronous" target="_blank" rel="nofollow noopener noreferrer">Notebook API</a>.</p>
<figure><img src="https://cdn.spencer.felinae98.cn/blog/2020/07/20200722-215810-8.gif" alt="tqdm outputting inside Jupyter Notebook / VS Code Python Interactive"><figcaption>tqdm outputting inside Jupyter Notebook / VS Code Python Interactive</figcaption></figure>
<p>To kick it up a notch, we can actually output “real” progress bars. Utilizing <code class="language-text">ipywidget</code>, we can directly draw interactive progress bars inside Jupyter Notebook. <a href="https://ipywidgets.readthedocs.io/en/latest/user_install.html" target="_blank" rel="nofollow noopener noreferrer">See here for instructions on how to enable </a><code class="language-text">ipywidgets</code><a href="https://ipywidgets.readthedocs.io/en/latest/user_install.html" target="_blank" rel="nofollow noopener noreferrer">.</a></p>
<p>After enabling these widgets inside Jupyter Notebook, all we have to do is change the way on how we imported <code class="language-text">tqdm</code>. Change:</p>
<pre class="language-python"><code class="language-python"><span class="token keyword">import</span> tqdm <span class="token keyword">as</span> tqdm</code></pre>
<p>To:</p>
<pre class="language-python"><code class="language-python"><span class="token keyword">import</span> tqdm<span class="token punctuation">.</span>notebook <span class="token keyword">as</span> tqdm</code></pre>
<p>Now, run the code once more, if everything goes well, we should be able to see a neat progress bar with colors indicating the current task’s state: pending, success or failure.</p>
<figure><img src="https://cdn.spencer.felinae98.cn/blog/2020/07/20200722-215810-9.gif" alt="tqdm integrating directly into Jupyter Notebook with ipywidgets - https://ipywidgets.readthedocs.io/en/latest"><figcaption>tqdm integrating directly into Jupyter Notebook with ipywidgets - https://ipywidgets.readthedocs.io/en/latest</figcaption></figure>
<hr>
<p>This is the end of the article, for more information on how to utilize <code class="language-text">tqdm</code>, I recommend this article: <a href="https://medium.com/better-programming/python-progress-bars-with-tqdm-by-example-ce98dbbc9697" target="_blank" rel="nofollow noopener noreferrer">Python Progress Bars with tqdm by Example</a>. Also, the official documentation of <code class="language-text">tqdm</code> is quite thorough, covering most cases when using <code class="language-text">tqdm</code>.</p>
<p>There’s one thing to note, and that is: you should not use “print” to output anything when a <code class="language-text">tqdm</code> progress bar is running, as this behavior will mess up the <code class="language-text">tqdm</code> output. If you have to write something, do remember to use:</p>
<pre class="language-python"><code class="language-python">tqdm<span class="token punctuation">.</span>write<span class="token punctuation">(</span><span class="token operator">&lt;</span>context<span class="token operator">></span><span class="token punctuation">)</span></code></pre>
<p>That’s all, thanks for reading.</p>
</div><div class="post__footer"><div class="post-tags"><a href="/tag/Tech/" class="post-tags__link"><span>#</span> Tech
  </a><a href="/tag/Neural%20Network/" class="post-tags__link"><span>#</span> Neural Network
  </a><a href="/tag/Terminal/" class="post-tags__link"><span>#</span> Terminal
  </a><a href="/tag/Principle/" class="post-tags__link"><span>#</span> Principle
  </a></div></div><div class="post__navigation"><a href="/2020/03/remote-jupyter-lab/" class="navlink" style="float: left;">◀ Remote Jupyter Lab：如何用 Jupyter Lab 将远程服务器资源最大化利用</a><a href="/2020/02/dont-use-anaconda/" class="navlink" style="float: right;">别用 Anaconda：如何搭建一个 decent 的机器学习开发环境？ ▶</a></div></div><div class="post-comments"><div id="disqus_thread"></div></div><!----><div class="author post-author"><img alt="Author image" src="data:image/svg+xml,%3csvg fill='none' viewBox='0 0 180 180' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3e%3cdefs%3e%3cfilter id='__svg-blur-c3c0558b010e14207322c3bce7e52ad2'%3e%3cfeGaussianBlur in='SourceGraphic' stdDeviation='5'/%3e%3c/filter%3e%3c/defs%3e%3cimage x='0' y='0' filter='url(%23__svg-blur-c3c0558b010e14207322c3bce7e52ad2)' width='180' height='180' xlink:href='data:image/jpeg%3bbase64%2c/9j/2wBDAAYEBQYFBAYGBQYHBwYIChAKCgkJChQODwwQFxQYGBcUFhYaHSUfGhsjHBYWICwgIyYnKSopGR8tMC0oMCUoKSj/2wBDAQcHBwoIChMKChMoGhYaKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCj/wAARCABAAEADASIAAhEBAxEB/8QAHAAAAgIDAQEAAAAAAAAAAAAAAAUGCAEDBwQJ/8QANxAAAQMEAQIDAwkJAQAAAAAAAQIDBAAFBhEhEjEHE0EiUWEUFSQyUnGBkaEjJkJDYmR0gtHB/8QAFAEBAAAAAAAAAAAAAAAAAAAAAP/EABQRAQAAAAAAAAAAAAAAAAAAAAD/2gAMAwEAAhEDEQA/ALU0UVHsmvUqPKj2mxstSL1KSVpDu/LjtA6LzmuekHgJHKjwNDZAPJMlmKyp6S82y0nlS3FBKR95PFIF5zjCXC2L5AWod/LdCx%2badisRcPty3G5N7Cr1cE8%2bfOAWEn%2bhv6iB9w37ye9SNttDaAhtIQgcBKRoCgTwcrsE9xLcS829x1R0GxISFn/UndOq8lwtkC5N%2bXcIUaUj7L7SVj9RUcexqXZfpOHyVMdPKrXJcUqK6PUJ3ssn3FPs%2b9JoJdRSrHb2xe4SnmkOMPsrLMiM8NOMODuhQ/EEEcEEEbBprQBrgd7g3C8nIr4zfLtGfuFtmzYTMN/5OEfJl%2bWyOpPtLHSeopJ1tZ49a74a5xi1mdn%2bHmPPQVNfOdvSvyi6PYc2pSHWl%2bvSobG/QgHnWqCkB8R81PfK75v/ADnP%2b1hPiHmi1BKcpvqlHgATXOT%2bdO/EfwxyCxZTNZt%2bOXk25bhVH%2bjKc6QeejqRtKtdtg863x2qfeA/gRfLlkUG95bBct1oiOpeTHkDpdkqSdpHR3Snetk632HfYDo%2beWOdj2EXx1q95ALxHsUac04u6PnodSvT5A6teqeOw3xqqvHxEzIk/vVfOf75z/tX6zuxN5RbZEa3yY3zmw040pC17Stt1BStpwDkJUPXuClJG9aNDL34Y5pZ7muDLxm6l0K6UqZjqdQvnulaQQaBx4ZZnlsjxIsTbeQXVbk6fFZkdUlavOQFgdK9n2gASOfQ19Caph4TeEORY3fMfyrJGTbkt3WK0xDc0XXOtfSVKH8AAPY8n3Crn0BUOxN8WS/XLGZaujrdcuFtKuA6w4oqcQn4tuKUCPsqQamNJsox6JkMJtqSp1iSwvzYsthXS9GcHZaFeh9CDwQSCCDQOaKQYPcJlzx1p%2b4uMPPpddZ%2bUMpKEvpQ4pAcCT9XqCd62e/HFP6Dzw4MSEp5UOKwwp9ZcdLTYSXFnupWu5%2bJr0UVCpeRXu83Wfa8UtzTbUN4xpN2nq/ZNuAAlLbST1OEBQ7lKd%2bpoDLnk3bLsdsEY9TkeQm7zCn%2bSy1sNg/FbhGvghR9KmtJMWx2PYI8gpdelz5bnnTJsgguyHNa2dcAAcJSNBI4Ap3QFL8hmKt9huU1H1o0Z14felBP/lMK1yWGpUZ2PJbS6w6goWhY2FJI0QR7tUCzD4fzfitoi60WojSVfFXQNn8903rCEhCAlIASBoAegrNAHtUYw72bvlrYACRdeofjGYJ/WpPWmPFYjOSFsMobXIX5rqkjRWrQTs%2b86SB%2bFBuooooP/9k=' /%3e%3c/svg%3e" width="180" data-src="/assets/static/author.e6b6009.39e7de96836586e14662ec7a3092de84.jpg" data-srcset="/assets/static/author.e6b6009.39e7de96836586e14662ec7a3092de84.jpg 180w" data-sizes="(max-width: 180px) 100vw, 180px" class="author__image g-image g-image--lazy g-image--loading"><noscript><img src="/assets/static/author.e6b6009.39e7de96836586e14662ec7a3092de84.jpg" class="author__image g-image g-image--loaded" width="180" alt="Author image"></noscript><!----><p class="author__intro">
    阿巴阿巴 o((&gt;ω&lt; ))o
  </p><p class="author__links"><a href="//spencerwoo.com" target="_blank" rel="noopener"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="id-badge" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512" class="svg-inline--fa fa-id-badge fa-w-12"><path fill="currentColor" d="M336 0H48C21.5 0 0 21.5 0 48v416c0 26.5 21.5 48 48 48h288c26.5 0 48-21.5 48-48V48c0-26.5-21.5-48-48-48zM144 32h96c8.8 0 16 7.2 16 16s-7.2 16-16 16h-96c-8.8 0-16-7.2-16-16s7.2-16 16-16zm48 128c35.3 0 64 28.7 64 64s-28.7 64-64 64-64-28.7-64-64 28.7-64 64-64zm112 236.8c0 10.6-10 19.2-22.4 19.2H102.4C90 416 80 407.4 80 396.8v-19.2c0-31.8 30.1-57.6 67.2-57.6h5c12.3 5.1 25.7 8 39.8 8s27.6-2.9 39.8-8h5c37.1 0 67.2 25.8 67.2 57.6v19.2z"></path></svg></a><a href="/feed.xml" aria-current="page" target="_blank" class="active--exact active" style="color:#F5A623;"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="rss" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="svg-inline--fa fa-rss fa-w-14"><path fill="currentColor" d="M128.081 415.959c0 35.369-28.672 64.041-64.041 64.041S0 451.328 0 415.959s28.672-64.041 64.041-64.041 64.04 28.673 64.04 64.041zm175.66 47.25c-8.354-154.6-132.185-278.587-286.95-286.95C7.656 175.765 0 183.105 0 192.253v48.069c0 8.415 6.49 15.472 14.887 16.018 111.832 7.284 201.473 96.702 208.772 208.772.547 8.397 7.604 14.887 16.018 14.887h48.069c9.149.001 16.489-7.655 15.995-16.79zm144.249.288C439.596 229.677 251.465 40.445 16.503 32.01 7.473 31.686 0 38.981 0 48.016v48.068c0 8.625 6.835 15.645 15.453 15.999 191.179 7.839 344.627 161.316 352.465 352.465.353 8.618 7.373 15.453 15.999 15.453h48.068c9.034-.001 16.329-7.474 16.005-16.504z"></path></svg></a><a href="/archives" style="color:var(--title-color);"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="archive" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="svg-inline--fa fa-archive fa-w-16"><path fill="currentColor" d="M32 448c0 17.7 14.3 32 32 32h384c17.7 0 32-14.3 32-32V160H32v288zm160-212c0-6.6 5.4-12 12-12h104c6.6 0 12 5.4 12 12v8c0 6.6-5.4 12-12 12H204c-6.6 0-12-5.4-12-12v-8zM480 32H32C14.3 32 0 46.3 0 64v48c0 8.8 7.2 16 16 16h480c8.8 0 16-7.2 16-16V64c0-17.7-14.3-32-32-32z"></path></svg></a><a href="/friends" style="color:#06a878;"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="comment-dots" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="svg-inline--fa fa-comment-dots fa-w-16"><path fill="currentColor" d="M256 32C114.6 32 0 125.1 0 240c0 49.6 21.4 95 57 130.7C44.5 421.1 2.7 466 2.2 466.5c-2.2 2.3-2.8 5.7-1.5 8.7S4.8 480 8 480c66.3 0 116-31.8 140.6-51.4 32.7 12.3 69 19.4 107.4 19.4 141.4 0 256-93.1 256-208S397.4 32 256 32zM128 272c-17.7 0-32-14.3-32-32s14.3-32 32-32 32 14.3 32 32-14.3 32-32 32zm128 0c-17.7 0-32-14.3-32-32s14.3-32 32-32 32 14.3 32 32-14.3 32-32 32zm128 0c-17.7 0-32-14.3-32-32s14.3-32 32-32 32 14.3 32 32-14.3 32-32 32z"></path></svg></a>/
    <a href="//twitter.com/realSpencerWoo" target="_blank" style="color:#1da1f2;"><svg aria-hidden="true" focusable="false" data-prefix="fab" data-icon="twitter" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="svg-inline--fa fa-twitter fa-w-16"><path fill="currentColor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"></path></svg></a><a href="//weibo.com/spencerwoo" target="_blank" style="color:#E00;"><svg aria-hidden="true" focusable="false" data-prefix="fab" data-icon="weibo" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="svg-inline--fa fa-weibo fa-w-16"><path fill="currentColor" d="M407 177.6c7.6-24-13.4-46.8-37.4-41.7-22 4.8-28.8-28.1-7.1-32.8 50.1-10.9 92.3 37.1 76.5 84.8-6.8 21.2-38.8 10.8-32-10.3zM214.8 446.7C108.5 446.7 0 395.3 0 310.4c0-44.3 28-95.4 76.3-143.7C176 67 279.5 65.8 249.9 161c-4 13.1 12.3 5.7 12.3 6 79.5-33.6 140.5-16.8 114 51.4-3.7 9.4 1.1 10.9 8.3 13.1 135.7 42.3 34.8 215.2-169.7 215.2zm143.7-146.3c-5.4-55.7-78.5-94-163.4-85.7-84.8 8.6-148.8 60.3-143.4 116s78.5 94 163.4 85.7c84.8-8.6 148.8-60.3 143.4-116zM347.9 35.1c-25.9 5.6-16.8 43.7 8.3 38.3 72.3-15.2 134.8 52.8 111.7 124-7.4 24.2 29.1 37 37.4 12 31.9-99.8-55.1-195.9-157.4-174.3zm-78.5 311c-17.1 38.8-66.8 60-109.1 46.3-40.8-13.1-58-53.4-40.3-89.7 17.7-35.4 63.1-55.4 103.4-45.1 42 10.8 63.1 50.2 46 88.5zm-86.3-30c-12.9-5.4-30 .3-38 12.9-8.3 12.9-4.3 28 8.6 34 13.1 6 30.8.3 39.1-12.9 8-13.1 3.7-28.3-9.7-34zm32.6-13.4c-5.1-1.7-11.4.6-14.3 5.4-2.9 5.1-1.4 10.6 3.7 12.9 5.1 2 11.7-.3 14.6-5.4 2.8-5.2 1.1-10.9-4-12.9z"></path></svg></a><a href="//t.me/realSpencerWoo" target="_blank" style="color:#179cde;"><svg aria-hidden="true" focusable="false" data-prefix="fab" data-icon="telegram" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512" class="svg-inline--fa fa-telegram fa-w-16"><path fill="currentColor" d="M248 8C111 8 0 119 0 256s111 248 248 248 248-111 248-248S385 8 248 8zm121.8 169.9l-40.7 191.8c-3 13.6-11.1 16.9-22.4 10.5l-62-45.7-29.9 28.8c-3.3 3.3-6.1 6.1-12.5 6.1l4.4-63.1 114.9-103.8c5-4.4-1.1-6.9-7.7-2.5l-142 89.4-61.2-19.1c-13.3-4.2-13.6-13.3 2.8-19.7l239.1-92.2c11.1-4 20.8 2.7 17.2 19.5z"></path></svg></a><a href="//github.com/spencerwooo" target="_blank" style="color:var(--title-color);"><svg aria-hidden="true" focusable="false" data-prefix="fab" data-icon="github" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512" class="svg-inline--fa fa-github fa-w-16"><path fill="currentColor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a></p></div></div></main><footer class="footer"><div class="footer__copyright">
      Copyright ©2017 - 2024.
    </div><div class="footer__links">
      Runs on <svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="rocket" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="svg-inline--fa fa-rocket fa-w-16"><path fill="currentColor" d="M505.12019,19.09375c-1.18945-5.53125-6.65819-11-12.207-12.1875C460.716,0,435.507,0,410.40747,0,307.17523,0,245.26909,55.20312,199.05238,128H94.83772c-16.34763.01562-35.55658,11.875-42.88664,26.48438L2.51562,253.29688A28.4,28.4,0,0,0,0,264a24.00867,24.00867,0,0,0,24.00582,24H127.81618l-22.47457,22.46875c-11.36521,11.36133-12.99607,32.25781,0,45.25L156.24582,406.625c11.15623,11.1875,32.15619,13.15625,45.27726,0l22.47457-22.46875V488a24.00867,24.00867,0,0,0,24.00581,24,28.55934,28.55934,0,0,0,10.707-2.51562l98.72834-49.39063c14.62888-7.29687,26.50776-26.5,26.50776-42.85937V312.79688c72.59753-46.3125,128.03493-108.40626,128.03493-211.09376C512.07526,76.5,512.07526,51.29688,505.12019,19.09375ZM384.04033,168A40,40,0,1,1,424.05,128,40.02322,40.02322,0,0,1,384.04033,168Z"></path></svg><a href="//gridsome.org">Gridsome</a> and
      <svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="heart" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="svg-inline--fa fa-heart fa-w-16"><path fill="currentColor" d="M462.3 62.6C407.5 15.9 326 24.3 275.7 76.2L256 96.5l-19.7-20.3C186.1 24.3 104.5 15.9 49.7 62.6c-62.8 53.6-66.1 149.8-9.9 207.9l193.5 199.8c12.5 12.9 32.8 12.9 45.3 0l193.5-199.8c56.3-58.1 53-154.3-9.8-207.9z"></path></svg>Love. Deployed on
      <svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="caret-square-up" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="svg-inline--fa fa-caret-square-up fa-w-14"><path fill="currentColor" d="M0 432V80c0-26.51 21.49-48 48-48h352c26.51 0 48 21.49 48 48v352c0 26.51-21.49 48-48 48H48c-26.51 0-48-21.49-48-48zm355.515-140.485l-123.03-123.03c-4.686-4.686-12.284-4.686-16.971 0L92.485 291.515c-7.56 7.56-2.206 20.485 8.485 20.485h246.059c10.691 0 16.045-12.926 8.486-20.485z"></path></svg><a href="//vercel.com/spencerwoo/blog">Vercel</a>.
    </div><div class="footer__links"></div><div class="footer__links">
      Subscriber statistics powered by
      <svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="rss" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="svg-inline--fa fa-rss fa-w-14"><path fill="currentColor" d="M128.081 415.959c0 35.369-28.672 64.041-64.041 64.041S0 451.328 0 415.959s28.672-64.041 64.041-64.041 64.04 28.673 64.04 64.041zm175.66 47.25c-8.354-154.6-132.185-278.587-286.95-286.95C7.656 175.765 0 183.105 0 192.253v48.069c0 8.415 6.49 15.472 14.887 16.018 111.832 7.284 201.473 96.702 208.772 208.772.547 8.397 7.604 14.887 16.018 14.887h48.069c9.149.001 16.489-7.655 15.995-16.79zm144.249.288C439.596 229.677 251.465 40.445 16.503 32.01 7.473 31.686 0 38.981 0 48.016v48.068c0 8.625 6.835 15.645 15.453 15.999 191.179 7.839 344.627 161.316 352.465 352.465.353 8.618 7.373 15.453 15.999 15.453h48.068c9.034-.001 16.329-7.474 16.005-16.504z"></path></svg><a href="//api.spencerwoo.com/substats">Substats</a>. DO SUBSCRIBE!
    </div><div id="rss-stats"><a href="https://blog.spencerwoo.com/feed.xml"><img src="https://img.shields.io/badge/Subscribe-RSS-ffa500?logo=rss" alt="rss"></a><a href="https://feedly.com/i/subscription/feed%2Fhttps%3A%2F%2Fblog.spencerwoo.com%2Ffeed.xml"><img src="https://img.shields.io/badge/dynamic/json?label=Feedly&query=%24.data.totalSubs&url=https%3A%2F%2Fapi.spencerwoo.com%2Fsubstats%2F%3Fsource%3Dfeedly%26queryKey%3Dhttps%3A%2F%2Fblog.spencerwoo.com%2Fposts%2Findex.xml&labelColor=2bb24c&logoColor=white&color=282c34&logo=feedly&longCache=true" alt="feedly"></a><a href="https://www.inoreader.com/feed/https%3A%2F%2Fblog.spencerwoo.com%2Ffeed.xml"><img src="https://img.shields.io/badge/dynamic/json?label=Inoreader&query=%24.data.totalSubs&url=https%3A%2F%2Fapi.spencerwoo.com%2Fsubstats%2F%3Fsource%3Dinoreader%26queryKey%3Dhttps%3A%2F%2Fblog.spencerwoo.com%2Fposts%2Findex.xml&color=282c34&logo=data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iNzIiIGhlaWdodD0iNzIiIHhtbG5zPSJodHRwOi8vd3d3LnczLm9yZy8yMDAwL3N2ZyI+PHBhdGggZD0iTTM2IDBjMTkuODgyIDAgMzYgMTYuMTE4IDM2IDM2UzU1Ljg4MiA3MiAzNiA3MiAwIDU1Ljg4MiAwIDM2IDE2LjExOCAwIDM2IDB6bS03Ljk5IDMwLjk4QzIwLjgyNSAzMC45OCAxNSAzNi44MDQgMTUgNDMuOTkgMTUgNTEuMTc1IDIwLjgyNSA1NyAyOC4wMSA1N2M3LjE4MyAwIDEzLjAwOS01LjgyNSAxMy4wMDktMTMuMDExIDAtNy4xODUtNS44MjYtMTMuMDA5LTEzLjAwOS0xMy4wMDl6bTMuNjcgNS41NjVhMy43MjcgMy43MjcgMCAxMS0uMDA1IDcuNDU0IDMuNzI3IDMuNzI3IDAgMDEuMDA0LTcuNDU0em0tMy42Ny0xNC43NTh2NC42ODdjOS42NTYgMCAxNy41MTYgNy44NTggMTcuNTE2IDE3LjUxNWg0LjY4OWMwLTEyLjI0Mi05Ljk2MS0yMi4yMDItMjIuMjA1LTIyLjIwMnptMC05Ljc4N3Y0LjY4N2M3LjI5MiAwIDE0LjE0OCAyLjg0IDE5LjMwNiA3Ljk5OCA1LjE1OCA1LjE1NyA3Ljk5NSAxMi4wMTQgNy45OTUgMTkuMzA0SDYwYzAtOC41NDQtMy4zMjgtMTYuNTc3LTkuMzctMjIuNjJDNDQuNTg1IDE1LjMyNiAzNi41NTQgMTIgMjguMDEgMTJ6IiBmaWxsPSIjRkZGIiBmaWxsLXJ1bGU9Im5vbnplcm8iLz48L3N2Zz4=&labelColor=007bc7&longCache=true" alt="inoreader"></a></div></footer></div> <script>window.__INITIAL_STATE__={"data":{"post":{"title":"「输出」开销：为什么我们向 stdout 输出时那么慢？","path":"\u002F2020\u002F02\u002Fstdout-overhead\u002F","date":"February 22. 2020","timeToRead":7,"cjkWordCount":1230,"cjkReadTime":6,"tags":[{"id":"Tech","title":"Tech","path":"\u002Ftag\u002FTech\u002F"},{"id":"Neural Network","title":"Neural Network","path":"\u002Ftag\u002FNeural%20Network\u002F"},{"id":"Terminal","title":"Terminal","path":"\u002Ftag\u002FTerminal\u002F"},{"id":"Principle","title":"Principle","path":"\u002Ftag\u002FPrinciple\u002F"}],"description":"Console output overhead: why is writing to stdout so slow?","published":true,"content":"\u003Cblockquote\u003E\n\u003Cp\u003E本文发布于 Medium: \u003Ca href=\"https:\u002F\u002Fmedium.com\u002Fspencerweekly\u002Fconsole-output-overhead-why-is-writing-to-stdout-so-slow-b0cc7c88704c\" target=\"_blank\" rel=\"nofollow noopener noreferrer\"\u003EConsole output overhead: why is writing to stdout so slow?\u003C\u002Fa\u003E\u003C\u002Fp\u003E\n\u003C\u002Fblockquote\u003E\n\u003Cp\u003EGenerally, when we code through our projects, we need to output certain values in order to understand what the program is currently doing and what results it shows. Normally we do this with a simple print function:\u003C\u002Fp\u003E\n\u003Cpre class=\"language-python\"\u003E\u003Ccode class=\"language-python\"\u003E\u003Cspan class=\"token keyword\"\u003Eprint\u003C\u002Fspan\u003E\u003Cspan class=\"token punctuation\"\u003E(\u003C\u002Fspan\u003E\u003Cspan class=\"token string\"\u003E'Hey look, this is the result.'\u003C\u002Fspan\u003E\u003Cspan class=\"token punctuation\"\u003E)\u003C\u002Fspan\u003E\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cp\u003EAnd for most of our use cases, this would be more than enough. However, when we are outputting a huge amount of data, like printing a file with 10k lines of text onto the terminal, we will eventually suffer from IO bottlenecks. This is something I discovered when trying to find a nice progress bar library for PyTorch in order to monitor the model’s training progress. Let me explain.\u003C\u002Fp\u003E\n\u003Ch2 id=\"what-did-i-discover\"\u003E\u003Ca href=\"#what-did-i-discover\" aria-hidden=\"true\"\u003E\u003Cspan class=\"icon icon-link\"\u003E\u003C\u002Fspan\u003E\u003C\u002Fa\u003EWhat did I discover?\u003C\u002Fh2\u003E\n\u003Cp\u003ENow, if you are familiar with modern neural network libraries, you may have heard of Keras. Keras is a highly encapsulated library that takes care of data output, model compilation, backward propaganda and more for you under the hood, meaning that it offers less agility than other low-level libraries like PyTorch. The one feature I really appreciate about Keras is its nice little training progress bar that shows the current training progress, iteration duration, ETA, loss and other relevant statistics. Below is an example.\u003C\u002Fp\u003E\n\u003Cfigure\u003E\u003Cimg src=\"https:\u002F\u002Fcdn.spencer.felinae98.cn\u002Fblog\u002F2020\u002F07\u002F20200722-215810.png\" alt=\"Visualizing training progress with Keras\"\u003E\u003Cfigcaption\u003EVisualizing training progress with Keras\u003C\u002Ffigcaption\u003E\u003C\u002Ffigure\u003E\n\u003Cp\u003EHowever, PyTorch offers far more customizable features when it comes to designing, compiling and training your model, but it also means you have to take care of outputting progress, training details and others by yourself. This is when I discovered IO bottlenecks when outputting directly to \u003Ccode class=\"language-text\"\u003Estdout\u003C\u002Fcode\u003E, that is, your terminal.\u003C\u002Fp\u003E\n\u003Cp\u003EThe library I first tried is called \u003Ca href=\"https:\u002F\u002Fgithub.com\u002Fyueyericardo\u002Fpkbar\" target=\"_blank\" rel=\"nofollow noopener noreferrer\"\u003E\u003Cstrong\u003Epkbar\u003C\u002Fstrong\u003E — a progress bar library intended to bring Keras style progress monitoring to the PyTorch ecosystem\u003C\u002Fa\u003E. \u003Cstrong\u003E(Don’t use it!)\u003C\u002Fstrong\u003E It’s outputs are like this:\u003C\u002Fp\u003E\n\u003Cfigure\u003E\u003Cimg src=\"https:\u002F\u002Fcdn.spencer.felinae98.cn\u002Fblog\u002F2020\u002F07\u002F20200722-215810-1.png\" alt=\"pkbar: Keras style progress bar for PyTorch\"\u003E\u003Cfigcaption\u003Epkbar: Keras style progress bar for PyTorch\u003C\u002Ffigcaption\u003E\u003C\u002Ffigure\u003E\n\u003Cp\u003EMost progress bar libraries are implemented by wrapping itself around an iterative object, in our case, a PyTorch Data Loader. When building a progress bar, we need to refresh our progress, statistics and other parameters on each iteration. This means every time we come to the end of an iteration, we need to output to our terminal with a delay when the program waits for the writing to return. This delay is often unnoticed when we are simply writing a single message, but when we are iterating through thousands of images, we need to write the latest status on each iteration. A small delay multiplied by a thousand iterations gives us a huge time difference, and results in bottlenecks we encounter now.\u003C\u002Fp\u003E\n\u003Cfigure\u003E\u003Cimg src=\"https:\u002F\u002Fcdn.spencer.felinae98.cn\u002Fblog\u002F2020\u002F07\u002F20200722-215810-2.png\" alt=\"Each iteration comes with a time cost that adds up to be a huge delay\"\u003E\u003Cfigcaption\u003EEach iteration comes with a time cost that adds up to be a huge delay\u003C\u002Ffigcaption\u003E\u003C\u002Ffigure\u003E\n\u003Cp\u003EWhat happened to me when I was using pkbar was exactly what I described above: \u003Cstrong\u003Eeach iteration was accompanied with a delay which eventually builds up into a time difference I simply can’t ignore.\u003C\u002Fstrong\u003E To put numbers into perspective, I was applying a DeepFool adversarial attack on a pre-trained ResNet18 CNN. It took about 5 minutes on my laptop running on GPU without pkbar, and when I added pkbar to visualize progress, it gave me an ETA of nearly 20 minutes! Holy crap, I removed pkbar almost immediately.\u003C\u002Fp\u003E\n\u003Ch2 id=\"so-why-were-there-delays\"\u003E\u003Ca href=\"#so-why-were-there-delays\" aria-hidden=\"true\"\u003E\u003Cspan class=\"icon icon-link\"\u003E\u003C\u002Fspan\u003E\u003C\u002Fa\u003ESo why were there delays?\u003C\u002Fh2\u003E\n\u003Cp\u003EIn order to find out what was causing this huge delay and how we could avoid it, I turned for help on Stack Overflow. A particular question gave me the answer: \u003Ca href=\"https:\u002F\u002Fstackoverflow.com\u002Fquestions\u002F3857052\u002Fwhy-is-printing-to-stdout-so-slow-can-it-be-sped-up\" target=\"_blank\" rel=\"nofollow noopener noreferrer\"\u003EWhy is printing to stdout so slow? Can it be sped up?\u003C\u002Fa\u003E\u003C\u002Fp\u003E\n\u003Cp\u003EIn this question, the author compared the following speeds:\u003C\u002Fp\u003E\n\u003Cul\u003E\n\u003Cli\u003EWriting output to the terminal, which is \u003Ccode class=\"language-text\"\u003Estdout\u003C\u002Fcode\u003E\u003C\u002Fli\u003E\n\u003Cli\u003EWriting output to a file\u003C\u002Fli\u003E\n\u003Cli\u003EWriting output to \u003Ccode class=\"language-text\"\u003Estdout\u003C\u002Fcode\u003E, but with \u003Ccode class=\"language-text\"\u003Estdout\u003C\u002Fcode\u003E redirected to \u003Ccode class=\"language-text\"\u003E\u002Fdev\u002Fnull\u003C\u002Fcode\u003E\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Cfigure\u003E\u003Cimg src=\"https:\u002F\u002Fcdn.spencer.felinae98.cn\u002Fblog\u002F2020\u002F07\u002F20200722-215810-3.png\" alt=\"Time cost for printing to stdout, file and \u002Fdev\u002Fnull\"\u003E\u003Cfigcaption\u003ETime cost for printing to stdout, file and \u002Fdev\u002Fnull\u003C\u002Ffigcaption\u003E\u003C\u002Ffigure\u003E\n\u003Cp\u003EWow, that shows a huge difference for printing to \u003Ccode class=\"language-text\"\u003Estdout\u003C\u002Fcode\u003E and redirecting \u003Ccode class=\"language-text\"\u003Estdout\u003C\u002Fcode\u003E to \u003Ccode class=\"language-text\"\u003E\u002Fdev\u002Fnull\u003C\u002Fcode\u003E. Even writing output to file is faster than directly writing to the terminal. So…, why?\u003C\u002Fp\u003E\n\u003Cblockquote\u003E\n\u003Cp\u003ECongratulations, you have just discovered the importance of I\u002FO buffering. :-)\u003C\u002Fp\u003E\n\u003C\u002Fblockquote\u003E\n\u003Cp\u003ESo it turns out that I\u002FO buffering is what made even “writing to file” faster than “writing to \u003Ccode class=\"language-text\"\u003Estdout\u003C\u002Fcode\u003E”. When we are directly writing outputs to our terminal, each writing operation is being done “synchronously”, which means our programs waits for the “write” to complete before it continues to the next commands.\u003C\u002Fp\u003E\n\u003Cfigure\u003E\u003Cimg src=\"https:\u002F\u002Fcdn.spencer.felinae98.cn\u002Fblog\u002F2020\u002F07\u002F20200722-215810-4.png\" alt=\"Synchronous writing to stdout\"\u003E\u003Cfigcaption\u003ESynchronous writing to stdout\u003C\u002Ffigcaption\u003E\u003C\u002Ffigure\u003E\n\u003Cp\u003EEach time our programs writes something to \u003Ccode class=\"language-text\"\u003Estdout\u003C\u002Fcode\u003E, we are met with this delay. However, writing to files are not of the same case. When we are writing to files, we have what is known as “I\u002FO buffering”, which means the program outputs whatever it needs to write to the file, and the OS catches all these contents to write where it then stores in a file \u003Cstrong\u003Eas a bulk\u003C\u002Fstrong\u003E afterwards. \u003Cstrong\u003EAll our “write” functions return before anything is actually written to a file.\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003Cfigure\u003E\u003Cimg src=\"https:\u002F\u002Fcdn.spencer.felinae98.cn\u002Fblog\u002F2020\u002F07\u002F20200722-215810-5.png\" alt=\"Buffered writing output to file\"\u003E\u003Cfigcaption\u003EBuffered writing output to file\u003C\u002Ffigcaption\u003E\u003C\u002Ffigure\u003E\n\u003Cp\u003ESo what should we do here? Well, in order to avoid I\u002FO overhead, we either have to do training and update output in parallel or asynchronously like we do in GUIs when we have an UI thread and a controller, or we will need to write in bulks to \u003Ccode class=\"language-text\"\u003Estdout\u003C\u002Fcode\u003E like what we are doing when writing to files. \u003Cstrong\u003EIt’s a trade-off: interactivity versus bulk efficiency.\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003Ch2 id=\"what-i-did-to-visualize-pytorch-training-with-minimum-overhead\"\u003E\u003Ca href=\"#what-i-did-to-visualize-pytorch-training-with-minimum-overhead\" aria-hidden=\"true\"\u003E\u003Cspan class=\"icon icon-link\"\u003E\u003C\u002Fspan\u003E\u003C\u002Fa\u003EWhat I did to visualize PyTorch training with minimum overhead?\u003C\u002Fh2\u003E\n\u003Cp\u003EAfter I ditched pkbar, I found a perfect progress bar library: \u003Ca href=\"https:\u002F\u002Fgithub.com\u002Ftqdm\u002Ftqdm\" target=\"_blank\" rel=\"nofollow noopener noreferrer\"\u003Etqdm — A Fast, Extensible Progress Bar for Python and CLI\u003C\u002Fa\u003E. With over 13.4k+ stars, \u003Ccode class=\"language-text\"\u003Etqdm\u003C\u002Fcode\u003E is easily the best Python library for us to implement training progress visualization.\u003C\u002Fp\u003E\n\u003Cfigure\u003E\u003Cimg src=\"https:\u002F\u002Fcdn.spencer.felinae98.cn\u002Fblog\u002F2020\u002F07\u002F20200722-215810-6.png\" alt=\"tqdm in action\"\u003E\u003Cfigcaption\u003Etqdm in action\u003C\u002Ffigcaption\u003E\u003C\u002Ffigure\u003E\n\u003Cp\u003E\u003Ccode class=\"language-text\"\u003Etqdm\u003C\u002Fcode\u003E is simple, efficient and comes with minimal overhead. The author claims that \u003Ccode class=\"language-text\"\u003Etqdm\u003C\u002Fcode\u003E only delays output for a minimum of 60ns per iteration. That’s impressive. In addition to its low overhead, \u003Ccode class=\"language-text\"\u003Etqdm\u003C\u002Fcode\u003E uses smart algorithms to predict the remaining time and to skip unnecessary iteration displays, which allows for a negligible overhead in most cases.\u003C\u002Fp\u003E\n\u003Ch3 id=\"installation\"\u003E\u003Ca href=\"#installation\" aria-hidden=\"true\"\u003E\u003Cspan class=\"icon icon-link\"\u003E\u003C\u002Fspan\u003E\u003C\u002Fa\u003EInstallation\u003C\u002Fh3\u003E\n\u003Cp\u003EEither install \u003Ccode class=\"language-text\"\u003Etqdm\u003C\u002Fcode\u003E using conda directly:\u003C\u002Fp\u003E\n\u003Cpre class=\"language-bash\"\u003E\u003Ccode class=\"language-bash\"\u003Econda \u003Cspan class=\"token function\"\u003Einstall\u003C\u002Fspan\u003E \u003Cspan class=\"token parameter variable\"\u003E-c\u003C\u002Fspan\u003E conda-forge tqdm\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cp\u003EAnd install \u003Ccode class=\"language-text\"\u003Eipywidgets\u003C\u002Fcode\u003E with pip to enable Jupyter Notebook integration:\u003C\u002Fp\u003E\n\u003Cpre class=\"language-bash\"\u003E\u003Ccode class=\"language-bash\"\u003Epip \u003Cspan class=\"token function\"\u003Einstall\u003C\u002Fspan\u003E ipywidgets\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cp\u003EOr integrate the following code to your \u003Ccode class=\"language-text\"\u003Eenvironment.yml\u003C\u002Fcode\u003E:\u003C\u002Fp\u003E\n\u003Cpre class=\"language-yaml\"\u003E\u003Ccode class=\"language-yaml\"\u003E\u003Cspan class=\"token key atrule\"\u003Echannels\u003C\u002Fspan\u003E\u003Cspan class=\"token punctuation\"\u003E:\u003C\u002Fspan\u003E\n  \u003Cspan class=\"token punctuation\"\u003E-\u003C\u002Fspan\u003E conda\u003Cspan class=\"token punctuation\"\u003E-\u003C\u002Fspan\u003Eforge\n  \u003Cspan class=\"token punctuation\"\u003E-\u003C\u002Fspan\u003E defaults\n\u003Cspan class=\"token key atrule\"\u003Edependencies\u003C\u002Fspan\u003E\u003Cspan class=\"token punctuation\"\u003E:\u003C\u002Fspan\u003E\n  \u003Cspan class=\"token punctuation\"\u003E-\u003C\u002Fspan\u003E tqdm\n  \u003Cspan class=\"token punctuation\"\u003E-\u003C\u002Fspan\u003E \u003Cspan class=\"token key atrule\"\u003Epip\u003C\u002Fspan\u003E\u003Cspan class=\"token punctuation\"\u003E:\u003C\u002Fspan\u003E\n      \u003Cspan class=\"token punctuation\"\u003E-\u003C\u002Fspan\u003E ipywidgets\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cp\u003EThen run:\u003C\u002Fp\u003E\n\u003Cpre class=\"language-bash\"\u003E\u003Ccode class=\"language-bash\"\u003Econda \u003Cspan class=\"token function\"\u003Eenv\u003C\u002Fspan\u003E update\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Ch3 id=\"integration\"\u003E\u003Ca href=\"#integration\" aria-hidden=\"true\"\u003E\u003Cspan class=\"icon icon-link\"\u003E\u003C\u002Fspan\u003E\u003C\u002Fa\u003EIntegration\u003C\u002Fh3\u003E\n\u003Cp\u003EWe can easily integrate \u003Ccode class=\"language-text\"\u003Etqdm\u003C\u002Fcode\u003E into our PyTorch project. Basically what you need to do is just to wrap \u003Ccode class=\"language-text\"\u003Etqdm\u003C\u002Fcode\u003E on an iterable object, and that’s it.\u003C\u002Fp\u003E\n\u003Cpre class=\"language-python\"\u003E\u003Ccode class=\"language-python\"\u003E\u003Cspan class=\"token keyword\"\u003Efrom\u003C\u002Fspan\u003E tqdm \u003Cspan class=\"token keyword\"\u003Eimport\u003C\u002Fspan\u003E tqdm\n\u003Cspan class=\"token keyword\"\u003Efor\u003C\u002Fspan\u003E i \u003Cspan class=\"token keyword\"\u003Ein\u003C\u002Fspan\u003E tqdm\u003Cspan class=\"token punctuation\"\u003E(\u003C\u002Fspan\u003E\u003Cspan class=\"token builtin\"\u003Erange\u003C\u002Fspan\u003E\u003Cspan class=\"token punctuation\"\u003E(\u003C\u002Fspan\u003E\u003Cspan class=\"token number\"\u003E10000\u003C\u002Fspan\u003E\u003Cspan class=\"token punctuation\"\u003E)\u003C\u002Fspan\u003E\u003Cspan class=\"token punctuation\"\u003E)\u003C\u002Fspan\u003E\u003Cspan class=\"token punctuation\"\u003E:\u003C\u002Fspan\u003E\n    \u003Cspan class=\"token punctuation\"\u003E.\u003C\u002Fspan\u003E\u003Cspan class=\"token punctuation\"\u003E.\u003C\u002Fspan\u003E\u003Cspan class=\"token punctuation\"\u003E.\u003C\u002Fspan\u003E\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cpre class=\"language-text\"\u003E\u003Ccode class=\"language-text\"\u003E76%|████████████████████████████ | 7568\u002F10000 [00:33&lt;00:10, 229.00it\u002Fs]\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cp\u003EFor PyTorch \u003Ccode class=\"language-text\"\u003EDataLoader\u003C\u002Fcode\u003E, we can first load our data like so:\u003C\u002Fp\u003E\n\u003Cpre class=\"language-python\"\u003E\u003Ccode class=\"language-python\"\u003Edataset_loader \u003Cspan class=\"token operator\"\u003E=\u003C\u002Fspan\u003E torch\u003Cspan class=\"token punctuation\"\u003E.\u003C\u002Fspan\u003Eutils\u003Cspan class=\"token punctuation\"\u003E.\u003C\u002Fspan\u003Edata\u003Cspan class=\"token punctuation\"\u003E.\u003C\u002Fspan\u003EDataLoader\u003Cspan class=\"token punctuation\"\u003E(\u003C\u002Fspan\u003Edataset\u003Cspan class=\"token punctuation\"\u003E,\u003C\u002Fspan\u003E batch_size\u003Cspan class=\"token operator\"\u003E=\u003C\u002Fspan\u003E\u003Cspan class=\"token number\"\u003E4\u003C\u002Fspan\u003E\u003Cspan class=\"token punctuation\"\u003E)\u003C\u002Fspan\u003E\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cp\u003EThen initialize \u003Ccode class=\"language-text\"\u003Etdqm\u003C\u002Fcode\u003E:\u003C\u002Fp\u003E\n\u003Cpre class=\"language-python\"\u003E\u003Ccode class=\"language-python\"\u003Epbar \u003Cspan class=\"token operator\"\u003E=\u003C\u002Fspan\u003E tqdm\u003Cspan class=\"token punctuation\"\u003E(\u003C\u002Fspan\u003Edataset_loader\u003Cspan class=\"token punctuation\"\u003E)\u003C\u002Fspan\u003E\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cp\u003EHere, we can add the description of the progress bar:\u003C\u002Fp\u003E\n\u003Cpre class=\"language-python\"\u003E\u003Ccode class=\"language-python\"\u003Epbar\u003Cspan class=\"token punctuation\"\u003E.\u003C\u002Fspan\u003Eset_description\u003Cspan class=\"token punctuation\"\u003E(\u003C\u002Fspan\u003E\u003Cspan class=\"token string\"\u003E'Validate predictions'\u003C\u002Fspan\u003E\u003Cspan class=\"token punctuation\"\u003E)\u003C\u002Fspan\u003E\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cp\u003EAlso, we can initialize the post-fix dict for the progress bar, so that we can update the post-fix on the fly:\u003C\u002Fp\u003E\n\u003Cpre class=\"language-python\"\u003E\u003Ccode class=\"language-python\"\u003Epbar\u003Cspan class=\"token punctuation\"\u003E.\u003C\u002Fspan\u003Eset_postfix\u003Cspan class=\"token punctuation\"\u003E(\u003C\u002Fspan\u003Eloss\u003Cspan class=\"token operator\"\u003E=\u003C\u002Fspan\u003E\u003Cspan class=\"token string\"\u003E'0.0%'\u003C\u002Fspan\u003E\u003Cspan class=\"token punctuation\"\u003E,\u003C\u002Fspan\u003E acc\u003Cspan class=\"token operator\"\u003E=\u003C\u002Fspan\u003E\u003Cspan class=\"token string\"\u003E'0.0%'\u003C\u002Fspan\u003E\u003Cspan class=\"token punctuation\"\u003E)\u003C\u002Fspan\u003E\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cp\u003EAnd Voilà! We can directly iterate through \u003Ccode class=\"language-text\"\u003Etdqm\u003C\u002Fcode\u003E, just like we did with PyTorch \u003Ccode class=\"language-text\"\u003EDataLoader\u003C\u002Fcode\u003E. (My model is wrapped with Foolbox, so I can make predictions with the \u003Ccode class=\"language-text\"\u003E.forward()\u003C\u002Fcode\u003E function.)\u003C\u002Fp\u003E\n\u003Cpre class=\"language-python\"\u003E\u003Ccode class=\"language-python\"\u003E\u003Cspan class=\"token keyword\"\u003Efor\u003C\u002Fspan\u003E image\u003Cspan class=\"token punctuation\"\u003E,\u003C\u002Fspan\u003E label \u003Cspan class=\"token keyword\"\u003Ein\u003C\u002Fspan\u003E pbar\u003Cspan class=\"token punctuation\"\u003E:\u003C\u002Fspan\u003E\n  \u003Cspan class=\"token comment\"\u003E# make a prediction\u003C\u002Fspan\u003E\n  prob \u003Cspan class=\"token operator\"\u003E=\u003C\u002Fspan\u003E model\u003Cspan class=\"token punctuation\"\u003E.\u003C\u002Fspan\u003Eforward\u003Cspan class=\"token punctuation\"\u003E(\u003C\u002Fspan\u003Eimage\u003Cspan class=\"token punctuation\"\u003E.\u003C\u002Fspan\u003Enumpy\u003Cspan class=\"token punctuation\"\u003E(\u003C\u002Fspan\u003E\u003Cspan class=\"token punctuation\"\u003E)\u003C\u002Fspan\u003E\u003Cspan class=\"token punctuation\"\u003E)\u003C\u002Fspan\u003E\n\n  \u003Cspan class=\"token punctuation\"\u003E.\u003C\u002Fspan\u003E\u003Cspan class=\"token punctuation\"\u003E.\u003C\u002Fspan\u003E\u003Cspan class=\"token punctuation\"\u003E.\u003C\u002Fspan\u003E  \u003Cspan class=\"token comment\"\u003E# calculate and update loss\u003C\u002Fspan\u003E\n  pbar\u003Cspan class=\"token punctuation\"\u003E.\u003C\u002Fspan\u003Eset_postfix\u003Cspan class=\"token punctuation\"\u003E(\u003C\u002Fspan\u003Eloss\u003Cspan class=\"token operator\"\u003E=\u003C\u002Fspan\u003E\u003Cspan class=\"token string\"\u003E'{:.2f}%'\u003C\u002Fspan\u003E\u003Cspan class=\"token punctuation\"\u003E.\u003C\u002Fspan\u003E\u003Cspan class=\"token builtin\"\u003Eformat\u003C\u002Fspan\u003E\u003Cspan class=\"token punctuation\"\u003E(\u003C\u002Fspan\u003Eloss\u003Cspan class=\"token punctuation\"\u003E)\u003C\u002Fspan\u003E\u003Cspan class=\"token punctuation\"\u003E)\u003C\u002Fspan\u003E\n\n  \u003Cspan class=\"token comment\"\u003E# calculate and update accuracy\u003C\u002Fspan\u003E\n  pbar\u003Cspan class=\"token punctuation\"\u003E.\u003C\u002Fspan\u003Eset_postfix\u003Cspan class=\"token punctuation\"\u003E(\u003C\u002Fspan\u003Eacc\u003Cspan class=\"token operator\"\u003E=\u003C\u002Fspan\u003E\u003Cspan class=\"token string\"\u003E'{:.2f}%'\u003C\u002Fspan\u003E\u003Cspan class=\"token punctuation\"\u003E.\u003C\u002Fspan\u003E\u003Cspan class=\"token builtin\"\u003Eformat\u003C\u002Fspan\u003E\u003Cspan class=\"token punctuation\"\u003E(\u003C\u002Fspan\u003Eacc\u003Cspan class=\"token punctuation\"\u003E)\u003C\u002Fspan\u003E\u003Cspan class=\"token punctuation\"\u003E)\u003C\u002Fspan\u003E\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Ch3 id=\"features\"\u003E\u003Ca href=\"#features\" aria-hidden=\"true\"\u003E\u003Cspan class=\"icon icon-link\"\u003E\u003C\u002Fspan\u003E\u003C\u002Fa\u003EFeatures\u003C\u002Fh3\u003E\n\u003Cp\u003E\u003Ccode class=\"language-text\"\u003Etqdm\u003C\u002Fcode\u003E can be used directly in a CLI environment with no special configuration whatsoever.\u003C\u002Fp\u003E\n\u003Cfigure\u003E\u003Cimg src=\"https:\u002F\u002Fcdn.spencer.felinae98.cn\u002Fblog\u002F2020\u002F07\u002F20200722-215810-7.gif\" alt=\"tqdm in a CLI enviroment\"\u003E\u003Cfigcaption\u003Etqdm in a CLI enviroment\u003C\u002Ffigcaption\u003E\u003C\u002Ffigure\u003E\n\u003Cp\u003EAlso, we can run \u003Ccode class=\"language-text\"\u003Etqdm\u003C\u002Fcode\u003E in VS Code’s Python Interactive console, or Jupyter Notebook. In this case, \u003Ccode class=\"language-text\"\u003Etqdm\u003C\u002Fcode\u003E will output progress asynchronously according to the \u003Ca href=\"https:\u002F\u002Fjupyter-notebook.readthedocs.io\u002Fen\u002Fstable\u002Fexamples\u002FNotebook\u002FRunning%20Code.html#Output-is-asynchronous\" target=\"_blank\" rel=\"nofollow noopener noreferrer\"\u003ENotebook API\u003C\u002Fa\u003E.\u003C\u002Fp\u003E\n\u003Cfigure\u003E\u003Cimg src=\"https:\u002F\u002Fcdn.spencer.felinae98.cn\u002Fblog\u002F2020\u002F07\u002F20200722-215810-8.gif\" alt=\"tqdm outputting inside Jupyter Notebook \u002F VS Code Python Interactive\"\u003E\u003Cfigcaption\u003Etqdm outputting inside Jupyter Notebook \u002F VS Code Python Interactive\u003C\u002Ffigcaption\u003E\u003C\u002Ffigure\u003E\n\u003Cp\u003ETo kick it up a notch, we can actually output “real” progress bars. Utilizing \u003Ccode class=\"language-text\"\u003Eipywidget\u003C\u002Fcode\u003E, we can directly draw interactive progress bars inside Jupyter Notebook. \u003Ca href=\"https:\u002F\u002Fipywidgets.readthedocs.io\u002Fen\u002Flatest\u002Fuser_install.html\" target=\"_blank\" rel=\"nofollow noopener noreferrer\"\u003ESee here for instructions on how to enable \u003C\u002Fa\u003E\u003Ccode class=\"language-text\"\u003Eipywidgets\u003C\u002Fcode\u003E\u003Ca href=\"https:\u002F\u002Fipywidgets.readthedocs.io\u002Fen\u002Flatest\u002Fuser_install.html\" target=\"_blank\" rel=\"nofollow noopener noreferrer\"\u003E.\u003C\u002Fa\u003E\u003C\u002Fp\u003E\n\u003Cp\u003EAfter enabling these widgets inside Jupyter Notebook, all we have to do is change the way on how we imported \u003Ccode class=\"language-text\"\u003Etqdm\u003C\u002Fcode\u003E. Change:\u003C\u002Fp\u003E\n\u003Cpre class=\"language-python\"\u003E\u003Ccode class=\"language-python\"\u003E\u003Cspan class=\"token keyword\"\u003Eimport\u003C\u002Fspan\u003E tqdm \u003Cspan class=\"token keyword\"\u003Eas\u003C\u002Fspan\u003E tqdm\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cp\u003ETo:\u003C\u002Fp\u003E\n\u003Cpre class=\"language-python\"\u003E\u003Ccode class=\"language-python\"\u003E\u003Cspan class=\"token keyword\"\u003Eimport\u003C\u002Fspan\u003E tqdm\u003Cspan class=\"token punctuation\"\u003E.\u003C\u002Fspan\u003Enotebook \u003Cspan class=\"token keyword\"\u003Eas\u003C\u002Fspan\u003E tqdm\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cp\u003ENow, run the code once more, if everything goes well, we should be able to see a neat progress bar with colors indicating the current task’s state: pending, success or failure.\u003C\u002Fp\u003E\n\u003Cfigure\u003E\u003Cimg src=\"https:\u002F\u002Fcdn.spencer.felinae98.cn\u002Fblog\u002F2020\u002F07\u002F20200722-215810-9.gif\" alt=\"tqdm integrating directly into Jupyter Notebook with ipywidgets - https:\u002F\u002Fipywidgets.readthedocs.io\u002Fen\u002Flatest\"\u003E\u003Cfigcaption\u003Etqdm integrating directly into Jupyter Notebook with ipywidgets - https:\u002F\u002Fipywidgets.readthedocs.io\u002Fen\u002Flatest\u003C\u002Ffigcaption\u003E\u003C\u002Ffigure\u003E\n\u003Chr\u003E\n\u003Cp\u003EThis is the end of the article, for more information on how to utilize \u003Ccode class=\"language-text\"\u003Etqdm\u003C\u002Fcode\u003E, I recommend this article: \u003Ca href=\"https:\u002F\u002Fmedium.com\u002Fbetter-programming\u002Fpython-progress-bars-with-tqdm-by-example-ce98dbbc9697\" target=\"_blank\" rel=\"nofollow noopener noreferrer\"\u003EPython Progress Bars with tqdm by Example\u003C\u002Fa\u003E. Also, the official documentation of \u003Ccode class=\"language-text\"\u003Etqdm\u003C\u002Fcode\u003E is quite thorough, covering most cases when using \u003Ccode class=\"language-text\"\u003Etqdm\u003C\u002Fcode\u003E.\u003C\u002Fp\u003E\n\u003Cp\u003EThere’s one thing to note, and that is: you should not use “print” to output anything when a \u003Ccode class=\"language-text\"\u003Etqdm\u003C\u002Fcode\u003E progress bar is running, as this behavior will mess up the \u003Ccode class=\"language-text\"\u003Etqdm\u003C\u002Fcode\u003E output. If you have to write something, do remember to use:\u003C\u002Fp\u003E\n\u003Cpre class=\"language-python\"\u003E\u003Ccode class=\"language-python\"\u003Etqdm\u003Cspan class=\"token punctuation\"\u003E.\u003C\u002Fspan\u003Ewrite\u003Cspan class=\"token punctuation\"\u003E(\u003C\u002Fspan\u003E\u003Cspan class=\"token operator\"\u003E&lt;\u003C\u002Fspan\u003Econtext\u003Cspan class=\"token operator\"\u003E\u003E\u003C\u002Fspan\u003E\u003Cspan class=\"token punctuation\"\u003E)\u003C\u002Fspan\u003E\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cp\u003EThat’s all, thanks for reading.\u003C\u002Fp\u003E\n","cover_image":{"type":"image","mimeType":"image\u002Fpng","src":"\u002Fassets\u002Fstatic\u002Fstdout-overhead.a209973.f73a441d9c80efd3bf7fdcbc3c3bb935.png","size":{"width":1280,"height":400},"sizes":"(max-width: 1280px) 100vw, 1280px","srcset":["\u002Fassets\u002Fstatic\u002Fstdout-overhead.a67b0b2.f73a441d9c80efd3bf7fdcbc3c3bb935.png 480w","\u002Fassets\u002Fstatic\u002Fstdout-overhead.a209973.f73a441d9c80efd3bf7fdcbc3c3bb935.png 1280w"],"dataUri":"data:image\u002Fsvg+xml,%3csvg fill='none' viewBox='0 0 1280 400' xmlns='http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg' xmlns:xlink='http:\u002F\u002Fwww.w3.org\u002F1999\u002Fxlink'%3e%3cdefs%3e%3cfilter id='__svg-blur-c5617f729664b6247de180661ab366a7'%3e%3cfeGaussianBlur in='SourceGraphic' stdDeviation='10'\u002F%3e%3c\u002Ffilter%3e%3c\u002Fdefs%3e%3cimage x='0' y='0' filter='url(%23__svg-blur-c5617f729664b6247de180661ab366a7)' width='1280' height='400' xlink:href='data:image\u002Fpng%3bbase64%2cUklGRigBAABXRUJQVlA4IBwBAAAQBgCdASpAABQAPm0wlUakIq4hJAyRwA2JZAC1G3bHgRG5\u002F8WwduWydzuzDzv3xJMckyNgqozQ9YAA\u002FvmXB70fzx\u002F\u002FmaP3yeu7cLP4SPExOuZ7qGAoedL2\u002FqsnvMAlyPHHaHcB5TIMAD1IOFTda4\u002FpJh\u002FTLujnxy7cBnovrI8Sv9xg52rO2Im%2bwVTttiouKF\u002F9u17p3T78MGu1U3Te%2byRyt\u002F%2b0ajpX%2bBX1kBhCG6Hagx5s8DIZi2U\u002Fn9DCTpy3%2bGEUwJ1vRJs%2b3p6Tx\u002FSK9\u002F7oMqP7bszWSEuOUydyUeCe0ubGusnrBosYHydEwe18FnydHNSbTeYY6vyqeWJRJgnGr5zlb4AAyJuTu0ATKmUUzXDcom4V1h498WjgAA==' \u002F%3e%3c\u002Fsvg%3e"}},"previous":{"title":"Remote Jupyter Lab：如何用 Jupyter Lab 将远程服务器资源最大化利用","path":"\u002F2020\u002F03\u002Fremote-jupyter-lab\u002F"},"next":{"title":"别用 Anaconda：如何搭建一个 decent 的机器学习开发环境？","path":"\u002F2020\u002F02\u002Fdont-use-anaconda\u002F"}},"context":{"previousElement":"aad4c7672a13ebf499a5c676a1c44acc","nextElement":"019f5e132f75ba89b72a3541de102594","id":"05b7a955fd916d61da03ffe78743103a"}};(function(){var s;(s=document.currentScript||document.scripts[document.scripts.length-1]).parentNode.removeChild(s);}());</script><script src="/assets/js/app.95a869fe.js" defer></script><script src="/assets/js/page--src--templates--blog-post-vue.8c30e663.js" defer></script>
  </body>
</html>
